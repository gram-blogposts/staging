<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Accelerating Equivariant Graph Neural Networks with JAX |  
    
  
</title>
<meta name="author" content=" ">
<meta name="description" content="A Tutorial on How to Make EGNNs Faster">

  <meta name="keywords" content="geometry, machine-learning, generative-models, deep-learning, representation-learning">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/blog/2024/egnn-jax/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Accelerating Equivariant Graph Neural Networks with JAX",
            "description": "A Tutorial on How to Make EGNNs Faster",
            "published": "June 30, 2024",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "None",
                "affiliations": [
                  {
                    "name": "None",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
            
            
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/instructions/">instructions
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/reviews/">reviews
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Accelerating Equivariant Graph Neural Networks with JAX</h1>
        <p>A Tutorial on How to Make EGNNs Faster</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#introduction">Introduction</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#recap-of-equivariance">Recap of Equivariance</a>
                      </li>
                    
                      <li>
                        <a href="#equivariant-graph-neural-networks">Equivariant Graph Neural Networks</a>
                      </li>
                    
                      <li>
                        <a href="#why-jax">Why JAX?</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#experiments">Experiments</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#n-body-dataset">N-Body Dataset</a>
                      </li>
                    
                      <li>
                        <a href="#qm9-dataset">QM9 Dataset</a>
                      </li>
                    
                      <li>
                        <a href="#data-preparation">Data Preparation</a>
                      </li>
                    
                      <li>
                        <a href="#training">Training</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#evaluation">Evaluation</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#speed-comparison">Speed Comparison</a>
                      </li>
                    
                      <li>
                        <a href="#reproduction-results">Reproduction Results</a>
                      </li>
                    
                      <li>
                        <a href="#comparison-with-other-methods">Comparison with other Methods</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#concluding-remarks">Concluding Remarks</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <h2 id="introduction">Introduction</h2>

<p>This blogpost serves as a tutorial for the fast and scalable training of Equivariant Neural Networks, which are slower to train due to the handling of more complex data. We propose leveraging JAX’s capabilities to address these challenges. In this work, we analyze the benefits of utilizing JAX and provide a detailed breakdown of the steps needed to achieve a fully JIT-compatible framework. This approach not only enhances the performance of Neural Networks but also opens the door for future research in developing fully equivariant transformers using JAX.</p>

<p>This blogpost serves three purposes:</p>
<ol>
  <li>Explain the ideas of equivariance in networks while also explaining some of the methods used.</li>
  <li>Give an overview of the performance tests conducted on the two approaches.</li>
  <li>Provide an overview of reproduction results for the Equivariant Graph Neural Network.</li>
</ol>

<h3 id="recap-of-equivariance">Recap of Equivariance</h3>

<p>As equivariance is prevalent in the natural sciences <d-cite key="balaban1985applications"></d-cite><d-cite key="gupta2013wtf"></d-cite><d-cite key="miller1995wordnet"></d-cite><d-cite key="thölke2022equivariant"></d-cite><d-cite key="maron2020learning"></d-cite>, it makes sense to utilize them for our neural networks, especially given the evidence suggesting that it significantly improves performance through increasing the network’s generalizability <d-cite key="bronstein2021geometric"></d-cite>. One large area within this subfield of deep learning is learning 3D translation and rotation symmetries, where various techniques have been created such as Graph Convolutional Neural Networks <d-cite key="cohen2016group"></d-cite> and Tensor Field Networks <d-cite key="thomas2018tensor"></d-cite>.</p>

<p>Following these works, more efficient implementations have emerged, with the first being the Equivariant Graph Neural Network (EGNN) <d-cite key="satorras2021en"></d-cite>. Based on the GNN <d-cite key="gori2005new"></d-cite><d-cite key="kipf2018neural"></d-cite><d-cite key="bruna2014spectral"></d-cite>, which follows a message passing scheme, it innovates by inputting the relative squared distance between two coordinates into the edge operation and to make the output equivariant, updates the coordinates of the nodes per layer. This specific method bypasses any expensive computations/approximations relative to other, similar methods while retaining high performance levels, making it preferable compared to most other GNN architectures.</p>

<p>More recently, transformer architectures have been utilized within the field of equivariant models. While not typically used for these types of problems due to how they were originally developed for sequential tasks <d-cite key="devlin2019bert"></d-cite><d-cite key="baevski2020wav2vec"></d-cite>, recent work has suggested their effectiveness for tackling such issues <d-cite key="thölke2022equivariant"></d-cite><d-cite key="fuchs2020se"></d-cite><d-cite key="liao2023equiformer"></d-cite>. This is possible through the incorporation of domain-related inductive biases, allowing them to model geometric constraints and operations. In addition, one property of transformers is that they assume full adjacency by default, which is something that can be adjusted to better match the local connectivity of GNN approaches. These additions further increase the complexity of the framework, strongly highlighting the need for a more efficient alternative.</p>

<h3 id="equivariant-graph-neural-networks">Equivariant Graph Neural Networks</h3>

<p>Given a set of \(T_g\) transformations on a set \(X\) (\(T_g: X \rightarrow X\)) for an element \(g \in G\), where \(G\) is a group acting on \(X\), a function \(\varphi: X \rightarrow Y\) is equivariant to \(g\) iff an equivalent transformation \(S_g: Y \rightarrow Y\) exists on its output space \(Y\), such that:</p>

<div style="text-align: center;">
$$
\varphi(T_g(x)) = S_g(\varphi(x)). \qquad \qquad \text{(Equation 1)}
$$
</div>

<p>In other words, translating the input set \(T_g(x)\) and then applying \(\varphi(T_x(x))\) on it yields the same result as first running the function \(y = \varphi(x)\) and then applying an equivalent translation to the output \(S_g(y)\) such that Equation 1 is fulfilled and \(\varphi(x+g) = \varphi(x) + g\) <d-cite key="satorras2021en"></d-cite>.</p>

<h3 id="equivariant-graph-neural-networks-1">Equivariant Graph Neural Networks</h3>

<p>For a given graph \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\) with nodes \(v_i \in \mathcal{V}\) and edges
\(=e_{ij} \in \mathcal{E}\), we can define a graph convolutional layer as the following:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, a_{ij}), \qquad \qquad \text{(Equation 2)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 3)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i), \qquad \qquad \text{(Equation 4)}
$$
</div>

<p>where \(\mathbf{h}\_i^l \in \mathbb{R}^{nf}\) is the nf-dimensional embedding of node \(v_i\) at layer \(l\), \(a_{ij}\) are the edge attributes, \(\mathcal{N}\_i\) is the set of neighbors of node \(v_i\), and \(\varphi_e\) and \(\varphi_h\) are the
edge and node operations respectively, typically approximated by Multilayer Perceptrons (MLPs).</p>

<p>To make this implementation equivariant, <d-cite key="satorras2021en"></d-cite> introduced the inputting of the relative squared distances between two points and updating of the node positions at each time step, leading to the following formulae:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, ||\mathbf{x}\_i^l - \mathbf{x}\_j^l||^2, a_{ij}), \qquad \qquad \text{(Equation 5)}
$$
$$
x_i^{l+1} = x_i^l + C \sum_{j \neq i} (\mathbf{x}\_i^l - \mathbf{x}\_j^l) \varphi_x(\mathbf{m}\_{ij}), \qquad \qquad \text{(Equation 6)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 7)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i). \qquad \qquad \text{(Equation 8)}
$$
</div>

<p>This idea of using the distances during computation forms one of the bases of our proposed transformer architecture, as it is a simple yet effective way to impose geometric equivariance within a system.</p>

<h3 id="why-jax">Why JAX?</h3>

<p>JAX is a high-performance numerical computing library that provides several advantages over traditional frameworks. By default, JAX automatically compiles library calls using just-in-time (JIT) compilation, ensuring optimal execution. It utilizes XLA-optimized kernels, allowing for sophisticated algorithm expression without leaving Python. Furthermore, JAX also excels in utilizing multiple GPU or TPU cores and automatically evaluating gradients through differentiation transformations, making it ideal for high-compute scenarios.</p>

<p>This is partially caused by how JAX often uses pointers to reference elements in memory instead of copying them, which has several advantages:</p>

<ul>
  <li>
<strong>Efficiency:</strong> Through pointers, JAX avoids the unnecessary copying of data, resulting in faster computations and lower memory usage.</li>
  <li>
<strong>Functionally Pure:</strong> Since JAX functions are pure (i.e., contain no side effects), using pointers ensures that the data is not accidentally modified, maintaining the integrity of all operations.</li>
  <li>
<strong>Automatic Differentiation:</strong> JAX’s efficient gradient computation relies on its functional programming model. Pointers allow JAX to track operations and dependencies without data duplication.</li>
</ul>

<hr>

<h2 id="experiments">Experiments</h2>

<h3 id="n-body-dataset">N-Body dataset</h3>

<p>In this dataset, a dynamical system consisting of 5 atoms is modeled in 3D space. Each atom has a positive and negative charge, a starting position and a starting velocity. The task is to predict the position of the particles after 1000 time steps. The movement of the particles follow the rules of physics: Same charges repel and different charges attract. The task is equivariant in the sense, that translating and rotating the 5-body system on the input space is the same as rotating the output space.</p>

<h3 id="qm9-dataset">QM9 dataset</h3>

<p>This dataset consists of small molecules and the task is to predict a chemical property. The atoms of the molecules have 3 dimensional positions and each atom is one hot encoded to the atom type. This task is an invariant task, since the chemical property does not depend on position or rotation of the molecule. In addition, larger batch sizes were also experimented with due to smaller sizes causing bottlenecks during training.</p>

<h3 id="data-preparation">Data Preparation</h3>

<p>Here, we introduce a straightforward method for preprocessing data from a PyTorch-compatible format to one suitable for JAX. Our approach handles node features, edge attributes, indices, positions, and target properties. The key step would be converting the data to jax numpy (jnp) arrays, ensuring compatibility with JAX operations. For usage examples, refer to <code class="language-plaintext highlighter-rouge">qm9\utils.py</code> or <code class="language-plaintext highlighter-rouge">n_body\utils.py</code>.</p>

<h3 id="training">Training</h3>

<p>We now address the key differences and steps in adapting the training loop, model saving, and evalution functions for JAX (refer to <code class="language-plaintext highlighter-rouge">main_qm9.py</code> and <code class="language-plaintext highlighter-rouge">nbody_egnn_trainer.py</code>).</p>

<p>JAX uses a functional approach to define and update the model parameters. We use <code class="language-plaintext highlighter-rouge">jax.jit</code> via the <code class="language-plaintext highlighter-rouge">partial</code> decorator for JIT compilation, which ensures that our code runs efficiently by compiling the functions once and then executing them multiple times. We also utilize <code class="language-plaintext highlighter-rouge">static_argnames</code> as decorators for the loss and update functions, which specify the arguments to treat as static. By doing this, JAX can assume these arguments will not change and optimize the function accordingly.</p>

<p>Moreover, model initialization in JAX requires knowing the input sizes beforehand. We extract features to get their shapes and initialize the model using <code class="language-plaintext highlighter-rouge">model.init(jax_seed, *init_feat, max_num_nodes)</code>. This seed initializes the random number generators, which then produces the random number sequences used in virtually all processes. Also, this seed is created using the <code class="language-plaintext highlighter-rouge">jax.random.PRNGKey</code> function, which is used for all random operations. This ensures that they are all reproducible and can be split into multiple independent keys if needed.</p>

<p>The loss function is called through <code class="language-plaintext highlighter-rouge">jax.grad(loss_fn)(params, x, edge_attr, edge_index, pos, node_mask, edge_mask, max_num_nodes, target)</code>. <code class="language-plaintext highlighter-rouge">jax.grad</code> is a powerful tool in JAX for automatic differentiation, allowing us to compute gradients of scalar-valued functions with respect to their inputs.</p>

<hr>

<h2 id="evaluation">Evaluation</h2>

<h3 id="speed-comparison">Speed Comparison</h3>

<p>The EGNN authors <d-cite key="satorras2021en"></d-cite> note that while their approach is more computationally efficient, it is still slower than Linear and Graph Neural Networks. Thus, the aim is to preserve the properties of the model while also providing a fast alternative. We demonstrate the effectivity of building a JAX-based alternative by comparing the forward pass times of the original EGNN implementation with our version. The results of which can be seen in the following graph:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-12 text-center">
        <b>Figure 1.</b> EGNN speed comparison between JAX EGNN (ours) and the PyTorch EGNN <d-cite key="satorras2021en"></d-cite>. Benchmark results represent a single forward pass averaged over 100 tries. The batch sizes used here are 32, 64 and 128.
    </div>
</div>

<p>One notable observation is the consistency in performance. The JAX implementation exhibits less variance in duration values, resulting in more stable and predictable performances across runs. This is particularly important for large-scale applications where the performance consistency can impact overall system reliability and efficiency.</p>

<p>Additionally, as the number of nodes increases, the JAX implementation maintains a less steep increase in computation time compared to PyTorch. This indicates better scalability, making the JAX-based EGNN more suitable for handling larger and more complex graphs.</p>

<h3 id="reproduction-results">Reproduction Results</h3>

<p>To show that our implementation generally preserves the performance and characteristics of the base model, we perform a reproduction of the results reported in <d-cite key="satorras2021en"></d-cite> and display the results for several properties in both experiments. They can be found in the table below.</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th style="text-align: center">EGNN</th>
      <th style="text-align: center">EGNN (Ours)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>QM9 (ε<sub>HOMO</sub>) (meV)</td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">75</td>
    </tr>
    <tr>
      <td>N-Body (Position MSE)</td>
      <td style="text-align: center">0.0071</td>
      <td style="text-align: center">0.0025</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1.</strong> Reproduction results comparing <d-cite key="satorras2021en"></d-cite> with our JAX implementation.</p>

<p>Here, our EGNN implementation outperforms the original author’s implementation on the N-Body dataset. Moreover, other publicly available EGNN implementations also achieve a similar performance as our model on our data. We therefore argue that the increased performance stems from how the dataset is generated slightly differently compared to the one presented in <d-cite key="satorras2021en"></d-cite>.</p>

<hr>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>Our EGNN comparisons reveal that the JAX-based model is faster than traditional PyTorch implementations, benefiting from JIT compilation to optimize runtime performance. In addition, we also demonstrate that these JAX-based models also achieve comparable performances to the aforementioned PyTorch ones, meaning that they are generally more suitable for equivariance tasks.</p>

<p>We also adapted the model for two well-known datasets: the QM9 dataset for molecule property prediction and the N-body dataset for simulating physical systems. This demonstrates the flexibility and potential of our JAX framework as a strong foundation for further development. Our work suggests that the JAX-based EGNN framework can be effectively extended to other applications, facilitating future research and advancements in equivariant neural networks and beyond.</p>

<hr>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2024-06-30-egnn-jax/2024-06-30-egnn-jax.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    <div class="container">
      © Copyright 2024
      
      
      . 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


  
</body>
</html>
