<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Towards Equivariant Adaptation of Large Pretrained Models |  
    
  
</title>
<meta name="author" content=" ">
<meta name="description" content="How do you make your foundation model &lt;ins&gt;equivariant and robust&lt;/ins&gt; to known transformations without re-training from scratch?">

  <meta name="keywords" content="geometry, machine-learning, generative-models, deep-learning, representation-learning">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/blog/2024/towards-equivariant-adaptation/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
} .fixed-size-img2 {
  width: 100%;
  height: 240px; /* Adjust the height as needed */
  object-fit: cover; /* Ensures the image covers the specified area */
} .fixed-size-img3 {
  width: 100%;
  height: 180px; /* Adjust the height as needed */
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Towards Equivariant Adaptation of Large Pretrained Models",
            "description": "How do you make your foundation model <ins>equivariant and robust</ins> to known transformations without re-training from scratch?",
            "published": "July 21, 2024",
            "authors": [
              
              {
                "author": "Siba Smarak Panigrahi",
                "authorURL": "https://sibasmarak.github.io/",
                "affiliations": [
                  {
                    "name": "McGill University and Mila",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Arnab Kumar Mondal",
                "authorURL": "https://arnab39.github.io/",
                "affiliations": [
                  {
                    "name": "McGill University and Mila",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sékou-Oumar Kaba",
                "authorURL": "https://oumarkaba.github.io/",
                "affiliations": [
                  {
                    "name": "McGill University and Mila",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
            
            
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/instructions/">instructions
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/reviews/">reviews
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Towards Equivariant Adaptation of Large Pretrained Models</h1>
        <p>How do you make your foundation model <ins>equivariant and robust</ins> to known transformations without re-training from scratch?</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#what-is-equivariance">What is Equivariance?</a>
                </div>
                
              
                <div>
                  <a href="#decoupling-equivariance-from-architecture-with-canonicalization">Decoupling Equivariance from Architecture with Canonicalization</a>
                </div>
                
              
                <div>
                  <a href="#learning-to-predict-the-correct-orientation-for-the-pretrained-network">Learning to Predict the Correct Orientation for the Pretrained Network</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#enter-the-canonicalization-prior">Enter the Canonicalization Prior</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#results-at-a-glance">Results at a Glance</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#image-classification">Image Classification</a>
                      </li>
                    
                      <li>
                        <a href="#instance-segmentation">Instance Segmentation</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#conclusion">Conclusion</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <p>Deep learning has witnessed tremendous growth in the past decade. Still, as we strive for more nuanced understanding and performance improvements, one challenge emerges clearly: how do we ensure our models understand data transformations? Enter equivariance, an idea that can help our networks maintain consistent behaviour with data transformations. But with the rise of large pretrained models, how do we make them equivariant without changing their architecture or retraining the model from scratch with data augmentation? In this blogpost, we delve into ideas presented
in the paper “Equivariant Adaptation of Large Pretrained Models”<d-cite key="mondal2023equivariant"></d-cite> to answer this question.</p>

<h2 id="what-is-equivariance">What is Equivariance?</h2>
<p>Equivariant networks <d-cite key="cohen2016group,worrall2019deep,bronstein2021geometric"></d-cite> are deep neural networks that maintain consistent behaviour when input data undergo transformations like rotation, scaling, or translation. In simpler terms, if we rotate an image of a cat, an equivariant network would still recognize it as a cat! Another example of this would be segmentation maps on images. If we rotate an image, the segmentation map should rotate in the same way to maintain consistency.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image1-480.webp 480w,/assets/img/equivariant-adaptation/image1-800.webp 800w,/assets/img/equivariant-adaptation/image1-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image2-480.webp 480w,/assets/img/equivariant-adaptation/image2-800.webp 800w,/assets/img/equivariant-adaptation/image2-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image2.png" class="img-fluid rounded z-depth-1 fixed-size-img2" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Equivariant tasks. Instance segmentation requires segmentation maps to be consistent with the input image transformations and 
    classification requires the network to recognize the same object in different orientations.
</div>

<p>The beauty of this is that such networks lead to more accurate, robust predictions and need fewer samples to train – this is great in theory but hard to implement in practice, especially for large pretrained models whose equivariant counterparts are not trivial to design or are very expensive to re-train from scratch. These massive models pretrained on the entire internet are extremely good at solving and reasoning about different tasks and are called <code class="language-plaintext highlighter-rouge">foundation models</code> <d-cite key="bommasani2021opportunities"></d-cite>. Despite having such capabilities, foundation models are not naturally equivariant and usually don’t handle transformations well. (see the GPT-4 example below) Our goal is to incorporate the benefits of equivariance in existing foundation models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image3-480.webp 480w,/assets/img/equivariant-adaptation/image3-800.webp 800w,/assets/img/equivariant-adaptation/image3-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image3.png" class="img-fluid rounded z-depth-1 fixed-size-img3" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image4-480.webp 480w,/assets/img/equivariant-adaptation/image4-800.webp 800w,/assets/img/equivariant-adaptation/image4-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    ChatGPT-4 for image parsing. The model is accurate in extracting text from 'straight' images, but it fails to do so for 'inverted' images.
</div>

<h2 id="decoupling-equivariance-from-architecture-with-canonicalization">Decoupling Equivariance from Architecture with Canonicalization</h2>

<p>A recent alternative to designing equivariant networks was proposed by Kaba et al. <d-cite key="kaba2023equivariance"></d-cite>.  It suggests that instead of changing the network architecture to incorporate equivariance, why not first learn to transform the input data into a ‘standard’ format (or orientation), also known as <code class="language-plaintext highlighter-rouge">canonical form</code>. This way, our task prediction network can work on this standardized format, ensuring consistency. This process involves adding an additional inexpensive network called the <code class="language-plaintext highlighter-rouge">canonicalization network</code> or $c$, which learns to standardize the input. In our formulation, for an input $x$, the output from canonicalization network is $c(x) = g$, where $g$ denotes the group element corresponding to the orientation of $x$. The primary network that learns to solve the task based on the standardized input is called the <code class="language-plaintext highlighter-rouge">prediction network</code> or $\phi$. In this particular formulation, achieving equivariance requires only ensuring that the canonicalization process is invariant to the transformation of the input. This means no matter which orientation you see the input, the canonicalization process should always bring it back to the same canonical orientation. This is achieved by using a shallow and cheap equivariant architecture for the canonicalization network. (see <d-cite key="kaba2023equivariance"></d-cite> for more details)</p>

<p>Finally, the combination of the canonicalization network and the prediction network can be represented as $\Phi$:</p>

\[\Phi(x) = c(x) \circ \phi(c(x)^{-1}. x)\]

\[\Rightarrow \Phi(g. x) = c(g. x) \circ \phi(c(g. x)^{-1}. g. x)\]

\[\Rightarrow \Phi(g. x) = g.c(x) \circ \phi(c(x)^{-1}. x) = g \circ \Phi(x)\]

<p>The beauty of this approach lies in how the canonicalization network separates the equivariance requirement from the core prediction network architecture. This means that you have the flexibility to employ any powerful pretrained large neural network for the main prediction task.</p>

<p>Sounds straightforward? Well, it has a hitch.</p>

<p>The <strong>main challenge</strong> is ensuring the canonicalization network ‘plays nice’ with the prediction network. For example, the canonicalization network can output orientations that hurt the training of the prediction network, leading to poor task performance. This becomes more important when the prediction network is pretrained on a certain dataset. For instance, if the canonicalization network transforms all images to be upside-down, but our pretrained prediction network wasn’t trained on upside-down images, the whole system falls apart. So, it’s vital that the canonicalization network outputs orientations of the data that is in-distribution for the pretrained prediction network.</p>

<h2 id="learning-to-predict-the-correct-orientation-for-the-pretrained-network">Learning to Predict the Correct Orientation for the Pretrained Network</h2>

<p>The magic lies in designing our canonicalization function not just to transform data but to do so while being aware of how our prediction model was initially trained. The key is ensuring that the data being transformed (or standardized) is done to align with what the pretrained prediction model expects. Mathematically, the goal is to bring the predicted out-of-distribution orientations to the distribution of orientations the pretrained prediction network has seen.</p>

<h1 id="enter-the-canonicalization-prior">Enter the Canonicalization Prior</h1>

<p>In simple terms, it’s a guiding force ensuring that our canonicalization function behaves and produces output that the pretrained prediction network would expect and appreciate. 
We leverage the idea that our data can provide hints on the ‘typical’ transformations it undergoes. By encoding this into a prior, one can guide our canonicalization function to produce transformed data that’s not just standardized but also aligned with what the prediction network was trained on.</p>

<p>While mathematical and intricate, this entire process can be boiled down to ensuring that the large pretrained prediction network always looks at in-distribution samples. 
This results in a highly robust model that can confidently handle varied transformations in the input data, giving accurate predictions every time.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image5-480.webp 480w,/assets/img/equivariant-adaptation/image5-800.webp 800w,/assets/img/equivariant-adaptation/image5-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Training and inference with canonicalization prior. The canonicalization function learns to output the canonical orientations seen in the dataset during training by minimising KL between the orientation distributions of predicted and pretraing dataset (prior regularization). During inference, transformed data is brought back to the canonical orientation by the canonicalization process.
</div>

<h2 id="results-at-a-glance">Results at a Glance</h2>

<p>This section highlights the effectiveness of the approach for image classification and instance segmentation tasks. Additional results and experiments including point cloud classification and part segmentation are detailed in <d-cite key="mondal2023equivariant"></d-cite>.</p>

<h1 id="image-classification">Image Classification</h1>

<p>The authors select Vision Transformer (ViT) <d-cite key="dosovitskiy2020image"></d-cite> and ResNet-50 <d-cite key="he2016deep"></d-cite> as pretrained<d-footnote>Pretrained on ImageNet<d-cite key="deng2009imagenet"></d-cite>.</d-footnote> prediction network for image classification and \(C_8\) group, i.e., eight discrete rotations (multiples of 45\(^\circ\)) as the set of known transformations. The objective is to make the prediction networks equivariant and robust to these transformations, as an example, on CIFAR-100 dataset <d-cite key="krizhevsky2009learning"></d-cite>.</p>

<p>The authors compare different fine-tuning setups. First, <strong>Vanilla</strong> indicates the standard fine-tuning on the downstream dataset. <strong>C8-Aug.</strong> indicates fine-tuning on the downstream
dataset and \(C_8\) group data augmentations. <strong>LC</strong> is the learned canonicalization approach proposed in Kaba et. al. <d-cite key="kaba2023equivariance"></d-cite>. 
<strong>Prior-Regularized LC</strong> is the learned canonicalization approach with prior regularization (as described in above sections, proposed in <d-cite key="mondal2023equivariant"></d-cite>). The evaluation includes reporting the performance of the models on the test set of <strong>CIFAR-100</strong> and an augmented version, <strong>CIFAR-100 [C8]</strong>, where each sample is augmented with every transformation of \(C_8\) group.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image6-480.webp 480w,/assets/img/equivariant-adaptation/image6-800.webp 800w,/assets/img/equivariant-adaptation/image6-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fine-tuning performance of ResNet-50 and ViT on CIFAR-100 dataset. Vanilla has the highest performance on CIFAR-100, but it is the worst on CIFAR-100 [C8] which indicates poor
    robustness. Prior-Regularized LC preserves the performance on CIFAR-100 and outperforms other baselines on CIFAR-100 [C8].
</div>

<h1 id="instance-segmentation">Instance Segmentation</h1>
<p>Furthermore, the authors scale this idea to large foundation models like the Segment Anything Model (SAM) <d-cite key="kirillov2023segment"></d-cite> and make it robust to rotations while having a nominal increase in the number of parameters and inference speed.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image7-480.webp 480w,/assets/img/equivariant-adaptation/image7-800.webp 800w,/assets/img/equivariant-adaptation/image7-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/equivariant-adaptation/image7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Predicted masks from the Segment Anything Model (SAM) <d-cite key="kirillov2023segment"></d-cite> showcasing both the original model and prior-regularized equivariant adaptation 
    for 90-degrees counter-clockwise rotated input images taken from the COCO 2017 dataset <d-cite key="lin2014microsoft"></d-cite>. The approach makes SAM equivariant to the
    group of 90-degrees rotations while only requiring 0.3% extra parameters and modestly increasing the inference time by 7.3%.
</div>

<p>Finally, to facilitate the ideas discussed on equivariant adaptation of large-scale models, an open-source package <a href="https://github.com/arnab39/equiadapt" rel="external nofollow noopener" target="_blank">Equiadapt</a> is available from the authors.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In the ever-evolving world of AI and deep learning, it is critical to ensure models are robust and aware of symmetries. By learning to smartly transform our input data so that they are in the correct orientation for the pretrained models, we can create large-scale models that are powerful and aware of data transformations, bringing us a step closer to AI systems that understand the world as we do. As research into scaling continues, the fusion of large foundational models with equivariant adaptation techniques such as the one presented in this blogpost has the potential to emerge as a fundamental approach in enhancing the consistency and reliability of AI systems.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2024-07-11-equivariant-adaptation.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    <div class="container">
      © Copyright 2024
      
      
      . 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


  
</body>
</html>
