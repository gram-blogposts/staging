<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Effect of equivariance on training dynamics |  
    
  
</title>
<meta name="author" content=" ">
<meta name="description" content="Can relaxing equivariance help in finding better minima?">

  <meta name="keywords" content="geometry, machine-learning, generative-models, deep-learning, representation-learning">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/blog/2024/relaxed-equivariance/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Effect of equivariance on training dynamics",
            "description": "Can relaxing equivariance help in finding better minima?",
            "published": "June 30, 2024",
            "authors": [
              
              {
                "author": "Anon",
                "authorURL": "anon.com",
                "affiliations": [
                  {
                    "name": "Anon",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
            
            
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/instructions/">instructions
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/reviews/">reviews
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Effect of equivariance on training dynamics</h1>
        <p>Can relaxing equivariance help in finding better minima?</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#background">Background</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#regular-gcnn">Regular GCNN</a>
                      </li>
                    
                      <li>
                        <a href="#steerable-gcnn">Steerable GCNN</a>
                      </li>
                    
                      <li>
                        <a href="#relaxed-regular-gcnn">Relaxed regular GCNN</a>
                      </li>
                    
                      <li>
                        <a href="#relaxed-steerable-gcnn">Relaxed steerable GCNN</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#methodology">Methodology</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#datasets">Datasets</a>
                      </li>
                    
                      <li>
                        <a href="#training-dynamics-evaluation">Training Dynamics Evaluation</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#results">Results</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#smoke-plume-with-full-equivariance">Smoke Plume with Full Equivariance</a>
                      </li>
                    
                      <li>
                        <a href="#super-resolution">Super Resolution</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#concluding-remarks">Concluding Remarks</a>
                </div>
                
              
                <div>
                  <a href="#references">References</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <p><em>Group Equivariant Convolutional Network</em> (G-CNN) has gained significant traction in recent years owing to their ability to generalize the property of CNNs being equivariant to translations in convolutional layers. With equivariance, the network is able to exploit groups of symmetries and a direct consequence of this is that it generally needs less data to perform well. However, incorporating such knowledge into the network may not always be advantageous, especially when the data itself does not exhibit full equivariance. To address this issue, the G-CNN was modified, introducing <em>relaxed group equivariant CNNs</em> (RG-CNN). Such modified networks adaptively learn the degree of equivariance imposed on the network, i.e. enabling it to operate on a level between full equivariance and no equivariance.
Surprisingly, for rotational symmetries on fully equivariant data, <d-cite key="wang2023relaxed"></d-cite> found that a G-CNN exhibits poorer performance compared to a RG-CNN. This is a surprising result because a G-CNN, i.e. a fully equivariant network, is designed to perform well on fully equivariant data. Possibly the training dynamics benefit from relaxing of the equivariance constraint. To investigate this, we use the framework described in <d-cite key="park2022visiontransformerswork"></d-cite> for measuring convexity and flatness using the Hessian spectra.</p>

<p>Inspired by the aforementioned observations, this blog post aims to answer the question: <strong>How does the equivariance imposed on a network affect its training dynamics?</strong> We identify the following subquestions:</p>

<ol>
  <li>How does equivariance imposed on a network influence generalization?</li>
  <li>How does equivariance imposed on a network influence the convexity of the loss landscape?</li>
</ol>

<p>We tackle these subquestions by analyzing trained models to investigate their training dynamics.</p>

<p>In view of space constraint, in this blogpost, we omit our reproducibility study and refer the readers to <a href="https://github.com/*****/***/blob/main/blogpost.md" rel="external nofollow noopener" target="_blank">our extended blog post</a> (removed for anonimization). Nevertheless, our reproducibility studies corroborated the following claims:</p>
<ol>
  <li>Relaxed steerable G-CNN outperforms steerable G-CNN (fully equivariant network) on fully rotationally equivariant data as shown in the experiment on the super resolution dataset in <d-cite key="wang2023relaxed"></d-cite>.</li>
  <li>Relaxed G-CNN outperforms G-CNN on non-fully rotationally equivariant data as shown in the experiment on the smoke plume dataset in <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite>.</li>
</ol>

<h2 id="background">Background</h2>

<h3 id="regular-g-cnn">Regular G-CNN</h3>

<p>Consider the segmentation task depicted in the picture below.</p>

<div style="text-align: center;">
  <img src="https://analyticsindiamag.com/wp-content/uploads/2020/07/u-net-segmentation-e1542978983391.png" alt="Figure 1" style="max-width: 100%;">
  <p>Annotated segmented image taken from <d-cite key="cordts2016cityscapesdatasetsemanticurban"></d-cite></p>
</div>

<p>Naturally, applying segmentation on a rotated 2D image should give the same segmented image as applying such rotation after segmentation. Mathematically, for a neural network $NN$ to be equivariant w.r.t. the group $(G,\cdot)$, such as 2D rotations, then the following property needs to be satisfied:</p>

\[\begin{align*} 
NN (T_g x) = T'_g NN (x) &amp; \qquad \qquad \forall g \in G. \\
\end{align*}\]

<p>To build such a network, it is sufficient that each of its layers is equivariant in the same sense. Recall that a CNN achieves equivariance to translations by sharing weights in kernels that are translated across the input in each of its convolution layers. Hence, a G-CNN extends this concept of weight sharing to achieve equivariance w.r.t an arbitrary locally-compact group $G$.</p>

<h4 id="lifting-convolution">Lifting convolution</h4>

<p>Consider an input signal $f^0: \mathbb{R}^n \rightarrow \mathbb{R}^c$, where $c$ is the number of channels. When passing it through a G-CNN, from the outset, it undergoes the lifting convolution with kernel $k : \mathbb{R}^n \rightarrow \mathbb{R}^{m \times c}$ on $x \in \mathbb{R}^n$ and $g \in G$:</p>

\[(k*_{lifting} f^0)(g) = \int_{y \in \mathbb{R}^n}k(g^{-1}y)f^0(y) dy\]

<p>Suppose $f^1: G \rightarrow \mathbb{R}^m$ is the output signal thereof, which is fed to the next layer.</p>

<h4 id="g-equivariant-convolution">$G$-equivariant convolution</h4>

<p>Now, $f^1$ undergoes $G$-equivariant convolution with a kernel $\psi: G \rightarrow \mathbb{R}^{k \times m}$ on $g \in G$:</p>

\[(\psi *_{G} f^1)(g) = \int_{h \in G}\psi(g^{-1}h)f^1(h)dh\]

<p>For now on we will focus on affine groups, i.e., let $G := \mathbb{R}^n \rtimes H$, where $H$ can be, for example, the rotation subgroup $SO(n)$. Therefore we will have:</p>

\[\begin{gathered} 
(k*_{lifting} f^0)(x, h) = \int_{y \in \mathbb{R}^n}k(h^{-1}(y-x))f^0(y) dy\\
(\psi *_{G} f^1)(x,h) = \int_{y \in \mathbb{R}^n}\int_{h' \in H}\psi(h^{-1}(y-x), h^{-1}h')f^1(y, h')dh'dy
\end{gathered}\]

<p>This gives the output signal $f^2: \mathbb{R}^n \times H \rightarrow \mathbb{R}^k$. This way of convolving is repeated for all subsequent layers until the final aggregation layer, e.g. linear layer, if there is one.</p>

<p>Note that for <em>regular</em> group convolution to be practically feasible, $G$ has to be <strong>finite</strong> or addecuatly supsampled. Some of these limitations can be solved by <em>steerable</em> group convolutions.</p>

<h4 id="steerable-g-cnn">Steerable G-CNN</h4>

<p>First, consider the group representations $\rho_{in}: H \rightarrow \mathbb{R}^{in \times in}$ and $\rho_{out}: H \rightarrow \mathbb{R}^{out \times out}$. To address the aforementioned equivariance problem, $G$-steerable convolution modifies $G$-equivariant convolution with the following three changes:</p>

<ul>
  <li>The input signal becomes $f: \mathbb{R}^n \rightarrow \mathbb{R}^{in}$.</li>
  <li>The kernel $\psi: \mathbb{R}^n \rightarrow \mathbb{R}^{out \times in}$ used must satisfy the following constraint for all $h \in H$: \(\psi(hx) = \rho_{out}(h) \psi(x) \rho_{in}(h^{-1})\)</li>
  <li>Standard convolution only over $\mathbb{R}^n$ and not $G := \mathbb{R}^n \rtimes H$ is performed.</li>
</ul>

<p>To secure kernel $\psi$ has the mentioned property, we precompute a set of non-learnable basis kernels $(\psi_l)_{l=1}^L$ which do have it, and define all other kernels as weighted combinations of the basis kernels, using learnable weights with the same shape as the kernels.</p>

<p>Therefore, the convolution is of the form:</p>

\[(\psi*_{\mathbb{Z}^n}f) (x) = \sum_{y \in \mathbb{Z}^n} \sum_{l=1}^L (w_l ⊙ \psi_l(y))f(x+y)\]

<p>Whenever both $\rho_{in}$ and $\rho_{out}$ can be decomposed into smaller building blocks called <strong>irreducible representations</strong>, equivariance w.r.t. infinite group $G$ is achieved (see Appendix A.1 of <d-cite key="bekkers2024fastexpressivesenequivariant"></d-cite>).</p>

<h4 id="relaxed-g-cnn">Relaxed G-CNN</h4>

<p>The desirability of equivariance in a network depends on the amount of equivariance possessed by the data of interest. To this end, <em>relaxed</em> G-CNN is built on top of a regular G-CNN using a modified (relaxed) kernel consisting of a linear combination of standard G-CNN kernels. Consider $G := \mathbb{Z}^n \rtimes H$. Then, <em>relaxed</em> G-equivariant group convolution is defined as:</p>

<!---
$$
(\psi \tilde{*}_{G} f)(g) = \sum_{h \in G}\psi(g,h)f(h) = \sum_{h \in G}\sum_{l=1}^L w_l(h) \psi_l(g^{-1}h)f(h)
$$

$$
(\psi \tilde{*}_{G} f)(\mathbf{x}, h) = \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h') \psi(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')
$$
-->

\[(\psi \tilde{*}_{G} f)(\mathbf{x}, h) = \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h') \sum_{l=1}^L w_l(h) \psi_l(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')\]

<p>or equivalently as a linear combination of regular group convolutions with different kernels:</p>

\[\begin{aligned}
(\psi \tilde{*}_{G} f)(\mathbf{x}, h) &amp;= \sum_{l=1}^L w_l(h) \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h')  \psi_l(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')\\
 &amp;= \sum_{l=1}^L w_l(h) [(\psi_l *_{G} f)(\mathbf{x}, h)]
\end{aligned}\]

<p>This second formulation makes for a more interpretable visualization, as one can see in the following figure. There, one can observe how a network might learn to downweight the feature maps corresponding to 180 degree rotations, thus breaking rotational equivariance and allowing for different processing of images picturing 6s and 9s.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-relaxed-equivariance/rgconv-480.webp 480w,/assets/img/2024-06-30-relaxed-equivariance/rgconv-800.webp 800w,/assets/img/2024-06-30-relaxed-equivariance/rgconv-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-30-relaxed-equivariance/rgconv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
A visual example of a relaxed lifting convolution (for $L=1$). 
</div>

<!---
 $G$-equivariance of the group convolution arises from kernel $\psi$'s dependence on the composite variable $g^{-1}h$, rather than on both variables $g$ and $h$ separately. This property is broken in relaxed kernels, leading to a loss of equivariance.

Therefore, using relaxed group convolutions allows the network to relax strict symmetry constraints, offering greater flexibility at the cost of reduced equivariance.
-->

<h4 id="relaxed-steerable-g-cnn">Relaxed steerable G-CNN</h4>

<p>Relaxed steerable G-CNN modified steerable G-CNN in a similar manner. Again, let the kernel in convolution be a linear combination of other kernels, such that the weights used depend on the variable of integration, leading to loss of equivariance.</p>

\[(\psi \tilde{*}_{\mathbb{Z}^2} f) (x) = \sum_{y \in \mathbb{Z}^2} \sum_{l=1}^L (w_l(y) ⊙ \psi_l(y))f(x+y)\]

<p>Furthermore, <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite> introduces a regularization term to impose equivariance on both relaxed models mentioned above. In our experiments, however, the best-performing models were those without this term.</p>

<h2 id="methodology">Methodology</h2>
<h3 id="datasets">Datasets</h3>
<h4 id="super-resolution">Super-Resolution</h4>

<p>The data consists of liquid flowing in 3D space and is produced by a high-resolution state-of-the-art simulation hosted by the John Hopkins University <d-cite key="jhtdb"></d-cite> . Importantly, this dataset is forced to be isotropic, i.e. fully equivariant to rotations, by design.</p>

<p>For the experiment, a subset of 50 timesteps are taken, each downsampled from $1024^3$ to $64^3$ and processed into a task suitable for learning. The model is given an input of 3 consecutive timesteps, $t, t+1, t+2$ (which are first downsampled to $16^3$), and is tasked to upsample timestep $t+1$ to $64^3$, see Figure 1 for a visualization.</p>

<p>We use the following $3$ models from <d-cite key="wang2023relaxed"></d-cite>’s experiment on the same dataset in <a href="#Results">Results</a>:</p>

<ul>
  <li>CNN.</li>
  <li>Regular G-CNN.</li>
  <li>Relaxed G-CNN.</li>
</ul>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/r1WCqrL4A.png" alt="Figure 1" style="max-width: 100%;">
  <p>Figure 1: Super Resolution architecture, taken from [1].</p>
</div>

<h4 id="smoke-plume">Smoke Plume</h4>
<!-- For this experiment in [Wang et al. (2022)](#References), a specialized 2D smoke simulation was generated using PhiFlow [(Holl et al., 2020)](#References). -->
<p>This is a synthetic $64 \times 64$ 2D smoke simulation dataset generated by PhiFlow <d-cite key="phiflow"></d-cite>, where dispersion of smoke in a scene starting from an inflow position with a buoyant force is simulated (Figure 2).</p>

<p>The dataset we used has a fixed inflow with buoyant force only pointing in one of the following $4$ directions: upwards, downwards, left, or right. For our experiments we keep the buoyant force the same in all directions such that the data is fully equivariant w.r.t. $90$ degree rotations.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/S1RILeLE0.png" alt="Figure 2" style="max-width: 100%;">
  <p>Figure 2: Example of a Smoke Plume sequence generated by PhiFlow.</p>
</div>

<p>The models trained on this dataset are tasked with predicting the upcoming frame based on the current one. We use the following $2$ models in <a href="#Results">Results</a>:</p>

<ul>
  <li>Relaxed steerable G-CNN from <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite> with relaxed equivariance w.r.t the C4 group.</li>
  <li>Steerable G-CNN from <d-cite key="weiler2021generale2equivariantsteerablecnns"></d-cite> with full equivariance w.r.t the C4 group.</li>
</ul>

<h3 id="training-dynamics-evaluation">Training Dynamics Evaluation</h3>

<p>To assess the training dynamics of a network, we are interested in the final performance and the generalizability of the learned parameters, which are quantified by the final RMSE, and the sharpness of the loss landscape near the final weight-point proposed in <d-cite key="zhao2024improvingconvergencegeneralizationusing"></d-cite>.</p>

<h4 id="sharpness">Sharpness</h4>

<p>To measure the sharpness of the loss landscape after training, we consider changes in the loss averaged over random directions. Let $D$ denote a set of vectors randomly drawn from the unit sphere, and $T$ a set of displacements, i.e. real numbers. Then, the sharpness of the loss $\mathcal{L}$ at a point $w$ is:</p>

\[\phi(w,D,T) = \frac{1}{|D||T|} \sum_{t \in T} \sum_{d \in D} |\mathcal{L}(w+dt)-\mathcal{L}(w)|\]

<p>This definition is an adaptation from the one in <d-cite key="zhao2024improvingconvergencegeneralizationusing"></d-cite>. A sharper loss landscape around the model’s final weights usually implies a greater generalization gap.</p>

<h4 id="hessian-eigenvalue">Hessian Eigenvalue</h4>

<p>Finally, the Hessian eigenvalue spectrum (<d-cite key="park2022visiontransformerswork"></d-cite>) sheds light on both the efficiency and efficacy of neural network training. Negative Hessian eigenvalues indicate a non-convex loss landscape, which can disturb the optimization process, whereas very large eigenvalues indicate training instability, sharp minima and consequently poor generalization.</p>

<h2 id="results">Results</h2>
<p>In this section, we study how equivariance imposed on a network influences the convexity of the loss landscape and generalization, answering all the subquestions posed in <a href="#Introduction">Introduction</a>.</p>

<h3 id="smoke-plume-with-full-equivariance">Smoke Plume with full Equivariance</h3>

<p>First, we examine the training, validation and test RMSE for the E2CNN and Rsteer models on the fully equivariant Smoke Plume dataset.</p>
<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/ByqLXIUEA.png" alt="Figure 5" style="max-width: 100%;">
      <p align="center">Figure 5: Train RMSE curve for rsteer and E2CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/rJ58Q8LVC.png" alt="Figure 6" style="max-width: 100%;">
      <p align="center">Figure 6: Validation RMSE curve for rsteer and E2CNN models</p>
    </td>
  </tr>
</table>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/HyqIXLIEA.png" alt="Figure 7" style="max-width: 100%;">
  <p>Figure 7: Test RMSE for best models, averaged over five seeds</p>
</div>

<p>Figures 5 and 6 show the train and validation RMSE curves. While rsteer and E2CNN perform similarly on the training data, rsteer has lower RMSE on the validation data, indicating better generalization. Figure 7 confirms that rsteer performs best on the test set, consistent with results on the Isotropic Flow dataset in <d-cite key="wang2023relaxed"></d-cite>.</p>

<p>To understand why relaxed equivariant models outperform fully equivariant ones, we examine the sharpness of the loss and the Hessian spectra.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/S14l5XZ4A.png" alt="Figure 10" style="max-width: 100%;">
  <p>Figure 8: Sharpness at early and best epochs for rsteer and E2CNN models. On the equivariant Smokeplume dataset</p>
</div>

<p>Figure 10 shows that the rsteer model has much lower sharpness of the loss landscape compared to E2CNN for both checkpoints. This indicates a lower generalization gap, and thus more effective learning. This matches the lower validation RMSE curve we saw earlier.</p>

<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/rJJQKAyNC.png" alt="Epoch 3" style="max-width: 100%;">
      <p align="center">Figure 9: Hessian spectra at an early epoch for rsteer and E2CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/S1jQF0JN0.png" alt="Epoch best" style="max-width: 100%;">
      <p align="center">Figure 10: Hessian spectra at the best epoch for rsteer and E2CNN models</p>
    </td>
  </tr>
</table>

<p>Figures 9 and 10 show Hessian spectra for the same checkpoints as the previous analysis. Regarding loss landscape flatness, both plots indicate that E2CNN has much larger eigenvalues than rsteer, potentially leading to training instability, less flat minima, and poor generalization for E2CNN.</p>

<p>To evaluate the convexity of the loss landscape, we examine the negative eigenvalues in the Hessian spectra. Neither model shows any negative eigenvalues, suggesting that both E2CNN and rsteer encounter convex loss landscapes. Therefore, convexity does not seem to significantly impact performance in this case.</p>

<h3 id="super-resolution-1">Super Resolution</h3>

<p>Similarly, we also analyze the training dynamics of the superresolution models on the isotropic Super-Resolution dataset.</p>

<p>First, we examine the training and validation MAE curves for the Relaxed Equivariant (RGCNN), Fully Equivariant (GCNN), and non-equivariant (CNN) models (run on 6 different seeds).</p>

<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/HJrVE8IEA.png" alt="Figure 8" style="max-width: 100%;">
      <p align="center">Figure 11: Training MAE curve for RGCNN, GCNN and CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/HJrV48UN0.png" alt="Figure 9" style="max-width: 100%;">
      <p align="center">Figure 12: Validation MAE curve for RGCNN, GCNN and CNN models</p>
    </td>
  </tr>
</table>

<p>Here, we observe that early in the training (around epoch $3$), RGCNN starts outperforming the other two models and keeps this lead until its saturation at around $0.1$ MAE. For this reason, we take a checkpoint for each model on epoch $3$ (early) and on its best epoch (Best), to examine the corresponding sharpness values.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/SkpgREzV0.png" alt="Figure 10" style="max-width: 100%;">
  <p>Figure 13: Sharpness of the loss landscape on the super resolution dataset. Ran over 6 seeds, error bars represent the standard deviation. For early, the third epoch was chosen, while for best the epoch with the best validation loss was chosen.</p>
    
</div>

<p>Figure 13 shows that the relaxed model has the lowest sharpeness in both cases. This indicates that the relaxed steerable GCNN has better generalisability during its training and at its convergence, matching our findings on the previous dataset.</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>We reproduced and extended the relevant findings in <d-cite key="wang2023relaxed"></d-cite> reaffirming the effectiveness of relaxed equivariant models and demonstrating that they are able to outperform fully equivariant models even on perfectly equivariant datasets.</p>

<p>We furthermore investigated the authors’ speculation that this superior performance could be due to relaxed models having enhanced training dynamics. Our experiments empirically support this hypothesis, showing that relaxed models exhibit lower validation error, a flatter loss landscape around the final weights, and smaller Hessian eigenvalues, all of which are indicators of improved training dynamics and better generalization.</p>

<p>Our results suggest that replacing fully equivariant networks with relaxed equivariant networks could be advantageous in all application domains where some level of model equivariance is desired, including those where full equivariance is beneficial. For future research, we should investigate different versions of the relaxed model to find out which hyperparameters, like the number of filter banks, correlate with sharpness. Additionally, the method should be applied to different types of data to see if the same observations can be made there.</p>

<!---
## References

[1] Wang, R., Walters, R., & Smidt, T. E. (2023). Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. arXiv preprint arXiv:2310.02299.

[2] Gruver, N., Finzi, M., Goldblum, M., & Wilson, A. G. (2022). The lie derivative for measuring learned equivariance. arXiv preprint arXiv:2210.02984.

[3] Park, N., & Kim, S. (2022). How do vision transformers work?. arXiv preprint arXiv:2202.06709.

[4] Zhao, B., Gower, R. M., Walters, R., & Yu, R. (2023). Improving Convergence and Generalization Using Parameter Symmetries. arXiv preprint arXiv:2305.13404.

[5] Wang, R., Walters, R., & Yu, R. (2022, June). Approximately equivariant networks for imperfectly symmetric dynamics. In International Conference on Machine Learning (pp. 23078-23091). PMLR.

[6] Holl, P., Koltun, V., Um, K., & Thuerey, N. (2020). phiflow: A differentiable pde solving framework for deep learning via physical simulations. In NeurIPS workshop (Vol. 2).

[7]  Y. Li, E. Perlman, M. Wan, Y. Yang, C. Meneveau, R. Burns, S. Chen, A. Szalay & G. Eyink. "A public turbulence database cluster and applications to study Lagrangian evolution of velocity increments in turbulence". Journal of Turbulence 9, No. 31, 2008.

[8] E. Perlman, R. Burns, Y. Li, and C. Meneveau. "Data Exploration of Turbulence Simulations using a Database Cluster". Supercomputing SC07, ACM, IEEE, 2007.

[9] Super-resolution of Velocity Fields in Three-dimensional Fluid Dynamics: https://huggingface.co/datasets/*******/jhtdb

[10] Weiler, M. and Cesa, G. General E(2)-equivariant steerable CNNs. In Advances in Neural Information Processing Systems (NeurIPS), pp. 14334–14345, 2019b.

[11] Turbulence SuperResolution Replication W&B Report: https://api.wandb.ai/links/*******/hxj68bs1

[12] Equivariance and Training Stability W&B Report: https://api.wandb.ai/links/*******/yu9a85jn

[13] Rotation SmokePlume Replication W&B Report: https://api.wandb.ai/links/*******/hjsmj1u7

[14] `gconv` library for regular group convnets: https://github.com/*****/gconv

[15] Bekkers, E. J., Vadgama, S., Hesselink, R. D., van der Linden, P. A., & Romero, D. W. (2023). Fast, Expressive SE $(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. arXiv preprint arXiv:2310.02970.

[16] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016
-->

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2024-06-30-relaxed-equivariance.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    <div class="container">
      © Copyright 2024
      
      
      . 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


  
</body>
</html>
