<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Learning Embedding Spaces with Metrics via Contrastive Learning |  
    
  
</title>
<meta name="author" content=" ">
<meta name="description" content="GRaM Workshop
">

  <meta name="keywords" content="geometry, machine-learning, generative-models, deep-learning, representation-learning">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/blog/2024/contrast-learning/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  


    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Learning Embedding Spaces with Metrics via Contrastive Learning",
            "description": "",
            "published": "June 13, 2024",
            "authors": [
              
              {
                "author": "Patrick Feeney",
                "authorURL": "https://patrickfeeney.github.io/",
                "affiliations": [
                  {
                    "name": "Tufts University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
            
            
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/instructions/">instructions
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/reviews/">reviews
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Learning Embedding Spaces with Metrics via Contrastive Learning</h1>
        <p></p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#similarity-functions">Similarity Functions</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#cosine-similarity">Cosine Similarity</a>
                      </li>
                    
                      <li>
                        <a href="#negative-arc-length">Negative Arc Length</a>
                      </li>
                    
                      <li>
                        <a href="#negative-euclidean-distance">Negative Euclidean Distance</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#contrastive-losses">Contrastive Losses</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#margin-losses">Margin Losses</a>
                      </li>
                    
                      <li>
                        <a href="#cross-entropy-losses">Cross Entropy Losses</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#case-study-with-contrastive-learning-on-a-hypersphere">Case Study with Contrastive Learning on a Hypersphere</a>
                </div>
                
                  <ul>
                    
                      <li>
                        <a href="#supervised-learning-accuracy">Supervised Learning Accuracy</a>
                      </li>
                    
                      <li>
                        <a href="#supervised-learning-embedding-space">Supervised Learning Embedding Space</a>
                      </li>
                    
                  </ul>
                
              
                <div>
                  <a href="#discussion">Discussion</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <p>Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired properties, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo <d-cite key="he_momentum_2020"></d-cite>, <d-cite key="chen_empirical_2021"></d-cite>, SimCLR <d-cite key="chen_simple_2020"></d-cite>, <d-cite key="chen_big_2020"></d-cite>, and BYOL <d-cite key="grill_bootstrap_2020"></d-cite>, as well as supervised methods such as SupCon <d-cite key="khosla_supervised_2020"></d-cite> and SINCERE <d-cite key="feeney_sincere_2024"></d-cite>.</p>

<p>In contrastive learning, there are two components that determine the constraints on the learned embedding space: the similarity function and the contrastive loss. The similarity function takes a pair of embedding vectors and quantifies how similar they are as a scalar. The contrastive loss determines which pairs of embeddings have similarity evaluated and how the resulting set of similarity values are used to measure error with respect to a task, such as classification. Backpropagating to minimize this error causes a model to learn embeddings that best satisfy the constraints induced by the similarity function and contrastive loss.</p>

<p>This blog post examines how similarity functions and contrastive losses affect the learned embedding spaces. We first examine the different choices for similarity functions and contrastive losses. Then we conclude with a brief case study investigating the effects of different similarity functions on supervised contrastive learning.</p>

<h2 id="similarity-functions">Similarity Functions</h2>

<p>A similarity function \(s(z_1, z_2): \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\) maps a pair of \(d\)-dimensional embedding vectors \(z_1\) and \(z_2\) to a real similarity value, with greater values indicating greater similarity. A temperature hyperparameter \(0 &lt; \tau \leq 1\) is often included, via \(\frac{s(z_1, z_2)}{\tau}\), to scale a similarity function. If the similarity function has a range that is a subset of \(\mathbb{R}\), then \(\tau\) can increase that range. \(\tau\) is omitted for simplicity here.</p>

<h3 id="cosine-similarity">Cosine Similarity</h3>

<p>A common similarity function is cosine similarity:</p>

\[s(z_1, z_2) = \frac{z_1 \cdot z_2}{||z_1|| \cdot ||z_2||}\]

<p>This function measures the cosine of the angle between \(z_1\) and \(z_2\) as a scalar in \([-1, 1]\). Cosine similarity violates the triangle inequality, making it the only similarity function discussed here that is not derived from a distance metric.</p>

<h3 id="negative-arc-length">Negative Arc Length</h3>

<p>The recently proposed negative arc length similarity function <d-cite key="koishekenov_geometric_2023"></d-cite> provides an analogue for cosine similarity that is a distance metric:</p>

\[s(z_1, z_2) = 1 - \frac{\text{arccos}(z_1 \cdot z_2)}{\pi}\]

<p>This function assumes that 
\(||z_1|| = ||z_2|| = 1\)
which is a common normalization <d-cite key="le-khac_contrastive_2020"></d-cite> that restricts the embeddings to a hypersphere. The arc length \(\text{arccos}(z_1 \cdot z_2)\) is a natural choice for comparing such vectors as it is the geodesic distance, or the length of the shortest path between \(z_1\) and \(z_2\) on the hypersphere. Subtracting the arc length converts the distance metric into a similarity function with range \([0, 1]\). Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses.</p>

<h3 id="negative-euclidean-distance">Negative Euclidean Distance</h3>

<p>The negative Euclidean distance similarity function is simply:</p>

\[s(z_1, z_2) = -||z_1 - z_2||_2\]

<p>Euclidean distance measures the shortest path in Euclidean space, making it the geodesic distance when \(z_1\) and \(z_2\) can take any value in \(\mathbb{R}^d\). In this case the similarity function has range \([-\infty, 0]\).</p>

<p>The negative Euclidean distance can also be used with embeddings restricted to a hypersphere, resulting in range \([-2, 0]\). However, this is not the geodesic distance for the hypersphere as the path being measured is inside the sphere. The Euclidean distance will be less than the arc length unless \(z_1 = z_2\), in which case they both equal 0.</p>

<h2 id="contrastive-losses">Contrastive Losses</h2>

<p>A contrastive loss function maps a set of embeddings and a similarity function to a scalar value. Losses are written such that derivatives for backpropagation are taken with respect to the embedding \(z\).</p>

<h3 id="margin-losses">Margin Losses</h3>

<p>The original contrastive loss <d-cite key="chopra_learning_2005"></d-cite> maximizes similarity for examples \(z^+\) and minimizes similarity for examples \(z^-\) until the similarity is below margin hyperparameter \(m\):</p>

\[L(z, z^+) = s(z, z^+); L(z, z^-) = \max( 0, m - s(z, z^-) )\]

<p>The structure of this loss implies that \(z_1\) and \(z_2\) share a class if \(s(z_1, z_2) &lt; m\) and otherwise they do not share a class. This margin hyperparameter can be challenging to tune for efficiency throughout the training process because it needs to be satisfiable but also provide \(z^-\) samples within the margin in order to backpropagate the error.</p>

<p>The triplet loss <d-cite key="schroff_facenet_2015"></d-cite> avoids this by using a margin between similarity values:</p>

\[L(z, z^+, z^-) = \max( 0, s(z, z^+) - s(z, z^-) + m)\]

<p>The triplet loss only updates a network when its loss is positive, so finding triplets satisfying that condition are important for learning efficiency.</p>

<p>Lifted Structured Loss <d-cite key="oh_song_deep_2016"></d-cite> handles this by precomputing similarities for all pairs in a batch then selecting the \(z^-\) with maximal similarity:</p>

\[L(z, z^+) = \max( 0, s(z, z^+) + m - \max [ \max_{z^-} s(z, z^-), \max_{z^-} s(z^+, z^-) ] )\]

<p>The Batch Hard loss <d-cite key="hermans_defense_2017"></d-cite> takes this even further by selecting \(z^+\) with minimal similarity:</p>

\[L(z, z^+) = \max( 0, \min_{z^+} [ s(z, z^+) ] + m - \max_{z^-} [ s(z, z^-) ] )\]

<p>The decision to compute the loss based on comparisons between \(z\), a single \(z^+\), and a single \(z^-\) comes with advantages and disadvantages. These methods can be easier to adapt for learning with varying levels of supervision because complete knowledge of whether similarity should be maximized or minimized for each pair in the dataset is not required. However, these methods also make training efficiently difficult and provide relatively loose constraints on the embedding space.</p>

<h3 id="cross-entropy-losses">Cross Entropy Losses</h3>

<p>A common contrastive loss is the Information Noise Contrastive Estimation (InfoNCE) <d-cite key="oord_representation_2019"></d-cite> loss:</p>

\[L(z, z^+, z^-_1, z^-_2, \ldots, z^-_n) = -\log \frac{ e^{s(z, z^+)} }{ e^{s(z, z^+)} + \sum_{i=1}^n e^{s(z, z^-_i)} }\]

<p>InfoNCE is a cross entropy loss whose logits are similarities for \(z\). \(z^+\) is a single embedding whose similarity with \(z\) should be maximized while \(z^-_1, z^-_2, \ldots, z^-_n\) are a set of \(n\) embeddings whose similarity with \(z\) should be minimized. The structure of this loss implies that \(z_1\) shares a class with \(z_2\) if no other embedding has greater similarity with \(z_1\).</p>

<p>The choice of \(z^+\) and \(z^-\) sets varies across methods. The self-supervised InfoNCE loss chooses \(z^+\) to be an embedding of an augmentation of the input that produced \(z\) and \(z^-\) to be the other inputs and augmentations in the batch. This is called instance discrimination because only augmentations of the same input instance have their similarity maximized.</p>

<p>Supervised methods expand the definition of \(z^+\) to also include embeddings which share a class with \(z\). The expectation of InfoNCE loss over choices of \(z^+\) is used to jointly maximize their similarity to \(z\). The Supervised Contrastive (SupCon) loss <d-cite key="khosla_supervised_2020"></d-cite> uses all embeddings not currently set as \(z\) or \(z^+\) as \(z^-\), including embeddings that share a class with \(z\) and therefore will also be used as \(z^+\). This creates loss terms that would minimize similarity between embeddings that share a class. Supervised Information Noise-Contrastive Estimation REvisited (SINCERE) loss <d-cite key="feeney_sincere_2024"></d-cite> removes embeddings that share a class with \(z\) from \(z^-\), leaving only embeddings with different classes. An additional margin hyperparameter can also be added to these losses <d-cite key="barbano_unbiased_2022"></d-cite>, which allows for interpolation between the original losses and losses with the \(e^{s(z, z^+)}\) term removed from the denominator.</p>

<p>Considering a set of similarities during loss calculation allows the loss to implicitly perform hard negative mining <d-cite key="khosla_supervised_2020"></d-cite>, avoiding the challenge of selecting triplets required by a margin loss. The lack of a margin places strict constraints on the embedding space, as similarities are always being pushed towards the maximum or minimum. This enables analysis of embedding spaces that minimize the loss. For example, InfoNCE and SINCERE losses with cosine similarity are minimized by embedding spaces with clusters of inputs mapped to single points (maximizing similarity) that are uniformly distributed on the unit sphere (minimizing similarity) <d-cite key="wang_understanding_nodate"></d-cite>.</p>

<h2 id="case-study-contrastive-learning-on-a-hypersphere">Case Study: Contrastive Learning on a Hypersphere</h2>

<p>Many modern contrastive learning techniques build off of the combination of cosine similarity and cross entropy losses. However, few papers have explored changing similarity functions and losses outside of the context of a more complex model.</p>

<p>Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses. This change is motivated by the desire to use the geodesic distance on the embedding space, which in this case is a unit hypersphere. We investigate whether replacing cosine similarity with negative arc length similarity can improve performance with the SINCERE loss, which is supervised, and how each similarity affects the learned embedding space.</p>

<h3 id="supervised-learning-accuracy">Supervised Learning Accuracy</h3>

<p>We utilize the methodology of Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite> to evaluate if the results of Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> generalize to supervised cross entropy losses. Specifically, we train models with SINCERE loss and each similarity function then evaluate the models with nearest neighbor classifiers on the test set.</p>

<table>
  <tr>
   <td>
   </td>
   <td colspan="2">
<strong>CIFAR-10</strong>
   </td>
   <td colspan="2">
<strong>CIFAR-100</strong>
   </td>
  </tr>
  <tr>
   <td>
<strong>Similarity</strong>
   </td>
   <td>
<strong>1NN</strong>
   </td>
   <td>
<strong>5NN</strong>
   </td>
   <td>
<strong>1NN</strong>
   </td>
   <td>
<strong>5NN</strong>
   </td>
  </tr>
  <tr>
   <td>Cosine
   </td>
   <td>95.88
   </td>
   <td>95.91
   </td>
   <td>76.23
   </td>
   <td>76.13
   </td>
  </tr>
  <tr>
   <td>Negative Arc Length
   </td>
   <td>95.66
   </td>
   <td>95.65
   </td>
   <td>75.81
   </td>
   <td>76.41
   </td>
  </tr>
</table>

<p>We find no statistically significant difference based on the 95% confidence interval of the accuracy difference <d-cite key="foody_classification_2009"></d-cite> from 1,000 iterations of test set bootstrapping. This aligns with the results in Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite>, which used different loss functions with cosine similarity and found a similar lack of statistically significant results across choices of supervised contrastive cross entropy losses. This suggests that supervised learning accuracy is similar across choices of reasonable similarity functions and contrastive losses.</p>

<h3 id="supervised-learning-embedding-space">Supervised Learning Embedding Space</h3>

<p>We also visualize the learned embedding space for each CIFAR-10 model. For each test set image, the similarity value is plotted for the closest training set image that shares a class (“Target”) and that does not share a class (“Noise”). This visualizes the 1-nearest neighbor decision process. Both similarity functions are plotted for each model, with the title denoting the similarity function used during training.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with cosine similarity maximizes the similarity to target images well. There are a small number of noise images with near maximal similarity, but the majority are below 0.3 cosine similarity. Interestingly, the peaks seen in the noise similarity reflects the fact that individual classes will have different modes of their noise histograms.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp"></source>
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with negative arc length similarity does a better job of forcing target similarity values very close to 1 negative arc length similarity, but also has a notable number of target similarities near 0.5 negative arc length similarity. The noise distribution also reflects the fact that individual classes have different modes for their noise histograms, but in this case the modes are spread across more similarity values. Notably the peak for the horse class is very close to the max similarity due to a high similarity to the dog class, although they are still separated enough from the target similarities to not have an impact on accuracy.</p>

<h3 id="discussion">Discussion</h3>

<p>The choice of similarity function clearly has an effect on the learned embedding space despite a lack of statistically significant changes in accuracy. The cosine similarity histogram most cleanly aligns with the intuition that contrastive losses should be maximizing and minimizing similarities. In contrast, the negative arc length similarity histogram suggests similarity minimization is sacrificed for very consistent maximization, producing small differences in similarity between some target classes and noise examples. I hypothesize that this change in behavior arises from the difference in similarity function behavior with small angles described in Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite>.</p>

<p>These differences in the learned embedding spaces could affect performance on downstream tasks such as transfer learning. I hypothesize that the larger difference between target and noise similarities seen in the cosine similarity model would improve transfer learning performance, similar to the improvement of SINCERE over SupCon loss reported in Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite>.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2024-06-13-contrast-learning.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    <div class="container">
      © Copyright 2024
      
      
      . 
      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


  
</body>
</html>
