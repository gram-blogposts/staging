<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-22T16:23:06+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>GRaM Workshop
</subtitle><entry><title type="html">Equivariant Diffusion for Molecule Generation in 3D using Consistency Models</title><link href="http://localhost:4000/blog/2024/equivariant_diffusion/" rel="alternate" type="text/html" title="Equivariant Diffusion for Molecule Generation in 3D using Consistency Models" /><published>2024-07-18T00:00:00+02:00</published><updated>2024-07-18T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/equivariant_diffusion</id><content type="html" xml:base="http://localhost:4000/blog/2024/equivariant_diffusion/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this blog post, we discuss the paper <a href="https://arxiv.org/abs/2203.17003">“Equivariant Diffusion for Molecule Generation in 3D”</a> <d-cite key="hoogeboom2022equivariant"></d-cite>, 
which first introduced 3D molecule generation using diffusion models. Their Equivariant Diffusion Model (EDM) also
incorporated an Equivariant Graph Neural Network (EGNN) architecture, effectively grounding the model with inductive
priors about the symmetries in 3D space. EDM demonstrated strong improvement over other (non-diffusion) generative 
methods for molecules at the time and inspired many further influential works in the field <d-cite key="anstine2023generative"></d-cite><d-cite key="corso2023diffdock"></d-cite><d-cite key="igashov2024equivariant"></d-cite><d-cite key="xu2023geometric"></d-cite>.</p>

<p>Most diffusion models are unfortunately bottle-necked by the sequential denoising process, which can be slow and 
computationally expensive <d-cite key="song2023consistency"></d-cite>. Hence, we also introduce <a href="https://arxiv.org/abs/2303.01469">“Consistency Models”</a> <d-cite key="song2023consistency"></d-cite>
and demonstrate that an EDM can generate samples up to <em>24x faster</em> in this paradigm with as little as a single step.
However, we unfortunately found the quality of samples generated by the consistency model to be much worse 
than from the original EDM.</p>

<!---
Using Consistency Models can be a step towards enabling much larger GNN backbones, eventually observing 
similar scaling effects as other domains including language <d-cite key="brown2020language"></d-cite><d-cite key="kaplan2020scaling"></d-cite><d-cite key="touvron2023llama"></d-cite> 
or image and video generation <d-cite key="liu2024sora"></d-cite><d-cite key="ramesh2022hierarchical"></d-cite><d-cite key="rombach2022high"></d-cite><d-cite key="saharia2022photorealistic"></d-cite>.
Such improvement has been demonstrated in training Graph Neural Networks (GNN) <d-cite key="sriram2022towards"></d-cite>,
and scaling model parameters to take advantage of increasingly larger compute availability, is generally known to improve 
model performance <d-cite key="dosovitskiy2020image"></d-cite><d-cite key="kaplan2020scaling"></d-cite><d-cite key="krizhevsky2012imagenet"></d-cite>.
--->

<!--- 260 words --->

<p><br /></p>

<h4 id="briefly-on-equivariance-for-molecules">Briefly on Equivariance for molecules</h4>

<p>Equivariance is a property of certain functions, which ensures that their output transforms in a predictable manner under 
collections of transformations. This property is valuable in molecular modeling, where it can be used to ensure that the 
properties of molecular structures are consistent with their symmetries in the real world. Specifically, we are interested 
in ensuring that structure is preserved in the representation of a molecule under three types of transformations: 
<em>translation, rotation, and reflection</em>.</p>

<p>Formally, we say that a function \(f\) is equivariant to the action of a group \(G\) if:</p>

\[\begin{align}
T_g(f(x)) = f(S_g(x))
\end{align}\]

<p>for all \(g \in G\), where \(S_g,T_g\) are linear representations related to the group element \(g\) <d-cite key="serre1977linear"></d-cite>.</p>

<p>The three transformations: <em>translation, rotation, and reflection</em>, form the Euclidean group \(E(3)\), which is the group of all aforementioned isometries in three-dimensional space, for which \(S_g\) and 
\(T_g\) can be represented by a translation \(t\) and an orthogonal matrix $R$ that rotates or reflects coordinates.</p>

<p>A function \(f\) is then equivariant to a rotation or reflection \(R\) if transforming its input results in an equivalent transformation of its output <d-cite key="hoogeboom2022equivariant"></d-cite>:</p>

\[\begin{align}
Rf(x) = f(Rx)
\end{align}\]

<p><br /></p>

<!--- 330 words --->

<h4 id="introducing-equivariant-graph-neural-networks-egnns">Introducing Equivariant Graph Neural Networks (EGNNs)</h4>
<p>Molecules can very naturally be represented with graph structures, where the nodes are atoms and edges their bonds. 
The features of each atom, such as its element type or charge can be encoded into an embedding \(\mathbf{h}_i \in \mathbb{R}^d\) 
alongside with the atoms 3D position \(\mathbf{x}_i \in \mathbb{R}^3\).</p>

<p>To learn and operate on such structured inputs, Graph Neural Networks (GNNs) <d-cite key="zhou2021graphneuralnetworksreview"></d-cite> 
have been developed, falling under the message passing paradigm <d-cite key="gilmer2017neuralmessagepassingquantum"></d-cite>. 
This architecture consists of several layers, each of which updates the representation of each node, using the information 
in nearby nodes.</p>

<style>
.custom-img-size {
    width: 50%; /* Adjust width as needed */
    height: auto; /* Maintains aspect ratio */
    display: block;
    margin-left: auto;
    margin-right: auto;
}

.custom-img-size-2 {

    width: 60%; /* Adjust width as needed */
    height: auto; /* Maintains aspect ratio */
    display: block;
    margin-left: auto;
    margin-right: auto;
}
</style>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/message_passing-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/message_passing-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/message_passing-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/message_passing.png" class="img-fluid rounded z-depth-1 custom-img-size" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 1: visualization of a message passing network</figcaption>
        </figure>
    </div>
</div>

<p>The previously mentioned \(E(3)\) equivariance property of molecules can be injected as an inductive prior into to the model 
architecture of a message passing graph neural network, resulting in an \(E(3)\) EGNN. This property improves generalisation <d-cite key="hoogeboom2022equivariant"></d-cite> and also beats similar non-equivariant Graph Convolution Networks on 
the molecular generation task <d-cite key="verma2022modular"></d-cite>.</p>

<p>The EGNN is built with <em>equivariant</em> graph convolution layers (EGCLs):</p>

\[\begin{align}
\mathbf{x}^{l+1},\mathbf{h}^{l+1}=EGCL[ \mathbf{x}^l, \mathbf{h}^l ]
\end{align}\]

<p>An EGCL layer can be formally defined by:</p>

<div align="center">

$$
\begin{align}
\mathbf{m}_{ij} = \phi_e(\mathbf{h}_i^l, \mathbf{h}_j^l, d^2_{ij})
\end{align}
$$

$$
\begin{align}
\mathbf{h}_i^{l+1} = \phi_h\left(\mathbf{h}_i^l, \sum_{j \neq i} \tilde{e}_{ij} \mathbf{m}_{ij}\right) 
\end{align}
$$

$$
\begin{align}
\mathbf{x}_i^{l+1} = \mathbf{x}_i^l + \sum_{j \neq i} \frac{\mathbf{x}_i^l \mathbf{x}_j^l}{d_{ij} + 1} \phi_x(\mathbf{h}_i^l, \mathbf{h}_j^l, d^2_{ij})
\end{align}
$$

</div>

<p>where \(h_l\) represents the feature $h$ at layer \(l\), \(x_l\) represents the coordinate at layer \(l\) and 
\(d_{ij}= ||x_i^l-x^l_j||_2\) is the Euclidean distance between nodes \(v_i\) and \(v_j\).</p>

<p>A fully connected neural network is used to learn the functions \(\phi_e\), \(\phi_x\), and \(\phi_h\). 
At each layer, a message \(m_{ij}\) is computed from the previous layer’s feature representation. 
Using the previous feature and the sum of these messages, the model computes the next layer’s feature representation.</p>

<p>This architecture then satisfies translation and rotation equivariance. Notably, the messages depend on the distance 
between the nodes and these distances are not changed by isometric transformations.</p>

<!--- 600 words --->

<h2 id="equivariant-diffusion-models-edm">Equivariant Diffusion Models (EDM)</h2>
<p>This section introduces diffusion models and describes how their predictions can be made \(E(3)\) equivariant. 
The categorical properties of atoms are already invariant to \(E(3)\) transformations, hence, we are only 
interested in enforcing this property on the sampled atom positions.</p>

<h3 id="what-are-diffusion-models">What are Diffusion Models?</h3>

<p>Diffusion models <d-cite key="sohl2015deep"></d-cite><d-cite key="ho2020denoising"></d-cite> are inspired by the principles 
of diffusion in physics, and model the flow of a data distribution to pure noise over time. A neural network is then 
trained to learn a reverse process that reconstructs samples on the data distribution from pure noise.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 2: The Markov process of forward and reverse diffusion <d-cite key="ho2020denoising"></d-cite></figcaption>
        </figure>
    </div>
</div>

<p>The “forward” noising process can be parameterized by a Markov process <d-cite key="ho2020denoising"></d-cite>, 
where transition at each time step \(t\) adds Gaussian noise with a variance of \(\beta_t \in (0,1)\):</p>

\[\begin{align}
q\left( x_t \mid x_{t-1} \right) := \mathcal{N}\left( x_t ; \sqrt{1-\beta_t} x_{t-1}, \beta_t \mathbf{I} \right) 
\end{align}\]

<p>The whole Markov process leading to time step \(T\) is given as a chain of these transitions:</p>

\[\begin{align}
q\left( x_1, \ldots, x_T \mid x_0 \right) := \prod_{t=1}^T q \left( x_t \mid x_{t-1} \right)
\end{align}\]

<p>The “reverse” process transitions are unknown and need to be approximated using a neural network parametrized by \(\theta\):</p>

\[\begin{align}
p_\theta \left( x_{t-1} \mid x_t \right) := \mathcal{N} \left( x_{t-1} ; \mu_\theta \left( x_t, t \right), \Sigma_\theta \left( x_t, t \right) \right)
\end{align}\]

<p>Because we know the dynamics of the forward process, the variance \(\Sigma_\theta \left( x_t, t \right)\) at time \(t\) is 
known and can be fixed to \(\beta_t \mathbf{I}\).</p>

<p>The predictions then only need to obtain the mean \(\mu_\theta \left( x_t, t \right)\), given by:</p>

\[\begin{align}
\mu_\theta \left( x_t, t \right) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta\_t}{\sqrt{1 - \bar{\alpha}\_t}} \epsilon\_\theta \left( x_t, t \right) \right)
\end{align}\]

<p>where \(\alpha_t = \Pi_{s=1}^t \left( 1 - \beta_s \right)\).</p>

<p>Hence, we can directly predict \(x_{t-1}\) from \(x_{t}\) using the network \(\theta\):</p>

\[\begin{align}
x_{t-1} = \frac{1}{\sqrt{1 - \beta_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta \left( x_t, t \right) \right) + \sqrt{\beta_t} v_t
\end{align}\]

<p>where \(v_T \sim \mathcal{N}(0, \mathbf{I})\) is a sample from the pure Gaussian noise.</p>

<!--- 850 words --->

<h3 id="enforcing-e3-equivariant-diffusion">Enforcing E(3) equivariant diffusion</h3>
<!--- check rotations and reflections or jsut rotations? --->
<p>Equivariance to rotations and reflections effectively means that if any orthogonal rotation matrix \(\mathbf{R}\) is 
applied to a sample \(\mathbf{x}_t\) at any given time step \(t\), we should still generate a correspondingly rotated 
“next best sample” \(\mathbf{R}\mathbf{x}_{t+1}\) at time \(t+1\).</p>

<p>In other words, the likelihood of this next best sample does not depend on the molecule’s rotation and the probability 
distribution for each transition in the Markov Chain is hence roto-invariant:</p>

\[\begin{align}
p(y|x) = p(\mathbf{R}y|\mathbf{R}x)
\end{align}\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure class="custom-figure">
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus.png" class="img-fluid rounded z-depth-1 custom-image" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        </figure>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure class="custom-figure">
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut.png" class="img-fluid rounded z-depth-1 custom-image" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        </figure>
    </div>
</div>
<div class="row">
    <div class="col text-center mt-3">
        <p>Figure 3: Examples of 2D roto-invariant distributions</p>
    </div>
</div>

<style>
    .custom-figure .custom-image {
        height: 250px; /* Set a fixed height for both images */
        width: auto; /* Maintain aspect ratio and adjust width accordingly */
        max-width: 100%; /* Ensure the image doesn't exceed the container width */
    }
</style>

<p>Such an invariant distribution composed with an equivariant invertible function results in another invariant distribution <d-cite key="kohler2020equivariant"></d-cite>. 
Furthermore, if \(x \sim p(x)\) is invariant to a group, and the transition probabilities of a Markov chain \(y \sim p(y|x)\) 
are equivariant, then the marginal distribution of \(y\) at any time step \(t\) is also invariant to that group <d-cite key="xu2022geodiff"></d-cite>.</p>

<p>Since the underlying EGNN already ensures  this equivariance, the remaining constraint can easily be achieved by 
setting the initial sampling distribution to something roto-invariant, such as a simple mean zero Gaussian with a 
diagonal covariance matrix, as illustrated in Figure 3 (left).</p>

<p><em>Translation equivariance</em> requires a few tricks. It has been shown, that it is impossible to have non-zero distributions 
invariant to translations <d-cite key="satorras2021en"></d-cite>. Intuitively, the translation invariance property 
means that any point \(\mathbf{x}\) results in the same assigned \(p(\mathbf{x})\), leading to a uniform distribution, 
which, if stretched over an unbounded space, would be approaching zero-valued probabilities thus not integrating 
to one.</p>

<p>The EDM authors bypass this with a clever trick of always re-centering the generated samples to have center of gravity at
\(\mathbf{0}\) and further show that these \(\mathbf{0}\)-centered distributions lie on a linear subspace that can reliably be used 
for equivariant diffusion <d-cite key="hoogeboom2022equivariant"></d-cite><d-cite key="xu2022geodiff"></d-cite>.</p>

<!---
We hypothesize that, intuitively, moving a coordinate from e.g. 5 to 6 on any given axis is the same as moving from 
8 to 9. But EDM predicts the actual atom positions, not a relative change, hence the objective needs to adjusted. 
By constraining the model to this "subspace" of options where the center of the molecule is always at $\mathbf{0}$, 
the absolute positions are effectively turned into relative ones w.r.t. to the center of the molecule, hence the model 
can now learn relationships that do not depend on the absolute position of the whole molecule in 3D space.
--->

<!--- (below) 1100 words --->

<h3 id="how-to-train-the-edm">How to train the EDM?</h3>

<p>The training objective of diffusion-based generative models amounts to <strong>“maximizing the log-likelihood of the 
sample on the original data distribution.”</strong></p>

<p>During training, a diffusion model learns to approximate the parameters of a posterior distributions at the next time
step by minimizing the KL divergence between this estimate and the ground truth, which is equivalent
objective to minimizing the negative log likelihood.</p>

\[\begin{align}
L_{vlb} := L_{t-1} := D_{KL}(q(x_{t-1}|x_{t}, x_{0}) \parallel p_{\theta}(x_{t-1}|x_{t}))
\end{align}\]

<p>The EDM adds a caveat that the predicted distributions must be calibrated to have center of gravity at \(\mathbf{0}\), 
in order to ensure equivariance.</p>

<p>Using the KL divergence loss term with the EDM model parametrization simplifies the loss function to:</p>

\[\begin{align}
\mathcal{L}_t = \mathbb{E}_{\epsilon_t \sim \mathcal{N}_{x_h}(0, \mathbf{I})} \left[ \frac{1}{2} w(t) \| \epsilon_t - \hat{\epsilon}_t \|^2 \right]
\end{align}\]

<p>where 
\(w(t) = \left(1 - \frac{\text{SNR}(t-1)}{\text{SNR}(t)}\right)\) and \(\hat{\epsilon}_t = \phi(z_t, t)\).</p>

<p>The EDM authors found that the model performs best with a constant \(w(t) = 1\), thus effectively simplifying 
the loss function to an MSE. Since coordinates and categorical features are on different scales, it was also 
found that scaling the inputs before inference and then rescaling them back also improves performance.</p>

<!--- 1250 words --->

<h2 id="consistency-models">Consistency Models</h2>

<p>As previously mentioned, diffusion models are bottlenecked by the sequential denoising process <d-cite key="song2023consistency"></d-cite>.
Consistency Models reduce the number of steps during de-noising up to just a single step, significantly speeding up 
this costly process, while allowing for a controlled trade-off between speed and sample quality.</p>

<h3 id="modelling-the-noising-process-as-an-sde">Modelling the noising process as an SDE</h3>

<p>Song et al. <d-cite key="song2021score"></d-cite> have shown that the noising process in diffusion can be described with a Stochastic Differential Equation (SDE)
transforming the data distribution \(p_{\text{data}}(\mathbf{x})\) in time:</p>

\[\begin{align}
d\mathbf{x}_t = \mathbf{\mu}(\mathbf{x}_t, t) dt + \sigma(t) d\mathbf{w}_t
\end{align}\]

<p>Where \(t\) is the time-step, \(\mathbf{\mu}\) is the drift coefficient, \(\sigma\) is the diffusion coefficient,
and \(\mathbf{w}_t\) is the stochastic component denoting standard Brownian motion. This stochastic component effectively
represents the iterative adding of noise to the data in the forward diffusion process and dictates the shape of the final
distribution at time \(T\).</p>

<p>Typically, this SDE is designed such that \(p_T(\mathbf{x})\) at the final time-step \(T\) is close to a tractable Gaussian.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot.png" class="img-fluid rounded z-depth-1 custom-img-size-2" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 4: Illustration of a bimodal distribution evolving to a Gaussian over time</figcaption>
        </figure>
    </div>
</div>

<!--- 1400 words --->

<h3 id="existence-of-the-pf-ode">Existence of the PF ODE</h3>

<p>This SDE has a remarkable property, that a special ODE exists, whose trajectories sampled at \(t\) are distributed
according to \(p_t(\mathbf{x})\) <d-cite key="song2023consistency"></d-cite>:</p>

\[\begin{align}
d\mathbf{x}_t = \left[ \mathbf{\mu}(\mathbf{x}_t, t) - \frac{1}{2} \sigma(t)^2 \nabla \log p_t(\mathbf{x}_t) \right] dt
\end{align}\]

<p>This ODE is dubbed the Probability Flow (PF) ODE by Song et al. <d-cite key="song2023consistency"></d-cite> and corresponds to the different view of diffusion
manipulating probability mass over time we hinted at in the beginning of the section.</p>

<p>A score model \(s_\phi(\mathbf{x}, t)\) can be trained to approximate \(\nabla log p_t(\mathbf{x})\) via score matching <d-cite key="song2023consistency"></d-cite>.
Since we know the parametrization of the final distribution \(p_T(\mathbf{x})\) to be a standard Gaussian parametrized 
with \(\mathbf{\mu}=0\) and \(\sigma(t) = \sqrt{2t}\), this score model can be plugged into the equation (16) and the 
expression reduces itself to an empirical estimate of the PF ODE:</p>

\[\begin{align}
\frac{dx_t}{dt} = -ts\phi(\mathbf{x}_t, t)
\end{align}\]

<p>With \(\mathbf{\hat{x}}_T\) sampled from the specified Gaussian at time \(T\), the PF ODE can be solved backwards in time 
to obtain a solution trajectory mapping all points along the way to the initial data distribution at time \(\epsilon\) very close to zero.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 5: Solution trajectories of the PF ODE. <d-cite key="dosovitskiy2020image"></d-cite></figcaption>
        </figure>
    </div>
</div>

<p>Given any off-the-shelf ODE solver (e.g. Euler) and a trained score model \(s_\phi(\mathbf{x}, t)\), we can solve this PF ODE.
The time horizon \([\epsilon, T]\) is discretized into sub-intervals for improved performance <d-cite key="karras2022elucidating"></d-cite>. A solution trajectory, denoted \(\\{\mathbf{x}_t\\}\), 
is then given as a finite set of samples \(\mathbf{x}_t\) for every discretized time-step \(t\) between \(\epsilon\) and \(T\).</p>

<!--- 1600 words --->

<h3 id="consistency-function">Consistency Function</h3>

<p>Given a solution trajectory \({\mathbf{x}_t}\), we define the <em>consistency function</em> as:</p>

\[\begin{align}
f: (\mathbf{x}_t, t) \to \mathbf{x}_{\epsilon}
\end{align}\]

<p>In other words, for every pair (\(\mathbf{x}_t\), \(t\)), a consistency function always outputs a corresponding datapoint 
at time $\epsilon$, which will be very close to the original data distribution.</p>

<p>Importantly, this function has the property of <em>self-consistency</em>: i.e. its outputs are consistent for arbitrary pairs of
\((x_t, t)\) that lie on the same PF ODE trajectory. Hence, we have</p>

\[f(x_t, t) = f(x_{t'}, t') \text{ for all } t, t' \in [\epsilon, T]\]

<p>The goal of a <em>consistency model</em>, denoted by \(f_\theta\), is to estimate this consistency function \(f\) from data by
being enforced with this self-consistency property during training.</p>

<!--- 1700 words --->

<!---
### Boundary Condition & Function Parametrization

For any consistency function $f(\cdot, \cdot)$, we must have $f(x_\epsilon, \epsilon) = x_\epsilon$, i.e., $f(\cdot, 
\epsilon)$ being an identity function. This constraint is called the _boundary condition_ <d-cite key="song2023consistency"></d-cite>.

The boundary condition has to be met by all consistency models, as we have hinted before that much of the training relies
on the assumption that $p_\epsilon$ is borderline identical to $p_0$. However, it is also a big architectural
constraint on consistency models.

For consistency models based on deep neural networks, there are two ways to implement this boundary condition almost
for free <d-cite key="song2023consistency"></d-cite>. Suppose we have a free-form deep neural network $F_\theta (x, t)$ whose output has the same dimensionality
as $x$.

1.) One way is to simply parameterize the consistency model as:

$$
f_\theta (x, t) =
\begin{cases}
x & t = \epsilon \\
F_\theta (x, t) & t \in (\epsilon, T]
\end{cases} \\
\qquad \text{(27)}
$$

2.) Another method is to parameterize the consistency model using skip connections, that is:

$$
f_\theta (x, t) = c_{\text{skip}} (t) x + c_{\text{out}} (t) F_\theta (x, t) \qquad \text{(28)}
$$

where $c_{\text{skip}} (t)$ and $c_{\text{out}} (t)$ are differentiable functions such that $c_{\text{skip}} (\epsilon) = 1$,
and $c_{\text{out}} (\epsilon) = 0$.

This way, the consistency model is differentiable at $t = \epsilon$ if $F_\theta (x, t)$, $c_{\text{skip}} (t)$, $c_{\text{out}} (t)$
are all differentiable, which is critical for training continuous-time consistency models.

In our work, we utilize the latter methodology in order to satisfy the boundary condition.
--->

<h3 id="sampling">Sampling</h3>

<p>With a fully trained consistency model \(f_\theta(\cdot, \cdot)\), we can generate new samples by simply sampling from the initial
Gaussian \(\hat{x_T}\) \(\sim \mathcal{N}(0, T^2I)\) and propagating this through the consistency model to obtain
samples on the data distribution \(\hat{x_{\epsilon}}\) \(= f_\theta(\hat{x_T}, T)\) with as little as one diffusion step.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 6: Visualization of PF ODE trajectories for molecule generation in 3D. <d-cite key="fan2023ecconf"></d-cite></figcaption>
        </figure>
    </div>
</div>

<!--- 1750 words --->

<h3 id="training-consistency-models">Training Consistency Models</h3>

<p>Consistency models can either be trained by “distillation” from a pre-trained diffusion model, or in “isolation” as a standalone generative model from scratch. In the context of our work, we focused only on the latter because the distillation approach has a hard requirement of using a pretrained score based diffusion. 
In order to train in isolation we need to leverage the following unbiased estimator:</p>

\[\begin{align}
\nabla \log p_t(x_t) = - \mathbb{E} \left[ \frac{x_t - x}{t^2} \middle| x_t \right]
\end{align}\]

<p>where \(x \sim p_\text{data}\) and \(x_t \sim \mathcal{N}(x; t^2 I)\).</p>

<p>That is, given \(x\) and \(x_t\), we can estimate \(\nabla \log p_t(x_t)\) with \(-(x_t - x) / t^2\).
This unbiased estimate suffices to replace the pre-trained diffusion model in consistency distillation
when using the Euler ODE solver in the limit of \(N \to \infty\) <d-cite key="song2023consistency"></d-cite>.</p>

<p>Song et al. <d-cite key="song2023consistency"></d-cite> justify this with a further theorem in their paper and show that the consistency training objective (CT loss)
can then be defined as:</p>

\[\begin{align}
\mathcal{L}_{CT}^N (\theta, \theta^-) &amp;= \mathbb{E}[\lambda(t_n)d(f_\theta(x + t_{n+1} \mathbf{z}, t_{n+1}), f_{\theta^-}(x + t_n \mathbf{z}, t_n))]
\end{align}\]

<p>where \(\mathbf{z} \sim \mathcal{N}(0, I)\).</p>

<p>Crucially, \(\mathcal{L}(\theta, \theta^-)\) only depends on the online network \(f_\theta\), and the target network
\(f_{\theta^-}\), while being completely agnostic to diffusion model parameters \(\phi\).</p>

<!--- ~1900 words --->

<h2 id="experiments">Experiments</h2>

<p>We replicate the original EDM set-up and evaluate on the QM9 dataset <d-cite key="ramakrishnan2014quantum"></d-cite>. 
Due to computational constraints and the demonstrational nature of this blogpost, we only trained models for 
130 epochs with the default hyperparameter settings given by the original EDM implementation <d-cite key="hoogeboom2022equivariant"></d-cite>
to illustrate the trade-offs in speed and quality of samples.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model / Sampling Time (seconds)</strong></th>
      <th><strong>Mean</strong></th>
      <th><strong>STD</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Default EDM</td>
      <td>0.6160</td>
      <td>0.11500</td>
    </tr>
    <tr>
      <td>Consistency Model (single step)</td>
      <td>0.0252</td>
      <td>0.00488</td>
    </tr>
  </tbody>
</table>

<div class="caption" style="text-align: center;">
    Table 1: EDM and Consistency Model inference speed
</div>

<p>As expected, we observed in table 1., that the consistency model in single-step mode is significantly faster than the EDM, 
providing up to a <em>24x speed-up</em> averaged over 5 sampling runs. This number represents the time it takes the model to generate
a sample on the data distribution from a pure Gaussian noise input, excluding other computational overheads shared by 
both models equally, such as logging.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model / Metric</strong></th>
      <th><strong>Training NLL</strong></th>
      <th><strong>Validation NLL</strong></th>
      <th><strong>Best Cross-Validated Test NLL</strong></th>
      <th><strong>Best Atom Stability</strong></th>
      <th><strong>Best Molecule Stability</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Default EDM</td>
      <td>2.524</td>
      <td>-30.066</td>
      <td>-17.178</td>
      <td>0.873</td>
      <td>0.196</td>
    </tr>
    <tr>
      <td>Consistency Model (single step)</td>
      <td>2.482</td>
      <td>94176</td>
      <td>80363</td>
      <td>0.19</td>
      <td>0</td>
    </tr>
    <tr>
      <td>Consistency Model (multi-step)</td>
      <td>2.484</td>
      <td>166264</td>
      <td>179003</td>
      <td>0.12</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<div class="caption" style="text-align: center;">
    Table 2: EDM and Consistency Model results on the QM9 dataset after 130 epochs.
</div>

<p>We observed that the consistency models converge on the training set with similar rate as the regular EDM, even
achieving slightly lower training NLLs. However, they completely fail to generalize to the validation and test sets with
much lower atom stability (the proportion of atoms that have the right valency) and no molecule stability (the proportion 
of generated molecules for which all atoms are stable). These results are surprisingly poor, given that
the dataset is not particularly complicated, and consistency models have already shown promising results on 
images <d-cite key="song2023consistency"></d-cite> and reportedly, shows competitive results on QM9 as well <d-cite key="fan2023ecconf"></d-cite>.</p>

<p>To improve these results, we attempted to use multi-step sampling, which  should in theory allow us to replicate results 
close to the EDM with the same number of sampling steps. However, we observed no such improvement in our experiments. 
We tested multiple different amounts of steps and report results for 100, which performed best overall. 
Oddly, the multi-step sampling actually yields worse results than the single-step sampling most of the time,
which is highly unexpected and requires further investigation.</p>

<p>It should also be noted that the default EDM with more training is capable of achieving results much better than what we 
report in table 2. However, it still comfortably outperforms all consistency model variations on all metrics using equal
amounts of compute.</p>

<h2 id="discussion">Discussion</h2>

<p>Consistency models are able to reduce the number of steps during sampling up to just a single step, significantly 
speeding up the sampling process. We were able to successfully demonstrate this and train an EDM as a consistency 
model in isolation, achieving nearly identical training loss with up to 24x faster sampling times. However, using 
the single-step sampling only achieves up to 19% atom stability in best case scenario, compared with the default 
EDM which consistently reaches 87% or much more with further training. We suspect that a model trained in this 
set-up might be too prone to overfitting and struggles with generalization to anything outside the training data
distribution, compared to sequential de-noising predictions of the EDM, which are more robust by design.</p>

<p>Using multi-step sampling should in theory yield competitive results, but we observed no such improvement. 
Since it cannot be conclusively ruled out that this was caused by a bug in our multi-step sampling code, we hope 
to continue investigating if the consistency model paradigm can reliably be used for molecule generation in the future
and show more competitive results as previous works suggest is possible <d-cite key="fan2023ecconf"></d-cite>.</p>]]></content><author><name>Martin Sedlacek *</name></author><category term="equivariance," /><category term="diffusion," /><category term="molecule" /><category term="generation," /><category term="consistency" /><category term="models" /><summary type="html"><![CDATA[Introduction to the seminal papers &quot;Equivariant Diffusion for Molecule Generation in 3D&quot; and &quot;Consistency Models&quot; with an adaptation fusing the two together for fast molecule generation.]]></summary></entry><entry><title type="html">Towards Equivariant Adaptation of Large Pretrained Models</title><link href="http://localhost:4000/blog/2024/towards-equivariant-adaptation/" rel="alternate" type="text/html" title="Towards Equivariant Adaptation of Large Pretrained Models" /><published>2024-07-11T00:00:00+02:00</published><updated>2024-07-11T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/towards-equivariant-adaptation</id><content type="html" xml:base="http://localhost:4000/blog/2024/towards-equivariant-adaptation/"><![CDATA[<p>Deep learning has witnessed tremendous growth in the past decade. Still, as we strive for more nuanced understanding and performance improvements, one challenge emerges clearly: how do we ensure our models understand data transformations? Enter equivariance, an idea that can help our networks maintain consistent behaviour with data transformations. But with the rise of large pretrained models, how do we make them equivariant without changing their architecture or retraining the model from scratch with data augmentation? In this blogpost, we delve into ideas presented
in the paper “Equivariant Adaptation of Large Pretrained Models”<d-cite key="mondal2023equivariant"></d-cite> to answer this question.</p>

<h2 id="what-is-equivariance">What is Equivariance?</h2>
<p>Equivariant networks <d-cite key="cohen2016group,worrall2019deep,bronstein2021geometric"></d-cite> are deep neural networks that maintain consistent behaviour when input data undergo transformations like rotation, scaling, or translation. In simpler terms, if we rotate an image of a cat, an equivariant network would still recognize it as a cat! Another example of this would be segmentation maps on images. If we rotate an image, the segmentation map should rotate in the same way to maintain consistency.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image1-480.webp 480w,/assets/img/equivariant-adaptation/image1-800.webp 800w,/assets/img/equivariant-adaptation/image1-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image2-480.webp 480w,/assets/img/equivariant-adaptation/image2-800.webp 800w,/assets/img/equivariant-adaptation/image2-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image2.png" class="img-fluid rounded z-depth-1 fixed-size-img2" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Equivariant tasks. Instance segmentation requires segmentation maps to be consistent with the input image transformations and 
    classification requires the network to recognize the same object in different orientations.
</div>

<p>The beauty of this is that such networks lead to more accurate, robust predictions and need fewer samples to train – this is great in theory but hard to implement in practice, especially for large pretrained models whose equivariant counterparts are not trivial to design or are very expensive to re-train from scratch. These massive models pretrained on the entire internet are extremely good at solving and reasoning about different tasks and are called <code class="language-plaintext highlighter-rouge">foundation models</code> <d-cite key="bommasani2021opportunities"></d-cite>. Despite having such capabilities, foundation models are not naturally equivariant and usually don’t handle transformations well. (see the GPT-4 example below) Our goal is to incorporate the benefits of equivariance in existing foundation models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image3-480.webp 480w,/assets/img/equivariant-adaptation/image3-800.webp 800w,/assets/img/equivariant-adaptation/image3-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image3.png" class="img-fluid rounded z-depth-1 fixed-size-img3" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image4-480.webp 480w,/assets/img/equivariant-adaptation/image4-800.webp 800w,/assets/img/equivariant-adaptation/image4-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    ChatGPT-4 for image parsing. The model is accurate in extracting text from 'straight' images, but it fails to do so for 'inverted' images.
</div>

<h2 id="decoupling-equivariance-from-architecture-with-canonicalization">Decoupling Equivariance from Architecture with Canonicalization</h2>

<p>A recent alternative to designing equivariant networks was proposed by Kaba et al. <d-cite key="kaba2023equivariance"></d-cite>.  It suggests that instead of changing the network architecture to incorporate equivariance, why not first learn to transform the input data into a ‘standard’ format, also known as <code class="language-plaintext highlighter-rouge">canonical form</code>. This way, our task prediction network can work on this standardized format, ensuring consistency. This process involves adding an additional inexpensive network called the <code class="language-plaintext highlighter-rouge">canonicalization network</code>, which learns to standardize the input. The primary network that learns to solve the task based on the standardized input is called the <code class="language-plaintext highlighter-rouge">prediction network</code>. In this particular formulation, achieving equivariance requires only ensuring that the canonicalization process is invariant to the transformation of the input. This means no matter which orientation you see the input, the canonicalization process should always bring it back to the same canonical orientation. This is achieved by using a shallow and cheap equivariant architecture for the canonicalization network. (see <d-cite key="kaba2023equivariance"></d-cite> for more details)</p>

<p>The beauty of this approach lies in how the canonicalization network separates the equivariance requirement from the core prediction network architecture. This means that you have the flexibility to employ any powerful pretrained large neural network for the main prediction task.</p>

<p>Sounds straightforward? Well, it has a hitch.</p>

<p>The <strong>main challenge</strong> is ensuring the canonicalization network ‘plays nice’ with the prediction network. For example, the canonicalization network can output orientations that hurt the training of the prediction network, leading to poor task performance. This becomes more important when the prediction network is pretrained on a certain dataset. For instance, if the canonicalization network transforms all images to be upside-down, but our pretrained prediction network wasn’t trained on upside-down images, the whole system falls apart. So, it’s vital that the canonicalization network outputs orientations of the data that is in-distribution for the pretrained prediction network.</p>

<h2 id="learning-to-predict-the-correct-orientation-for-the-pretrained-network">Learning to Predict the Correct Orientation for the Pretrained Network</h2>

<p>The magic lies in designing our canonicalization function not just to transform data but to do so while being aware of how our prediction model was initially trained. The key is ensuring that the data being transformed (or standardized) is done to align with what the pretrained prediction model expects. Mathematically, the goal is to bring the predicted out-of-distribution orientations to the distribution of orientations the pretrained prediction network has seen.</p>

<h1 id="enter-the-canonicalization-prior">Enter the Canonicalization Prior</h1>

<p>In simple terms, it’s a guiding force ensuring that our canonicalization function behaves and produces output that the pretrained prediction network would expect and appreciate. 
We leverage the idea that our data can provide hints on the ‘typical’ transformations it undergoes. By encoding this into a prior, one can guide our canonicalization function to produce transformed data that’s not just standardized but also aligned with what the prediction network was trained on.</p>

<p>While mathematical and intricate, this entire process can be boiled down to ensuring that the large pretrained prediction network always looks at in-distribution samples. 
This results in a highly robust model that can confidently handle varied transformations in the input data, giving accurate predictions every time.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image5-480.webp 480w,/assets/img/equivariant-adaptation/image5-800.webp 800w,/assets/img/equivariant-adaptation/image5-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Training and inference with canonicalization prior. The canonicalization function learns to output the canonical orientations seen in the dataset during training by minimising KL between the orientation distributions of predicted and pretraing dataset (prior regularization). During inference, transformed data is brought back to the canonical orientation by the canonicalization proces
</div>

<h2 id="results-at-a-glance">Results at a Glance</h2>

<p>This section highlights the effectiveness of the approach for image classification and instance segmentation tasks. Additional results and experiments including point cloud classification and part segmentation are detailed in <d-cite key="mondal2023equivariant"></d-cite>.</p>

<h1 id="image-classification">Image Classification</h1>

<p>The authors select Vision Transformer (ViT) <d-cite key="dosovitskiy2020image"></d-cite> and ResNet-50 <d-cite key="he2016deep"></d-cite> as pretrained<d-footnote>Pretrained on ImageNet<d-cite key="deng2009imagenet"></d-cite>.</d-footnote> prediction network for image classification and \(C_8\) group, i.e., eight discrete rotations (multiples of 45\(^\circ\)) as the set of known transformations. The objective is to make the prediction networks equivariant and robust to these transformations, as an example, on CIFAR-100 dataset <d-cite key="krizhevsky2009learning"></d-cite>.</p>

<p>The authors compare different fine-tuning setups. First, <strong>Vanilla</strong> indicates the standard fine-tuning on the downstream dataset. <strong>C8-Aug.</strong> indicates fine-tuning on the downstream
dataset and \(C_8\) group data augmentations. <strong>LC</strong> is the learned canonicalization approach proposed in Kaba et. al. <d-cite key="kaba2023equivariance"></d-cite>. 
<strong>Prior-Regularized LC</strong> is the learned canonicalization approach with prior regularization (as described in above sections, proposed in <d-cite key="mondal2023equivariant"></d-cite>). The evaluation includes reporting the performance of the models on the test set of <strong>CIFAR-100</strong> and an augmented version, <strong>CIFAR-100 [C8]</strong>, where each sample is augmented with every transformation of \(C_8\) group.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image6-480.webp 480w,/assets/img/equivariant-adaptation/image6-800.webp 800w,/assets/img/equivariant-adaptation/image6-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fine-tuning performance of ResNet-50 and ViT on CIFAR-100 dataset. Vanilla has the highest performance on CIFAR-100, but it is the worst on CIFAR-100 [C8] which indicates poor
    robustness. Prior-Regularized LC preserves the performance on CIFAR-100 and outperforms other baselines on CIFAR-100 [C8].
</div>

<h1 id="instance-segmentation">Instance Segmentation</h1>
<p>Furthermore, the authors scale this idea to large foundation models like the Segment Anything Model (SAM) <d-cite key="kirillov2023segment"></d-cite> and make it robust to rotations while having a nominal increase in the number of parameters and inference speed.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/equivariant-adaptation/image7-480.webp 480w,/assets/img/equivariant-adaptation/image7-800.webp 800w,/assets/img/equivariant-adaptation/image7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/equivariant-adaptation/image7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Predicted masks from the Segment Anything Model (SAM) <d-cite key="kirillov2023segment"></d-cite> showcasing both the original model and prior-regularized equivariant adaptation 
    for 90-degrees counter-clockwise rotated input images taken from the COCO 2017 dataset <d-cite key="lin2014microsoft"></d-cite>. The approach makes SAM equivariant to the
    group of 90-degrees rotations while only requiring 0.3% extra parameters and modestly increasing the inference time by 7.3%.
</div>

<p>Finally, to facilitate the ideas discussed on equivariant adaptation of large-scale models, an open-source package <a href="https://github.com/arnab39/equiadapt">Equiadapt</a> is available from the authors.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In the ever-evolving world of AI and deep learning, it is critical to ensure models are robust and aware of symmetries. By learning to smartly transform our input data so that they are in the correct orientation for the pretrained models, we can create large-scale models that are powerful and aware of data transformations, bringing us a step closer to AI systems that understand the world as we do. As research into scaling continues, the fusion of large foundational models with equivariant adaptation techniques such as the one presented in this blogpost has the potential to emerge as a fundamental approach in enhancing the consistency and reliability of AI systems.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;&quot;&quot;, &quot;affiliations&quot;=&gt;{&quot;name&quot;=&gt;nil}}</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[How do you make your foundation model equivariant and robust to known transformations without re-training from scratch?]]></summary></entry><entry><title type="html">Correct, Incorrect and Extrinsic Equivariance</title><link href="http://localhost:4000/blog/2024/extrinsic/" rel="alternate" type="text/html" title="Correct, Incorrect and Extrinsic Equivariance" /><published>2024-07-07T00:00:00+02:00</published><updated>2024-07-07T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/extrinsic</id><content type="html" xml:base="http://localhost:4000/blog/2024/extrinsic/"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially or implicitly in the domain. We propose the definitions of correct, incorrect, and extrinsic equivariance, which describe the relationship between model symmetry and problem symmetry. We show that imposing extrinsic equivariance can improve the model’s performance. We also provide the lower error bound analysis of incorrect equivariance, quantitatively showing the degree to which the symmetry mismatch will impede learning.</p>

<!-- In this work, we
present a general theory for such a situation. We propose pointwise definitions of
correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study
the impact of various degrees of incorrect or extrinsic symmetry on model error.
We prove error lower bounds for invariant or equivariant networks in classification
or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results
in three different environments.

Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. -->

<h2 id="introduction">Introduction</h2>

<p>Equivariant Networks have shown great benefit for improving sample efficiency.</p>

<!-- <p align="center">
  <img src="assets/img/2024-07-07-extrinsic/equi.gif" width="450px">
</p> -->

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/equi-480.webp 480w,/assets/img/2024-07-07-extrinsic/equi-800.webp 800w,/assets/img/2024-07-07-extrinsic/equi-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/equi.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>For example, consider the above position estimation task. We can use a rotationally equivariant network which will automatically generalize to different rotations of the same input. However, a <strong>perfect top-down image</strong> is normally required in order to model the problem symmetry as transformations of the input image.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/non_equi-480.webp 480w,/assets/img/2024-07-07-extrinsic/non_equi-800.webp 800w,/assets/img/2024-07-07-extrinsic/non_equi-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/non_equi.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>Such assumption can be easily violated in the real world where there could be a fixed background or a tilted view angle.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/obj_trans_vs_img_trans-480.webp 480w,/assets/img/2024-07-07-extrinsic/obj_trans_vs_img_trans-800.webp 800w,/assets/img/2024-07-07-extrinsic/obj_trans_vs_img_trans-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/obj_trans_vs_img_trans.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<!-- <p align="center">
  <img src="img/obj_trans_vs_img_trans.gif" width="480px">
</p> -->

<p>In these cases, the transformation of the object will be different from that of the image</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/obj_trans-480.webp 480w,/assets/img/2024-07-07-extrinsic/obj_trans-800.webp 800w,/assets/img/2024-07-07-extrinsic/obj_trans-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/obj_trans.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<!-- <p align="center">
  <img src="img/obj_trans.gif" width="520">
</p> -->

<p>Such object transformation will be hard to model and an equivariant network will not directly apply.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/img_trans-480.webp 480w,/assets/img/2024-07-07-extrinsic/img_trans-800.webp 800w,/assets/img/2024-07-07-extrinsic/img_trans-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/img_trans.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<!-- <p align="center">
  <img src="img/img_trans.gif" width="520">
</p> -->

<p>However, we can still use an equivariant network that encodes the image-wise symmetry instead. In this work, we study what will happen if we use equivariant networks under such symmetry-mismatch scenarios.</p>

<!-- we propose to use an equivariant network that encodes the image-wise symmetry instead to help modeling the object-wise symmetry. We call this **extrinsic equivariance**. -->

<h2 id="correct-incorrect-and-extrinsic-equivariance">Correct, Incorrect, and Extrinsic Equivariance</h2>

<p>We first define <strong>correct</strong>, <strong>incorrect</strong>, and <strong>extrinsic</strong> equivariance, three different relationships between model symmetry and problem symmetry.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/task-480.webp 480w,/assets/img/2024-07-07-extrinsic/task-800.webp 800w,/assets/img/2024-07-07-extrinsic/task-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/task.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<!-- <p align="center">
  <img src="assets/img/2024-07-07-extrinsic/task.gif" width="200">
</p> -->

<p>Consider a classification task where the model needs to classify the blue and orange points in the plane.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/correct-480.webp 480w,/assets/img/2024-07-07-extrinsic/correct-800.webp 800w,/assets/img/2024-07-07-extrinsic/correct-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/correct.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        <div class="caption">
          If we enforce reflection symmetry across the horizontal axis, the transformed data under reflection will have the same color as the original data, so the model preserves the problem symmetry, and we call it correct equivariance.
        </div>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/incorrect-480.webp 480w,/assets/img/2024-07-07-extrinsic/incorrect-800.webp 800w,/assets/img/2024-07-07-extrinsic/incorrect-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/incorrect.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        <div class="caption">
          If we enforce rotation symmetry by pi, the transformed data under the rotation will have different color as the original data, so the model will be forced to generate wrong answers, and we call it incorrect equivariance.
        </div>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/extrinsic-480.webp 480w,/assets/img/2024-07-07-extrinsic/extrinsic-800.webp 800w,/assets/img/2024-07-07-extrinsic/extrinsic-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/extrinsic.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        <div class="caption">
          If we enforce scale symmetry, the transformed data under scaling will be outside of the input distribution shown in the gray ring, so we call it extrinsic equivariance.
        </div>
    </div>
</div>

<h2 id="extrinsic-equivairance-helps-learning">Extrinsic Equivairance Helps Learning</h2>

<p>We show that extrinsic equivariance can helps learning. Our hypothesis is that extrinsic equivariance can makes it easier for the network to generate the decision boundary.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/decision_boundary-480.webp 480w,/assets/img/2024-07-07-extrinsic/decision_boundary-800.webp 800w,/assets/img/2024-07-07-extrinsic/decision_boundary-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/decision_boundary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/exp-480.webp 480w,/assets/img/2024-07-07-extrinsic/exp-800.webp 800w,/assets/img/2024-07-07-extrinsic/exp-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<!-- <p align="center">
  <img src="img/exp.png" width="800">
</p> -->

<p>We test our proposal in robotic manipulation (and in other domains, please see the paper <d-cite key="iclr23"></d-cite>) using SO(2)-Equivariant SAC <d-cite key="iclr22"></d-cite>, but the observation is taken from a camera with a tilted view angle. This tilted view angle makes the symmetry extrinsic because a rotated image will be out-of-distribution. We show that the extrinsic equivariant methods <span style="color: #3b3bff">(blue)</span> significantly outperform the unconstrained baselines.</p>

<h2 id="lower-bound-of-incorrect-equivariance">Lower Bound of Incorrect Equivariance</h2>

<p>We futher analyze the lower bound of error caused by incorrect equivariance. Consider a digit classification task where we use a \(D_2\)-invariant network to classify digits.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/mixed_equi-480.webp 480w,/assets/img/2024-07-07-extrinsic/mixed_equi-800.webp 800w,/assets/img/2024-07-07-extrinsic/mixed_equi-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/mixed_equi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>For digit 2, a \(\pi\)-rotation symmetry is correct equivariance, while a vertical flip symmetry is incorrect (as it transforms 2 into a 5). For digit 3, a \(\pi\)-rotation symmetry is extrinsic (the rotated digit is out-of-distribution), while vertical flip symmetry is correct equivariance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/orbit-480.webp 480w,/assets/img/2024-07-07-extrinsic/orbit-800.webp 800w,/assets/img/2024-07-07-extrinsic/orbit-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/orbit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>Let us focus on an image \(x\) of digit 2 first, we can get the orbit \(Gx\) with respect to the group \(G=D_2\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/majority_label-480.webp 480w,/assets/img/2024-07-07-extrinsic/majority_label-800.webp 800w,/assets/img/2024-07-07-extrinsic/majority_label-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/majority_label.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>Inside the orbit, two elements will have correct equivariance, and two other elements will have incorrect equivariance. If we assume the probability \(p\) of all four images are identical, we can calculate the minimum error of a \(D_2\)-invariant network inside the orbit \(Gx\) as \(k(Gx)=\frac{1}{2}\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/all_digits-480.webp 480w,/assets/img/2024-07-07-extrinsic/all_digits-800.webp 800w,/assets/img/2024-07-07-extrinsic/all_digits-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/all_digits.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>Doing the same calculation for all digits, we can calculate the minimum error as the mean of all \(k(Gx)\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-07-07-extrinsic/theory-480.webp 480w,/assets/img/2024-07-07-extrinsic/theory-800.webp 800w,/assets/img/2024-07-07-extrinsic/theory-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-07-07-extrinsic/theory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>Formally, we define \(k(Gx)\) as the <strong>total dissent</strong> of the orbit \(Gx\), which is the integrated probability density of the elements in the orbit having a different label than the majority label. The invariant classification is then lower bounded by the integral of rotal dissent over the fundamental domain \(F\).</p>

<p>We also analyze the lower bounds of invariant and equivariant regression, please see them in the paper <d-cite key="neurips23"></d-cite>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this work, we both theoretically and empirically study the use of equivariant networks under a mismatch between the model symmetry and the problem symmetry. We define correct, incorrect, and extrinsic equivariance, and show that while incorrect equivariance will create an error lower bound, extrinsic equivariance can aid learning. For more information, please checkout our full papers <d-cite key="iclr23"></d-cite> and <d-cite key="neurips23"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[What happens if we use an equivariant network when there is a mismatch between the model symmetry and the problem symmetry?]]></summary></entry><entry><title type="html">Accelerating Equivariant Graph Neural Networks with JAX</title><link href="http://localhost:4000/blog/2024/egnn-jax/" rel="alternate" type="text/html" title="Accelerating Equivariant Graph Neural Networks with JAX" /><published>2024-06-30T00:00:00+02:00</published><updated>2024-06-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/egnn-jax</id><content type="html" xml:base="http://localhost:4000/blog/2024/egnn-jax/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This blogpost serves as a tutorial for the fast and scalable training of Equivariant Neural Networks, which are slower to train due to the handling of more complex data. We propose leveraging JAX’s capabilities to address these challenges. In this work, we analyze the benefits of utilizing JAX and provide a detailed breakdown of the steps needed to achieve a fully JIT-compatible framework. This approach not only enhances the performance of Neural Networks but also opens the door for future research in developing fully equivariant transformers using JAX.</p>

<p>This blogpost serves three purposes:</p>
<ol>
  <li>Explain the ideas of equivariance in networks while also explaining some of the methods used.</li>
  <li>Give an overview of the performance tests conducted on the two approaches.</li>
  <li>Provide an overview of reproduction results for the Equivariant Graph Neural Network.</li>
</ol>

<p><em>Editor’s Note</em> : You can find the code used in this tutorial in the following <a href="https://github.com/Ioana-Simion/egnn-jax">repository</a>.</p>

<h3 id="recap-of-equivariance">Recap of Equivariance</h3>

<p>As equivariance is prevalent in the natural sciences <d-cite key="balaban1985applications"></d-cite><d-cite key="gupta2013wtf"></d-cite><d-cite key="miller1995wordnet"></d-cite><d-cite key="thölke2022equivariant"></d-cite><d-cite key="maron2020learning"></d-cite>, it makes sense to utilize them for our neural networks, especially given the evidence suggesting that it significantly improves performance through increasing the network’s generalizability <d-cite key="bronstein2021geometric"></d-cite>. One large area within this subfield of deep learning is learning 3D translation and rotation symmetries, where various techniques have been created such as Graph Convolutional Neural Networks <d-cite key="cohen2016group"></d-cite> and Tensor Field Networks <d-cite key="thomas2018tensor"></d-cite>.</p>

<p>Following these works, more efficient implementations have emerged, with the first being the Equivariant Graph Neural Network (EGNN) <d-cite key="satorras2021en"></d-cite>. Based on the GNN <d-cite key="gori2005new"></d-cite><d-cite key="kipf2018neural"></d-cite><d-cite key="bruna2014spectral"></d-cite>, which follows a message passing scheme, it innovates by inputting the relative squared distance between two coordinates into the edge operation and to make the output equivariant, updates the coordinates of the nodes per layer. This specific method bypasses any expensive computations/approximations relative to other, similar methods while retaining high performance levels, making it preferable compared to most other GNN architectures.</p>

<p>More recently, transformer architectures have been utilized within the field of equivariant models. While not typically used for these types of problems due to how they were originally developed for sequential tasks <d-cite key="devlin2019bert"></d-cite><d-cite key="baevski2020wav2vec"></d-cite>, recent work has suggested their effectiveness for tackling such issues <d-cite key="thölke2022equivariant"></d-cite><d-cite key="fuchs2020se"></d-cite><d-cite key="liao2023equiformer"></d-cite>. This is possible through the incorporation of domain-related inductive biases, allowing them to model geometric constraints and operations. In addition, one property of transformers is that they assume full adjacency by default, which is something that can be adjusted to better match the local connectivity of GNN approaches. These additions further increase the complexity of the framework, strongly highlighting the need for a more efficient alternative.</p>

<h3 id="equivariant-graph-neural-networks">Equivariant Graph Neural Networks</h3>

<p>Given a set of \(T_g\) transformations on a set \(X\) (\(T_g: X \rightarrow X\)) for an element \(g \in G\), where \(G\) is a group acting on \(X\), a function \(\varphi: X \rightarrow Y\) is equivariant to \(g\) iff an equivalent transformation \(S_g: Y \rightarrow Y\) exists on its output space \(Y\), such that:</p>

<div style="text-align: center;">
$$
\varphi(T_g(x)) = S_g(\varphi(x)). \qquad \qquad \text{(Equation 1)}
$$
</div>

<p>In other words, translating the input set \(T_g(x)\) and then applying \(\varphi(T_x(x))\) on it yields the same result as first running the function \(y = \varphi(x)\) and then applying an equivalent translation to the output \(S_g(y)\) such that Equation 1 is fulfilled and \(\varphi(x+g) = \varphi(x) + g\) <d-cite key="satorras2021en"></d-cite>.</p>

<h3 id="equivariant-graph-neural-networks-1">Equivariant Graph Neural Networks</h3>

<p>For a given graph \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\) with nodes \(v_i \in \mathcal{V}\) and edges
\(=e_{ij} \in \mathcal{E}\), we can define a graph convolutional layer as the following:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, a_{ij}), \qquad \qquad \text{(Equation 2)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 3)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i), \qquad \qquad \text{(Equation 4)}
$$
</div>

<p>where \(\mathbf{h}\_i^l \in \mathbb{R}^{nf}\) is the nf-dimensional embedding of node \(v_i\) at layer \(l\), \(a_{ij}\) are the edge attributes, \(\mathcal{N}\_i\) is the set of neighbors of node \(v_i\), and \(\varphi_e\) and \(\varphi_h\) are the
edge and node operations respectively, typically approximated by Multilayer Perceptrons (MLPs).</p>

<p>To make this implementation equivariant, <d-cite key="satorras2021en"></d-cite> introduced the inputting of the relative squared distances between two points and updating of the node positions at each time step, leading to the following formulae:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, ||\mathbf{x}\_i^l - \mathbf{x}\_j^l||^2, a_{ij}), \qquad \qquad \text{(Equation 5)}
$$
$$
x_i^{l+1} = x_i^l + C \sum_{j \neq i} (\mathbf{x}\_i^l - \mathbf{x}\_j^l) \varphi_x(\mathbf{m}\_{ij}), \qquad \qquad \text{(Equation 6)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 7)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i). \qquad \qquad \text{(Equation 8)}
$$
</div>

<p>This idea of using the distances during computation forms an important basis in these architectures, as it is a simple yet effective way to impose geometric equivariance within a system.</p>

<h3 id="why-jax">Why JAX?</h3>

<p>JAX is a high-performance numerical computing library that provides several advantages over traditional frameworks. By default, JAX automatically compiles library calls using just-in-time (JIT) compilation, ensuring optimal execution. It utilizes XLA-optimized kernels, allowing for sophisticated algorithm expression without leaving Python. Furthermore, JAX also excels in utilizing multiple GPU or TPU cores and automatically evaluating gradients through differentiation transformations, making it ideal for high-compute scenarios.</p>

<p>This is partially caused by how JAX often uses pointers to reference elements in memory instead of copying them, which has several advantages:</p>

<ul>
  <li><strong>Efficiency:</strong> Through pointers, JAX avoids the unnecessary copying of data, resulting in faster computations and lower memory usage.</li>
  <li><strong>Functionally Pure:</strong> Since JAX functions are pure (i.e., contain no side effects), using pointers ensures that the data is not accidentally modified, maintaining the integrity of all operations.</li>
  <li><strong>Automatic Differentiation:</strong> JAX’s efficient gradient computation relies on its functional programming model. Pointers allow JAX to track operations and dependencies without data duplication.</li>
</ul>

<hr />

<h2 id="experiments">Experiments</h2>

<h3 id="n-body-dataset">N-Body dataset</h3>

<p>In this dataset, a dynamical system consisting of 5 atoms is modeled in 3D space. Each atom has a positive and negative charge, a starting position and a starting velocity. The task is to predict the position of the particles after 1000 time steps. The movement of the particles follow the rules of physics: Same charges repel and different charges attract. The task is equivariant in the sense, that translating and rotating the 5-body system on the input space is the same as rotating the output space.</p>

<h3 id="qm9-dataset">QM9 dataset</h3>

<p>This dataset consists of small molecules and the task is to predict a chemical property. The atoms of the molecules have 3 dimensional positions and each atom is one hot encoded to the atom type. This task is an invariant task, since the chemical property does not depend on position or rotation of the molecule. In addition, larger batch sizes were also experimented with due to smaller sizes causing bottlenecks during training.</p>

<h3 id="data-preparation">Data Preparation</h3>

<p>Here, we introduce a straightforward method for preprocessing data from a PyTorch-compatible format to one suitable for JAX. Our approach handles node features, edge attributes, indices, positions, and target properties. The key step would be converting the data to jax numpy (jnp) arrays, ensuring compatibility with JAX operations. For usage examples, refer to <code class="language-plaintext highlighter-rouge">qm9\utils.py</code> or <code class="language-plaintext highlighter-rouge">n_body\utils.py</code>.</p>

<h3 id="training">Training</h3>

<p>We now address the key differences and steps in adapting the training loop, model saving, and evalution functions for JAX (refer to <code class="language-plaintext highlighter-rouge">main_qm9.py</code> and <code class="language-plaintext highlighter-rouge">nbody_egnn_trainer.py</code>).</p>

<p>JAX uses a functional approach to define and update the model parameters. We use <code class="language-plaintext highlighter-rouge">jax.jit</code> via the <code class="language-plaintext highlighter-rouge">partial</code> decorator for JIT compilation, which ensures that our code runs efficiently by compiling the functions once and then executing them multiple times. We also utilize <code class="language-plaintext highlighter-rouge">static_argnames</code> as decorators for the loss and update functions, which specify the arguments to treat as static. By doing this, JAX can assume these arguments will not change and optimize the function accordingly.</p>

<p>Moreover, model initialization in JAX requires knowing the input sizes beforehand. We extract features to get their shapes and initialize the model using <code class="language-plaintext highlighter-rouge">model.init(jax_seed, *init_feat, max_num_nodes)</code>. This seed initializes the random number generators, which then produces the random number sequences used in virtually all processes. Also, this seed is created using the <code class="language-plaintext highlighter-rouge">jax.random.PRNGKey</code> function, which is used for all random operations. This ensures that they are all reproducible and can be split into multiple independent keys if needed.</p>

<p>The loss function is called through <code class="language-plaintext highlighter-rouge">jax.grad(loss_fn)(params, x, edge_attr, edge_index, pos, node_mask, edge_mask, max_num_nodes, target)</code>. <code class="language-plaintext highlighter-rouge">jax.grad</code> is a powerful tool in JAX for automatic differentiation, allowing us to compute gradients of scalar-valued functions with respect to their inputs.</p>

<hr />

<h2 id="evaluation">Evaluation</h2>

<h3 id="speed-comparison">Speed Comparison</h3>

<p>The EGNN authors <d-cite key="satorras2021en"></d-cite> note that while their approach is more computationally efficient, it is still slower than Linear and Graph Neural Networks. Thus, the aim is to preserve the properties of the model while also providing a fast alternative. We demonstrate the effectivity of building a JAX-based alternative by comparing the forward pass times of the original EGNN implementation with our version. The results of which can be seen in the following graph:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-12 text-center">
        <b>Figure 1.</b> EGNN speed comparison between JAX EGNN (ours) and the PyTorch EGNN <d-cite key="satorras2021en"></d-cite>. Benchmark results represent a single forward pass averaged over 100 tries. The batch sizes used here are 32, 64 and 128.
    </div>
</div>

<p>One notable observation is the consistency in performance. The JAX implementation exhibits less variance in duration values, resulting in more stable and predictable performances across runs. This is particularly important for large-scale applications where the performance consistency can impact overall system reliability and efficiency.</p>

<p>Additionally, as the number of nodes increases, the JAX implementation maintains a less steep increase in computation time compared to PyTorch. This indicates better scalability, making the JAX-based EGNN more suitable for handling larger and more complex graphs.</p>

<h3 id="reproduction-results">Reproduction Results</h3>

<p>To show that our implementation generally preserves the performance and characteristics of the base model, we perform a reproduction of the results reported in <d-cite key="satorras2021en"></d-cite> and display the results for several properties in both experiments. They can be found in the table below.</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th style="text-align: center">EGNN</th>
      <th style="text-align: center">EGNN (Ours)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>QM9 (ε<sub>HOMO</sub>) (meV)</td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">75</td>
    </tr>
    <tr>
      <td>N-Body (Position MSE)</td>
      <td style="text-align: center">0.0071</td>
      <td style="text-align: center">0.0025</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1.</strong> Reproduction results comparing <d-cite key="satorras2021en"></d-cite> with our JAX implementation.</p>

<p>Here, our EGNN implementation outperforms the original author’s implementation on the N-Body dataset. Moreover, other publicly available EGNN implementations also achieve a similar performance as our model on our data. We therefore argue that the increased performance stems from how the dataset is generated slightly differently compared to the one presented in <d-cite key="satorras2021en"></d-cite>.</p>

<hr />

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>Our EGNN comparisons reveal that the JAX-based model is faster than traditional PyTorch implementations, benefiting from JIT compilation to optimize runtime performance. In addition, we also demonstrate that these JAX-based models also achieve comparable performances to the aforementioned PyTorch ones, meaning that they are generally more suitable for equivariance tasks.</p>

<p>We also adapted the model for two well-known datasets: the QM9 dataset for molecule property prediction and the N-body dataset for simulating physical systems. This demonstrates the flexibility and potential of our JAX framework as a strong foundation for further development. Our work suggests that the JAX-based EGNN framework can be effectively extended to other applications, facilitating future research and advancements in equivariant neural networks and beyond.</p>

<p>You can find the code to our experiments <a href="https://github.com/Ioana-Simion/egnn-jax">here</a>.</p>

<hr />]]></content><author><name>Ioana Simion</name></author><category term="equivariance" /><summary type="html"><![CDATA[A Tutorial on How to Make EGNNs Faster]]></summary></entry><entry><title type="html">Effect of equivariance on training dynamics</title><link href="http://localhost:4000/blog/2024/relaxed-equivariance/" rel="alternate" type="text/html" title="Effect of equivariance on training dynamics" /><published>2024-06-30T00:00:00+02:00</published><updated>2024-06-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/relaxed-equivariance</id><content type="html" xml:base="http://localhost:4000/blog/2024/relaxed-equivariance/"><![CDATA[<p><em>Group Equivariant Convolutional Network</em> (G-CNN) has gained significant traction in recent years owing to their ability to generalize the property of CNNs being equivariant to translations in convolutional layers. With equivariance, the network is able to exploit groups of symmetries and a direct consequence of this is that it generally needs less data to perform well. However, incorporating such knowledge into the network may not always be advantageous, especially when the data itself does not exhibit full equivariance. To address this issue, the G-CNN was modified, introducing <em>relaxed group equivariant CNNs</em> (RG-CNN). Such modified networks adaptively learn the degree of equivariance imposed on the network, i.e. enabling it to operate on a level between full equivariance and no equivariance.
Surprisingly, for rotational symmetries on fully equivariant data, <d-cite key="wang2023relaxed"></d-cite> found that a G-CNN exhibits poorer performance compared to a RG-CNN. This is a surprising result because a G-CNN, i.e. a fully equivariant network, is designed to perform well on fully equivariant data. Possibly the training dynamics benefit from relaxing of the equivariance constraint. To investigate this, we use the framework described in <d-cite key="park2022visiontransformerswork"></d-cite> for measuring convexity and flatness using the Hessian spectra.</p>

<p>Inspired by the aforementioned observations, this blog post aims to answer the question: <strong>How does the equivariance imposed on a network affect its training dynamics?</strong> We identify the following subquestions:</p>

<ol>
  <li>How does equivariance imposed on a network influence generalization?</li>
  <li>How does equivariance imposed on a network influence the convexity of the loss landscape?</li>
</ol>

<p>We tackle these subquestions by analyzing trained models to investigate their training dynamics.</p>

<p>In view of space constraint, in this blogpost, we omit our reproducibility study and refer the readers to <a href="https://github.com/*****/***/blob/main/blogpost.md">our extended blog post</a> (removed for anonimization). Nevertheless, our reproducibility studies corroborated the following claims:</p>
<ol>
  <li>Relaxed steerable G-CNN outperforms steerable G-CNN (fully equivariant network) on fully rotationally equivariant data as shown in the experiment on the super resolution dataset in <d-cite key="wang2023relaxed"></d-cite>.</li>
  <li>Relaxed G-CNN outperforms G-CNN on non-fully rotationally equivariant data as shown in the experiment on the smoke plume dataset in <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite>.</li>
</ol>

<h2 id="background">Background</h2>

<h3 id="regular-g-cnn">Regular G-CNN</h3>

<p>Consider the segmentation task depicted in the picture below.</p>

<div style="text-align: center;">
  <img src="https://analyticsindiamag.com/wp-content/uploads/2020/07/u-net-segmentation-e1542978983391.png" alt="Figure 1" style="max-width: 100%;" />
  <p>Annotated segmented image taken from <d-cite key="cordts2016cityscapesdatasetsemanticurban"></d-cite></p>
</div>

<p>Naturally, applying segmentation on a rotated 2D image should give the same segmented image as applying such rotation after segmentation. Mathematically, for a neural network $NN$ to be equivariant w.r.t. the group $(G,\cdot)$, such as 2D rotations, then the following property needs to be satisfied:</p>

\[\begin{align*} 
NN (T_g x) = T'_g NN (x) &amp; \qquad \qquad \forall g \in G. \\
\end{align*}\]

<p>To build such a network, it is sufficient that each of its layers is equivariant in the same sense. Recall that a CNN achieves equivariance to translations by sharing weights in kernels that are translated across the input in each of its convolution layers. Hence, a G-CNN extends this concept of weight sharing to achieve equivariance w.r.t an arbitrary locally-compact group $G$.</p>

<h4 id="lifting-convolution">Lifting convolution</h4>

<p>Consider an input signal $f^0: \mathbb{R}^n \rightarrow \mathbb{R}^c$, where $c$ is the number of channels. When passing it through a G-CNN, from the outset, it undergoes the lifting convolution with kernel $k : \mathbb{R}^n \rightarrow \mathbb{R}^{m \times c}$ on $x \in \mathbb{R}^n$ and $g \in G$:</p>

\[(k*_{lifting} f^0)(g) = \int_{y \in \mathbb{R}^n}k(g^{-1}y)f^0(y) dy\]

<p>Suppose $f^1: G \rightarrow \mathbb{R}^m$ is the output signal thereof, which is fed to the next layer.</p>

<h4 id="g-equivariant-convolution">$G$-equivariant convolution</h4>

<p>Now, $f^1$ undergoes $G$-equivariant convolution with a kernel $\psi: G \rightarrow \mathbb{R}^{k \times m}$ on $g \in G$:</p>

\[(\psi *_{G} f^1)(g) = \int_{h \in G}\psi(g^{-1}h)f^1(h)dh\]

<p>For now on we will focus on affine groups, i.e., let $G := \mathbb{R}^n \rtimes H$, where $H$ can be, for example, the rotation subgroup $SO(n)$. Therefore we will have:</p>

\[\begin{gathered} 
(k*_{lifting} f^0)(x, h) = \int_{y \in \mathbb{R}^n}k(h^{-1}(y-x))f^0(y) dy\\
(\psi *_{G} f^1)(x,h) = \int_{y \in \mathbb{R}^n}\int_{h' \in H}\psi(h^{-1}(y-x), h^{-1}h')f^1(y, h')dh'dy
\end{gathered}\]

<p>This gives the output signal $f^2: \mathbb{R}^n \times H \rightarrow \mathbb{R}^k$. This way of convolving is repeated for all subsequent layers until the final aggregation layer, e.g. linear layer, if there is one.</p>

<p>Note that for <em>regular</em> group convolution to be practically feasible, $G$ has to be <strong>finite</strong> or addecuatly supsampled. Some of these limitations can be solved by <em>steerable</em> group convolutions.</p>

<h4 id="steerable-g-cnn">Steerable G-CNN</h4>

<p>First, consider the group representations $\rho_{in}: H \rightarrow \mathbb{R}^{in \times in}$ and $\rho_{out}: H \rightarrow \mathbb{R}^{out \times out}$. To address the aforementioned equivariance problem, $G$-steerable convolution modifies $G$-equivariant convolution with the following three changes:</p>

<ul>
  <li>The input signal becomes $f: \mathbb{R}^n \rightarrow \mathbb{R}^{in}$.</li>
  <li>The kernel $\psi: \mathbb{R}^n \rightarrow \mathbb{R}^{out \times in}$ used must satisfy the following constraint for all $h \in H$: \(\psi(hx) = \rho_{out}(h) \psi(x) \rho_{in}(h^{-1})\)</li>
  <li>Standard convolution only over $\mathbb{R}^n$ and not $G := \mathbb{R}^n \rtimes H$ is performed.</li>
</ul>

<p>To secure kernel $\psi$ has the mentioned property, we precompute a set of non-learnable basis kernels $(\psi_l)_{l=1}^L$ which do have it, and define all other kernels as weighted combinations of the basis kernels, using learnable weights with the same shape as the kernels.</p>

<p>Therefore, the convolution is of the form:</p>

\[(\psi*_{\mathbb{Z}^n}f) (x) = \sum_{y \in \mathbb{Z}^n} \sum_{l=1}^L (w_l ⊙ \psi_l(y))f(x+y)\]

<p>Whenever both $\rho_{in}$ and $\rho_{out}$ can be decomposed into smaller building blocks called <strong>irreducible representations</strong>, equivariance w.r.t. infinite group $G$ is achieved (see Appendix A.1 of <d-cite key="bekkers2024fastexpressivesenequivariant"></d-cite>).</p>

<h4 id="relaxed-g-cnn">Relaxed G-CNN</h4>

<p>The desirability of equivariance in a network depends on the amount of equivariance possessed by the data of interest. To this end, <em>relaxed</em> G-CNN is built on top of a regular G-CNN using a modified (relaxed) kernel consisting of a linear combination of standard G-CNN kernels. Consider $G := \mathbb{Z}^n \rtimes H$. Then, <em>relaxed</em> G-equivariant group convolution is defined as:</p>

<!---
$$
(\psi \tilde{*}_{G} f)(g) = \sum_{h \in G}\psi(g,h)f(h) = \sum_{h \in G}\sum_{l=1}^L w_l(h) \psi_l(g^{-1}h)f(h)
$$

$$
(\psi \tilde{*}_{G} f)(\mathbf{x}, h) = \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h') \psi(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')
$$
-->

\[(\psi \tilde{*}_{G} f)(\mathbf{x}, h) = \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h') \sum_{l=1}^L w_l(h) \psi_l(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')\]

<p>or equivalently as a linear combination of regular group convolutions with different kernels:</p>

\[\begin{aligned}
(\psi \tilde{*}_{G} f)(\mathbf{x}, h) &amp;= \sum_{l=1}^L w_l(h) \sum_{\mathbf{y} \in \mathbb{Z}^n}\sum_{h' \in H} f_1(\mathbf{y}, h')  \psi_l(h^{-1}(\mathbf{y} - \mathbf{x}), h^{-1} h')\\
 &amp;= \sum_{l=1}^L w_l(h) [(\psi_l *_{G} f)(\mathbf{x}, h)]
\end{aligned}\]

<p>This second formulation makes for a more interpretable visualization, as one can see in the following figure. There, one can observe how a network might learn to downweight the feature maps corresponding to 180 degree rotations, thus breaking rotational equivariance and allowing for different processing of images picturing 6s and 9s.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-relaxed-equivariance/rgconv-480.webp 480w,/assets/img/2024-06-30-relaxed-equivariance/rgconv-800.webp 800w,/assets/img/2024-06-30-relaxed-equivariance/rgconv-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-relaxed-equivariance/rgconv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
A visual example of a relaxed lifting convolution (for $L=1$). 
</div>

<!---
 $G$-equivariance of the group convolution arises from kernel $\psi$'s dependence on the composite variable $g^{-1}h$, rather than on both variables $g$ and $h$ separately. This property is broken in relaxed kernels, leading to a loss of equivariance.

Therefore, using relaxed group convolutions allows the network to relax strict symmetry constraints, offering greater flexibility at the cost of reduced equivariance.
-->

<h4 id="relaxed-steerable-g-cnn">Relaxed steerable G-CNN</h4>

<p>Relaxed steerable G-CNN modified steerable G-CNN in a similar manner. Again, let the kernel in convolution be a linear combination of other kernels, such that the weights used depend on the variable of integration, leading to loss of equivariance.</p>

\[(\psi \tilde{*}_{\mathbb{Z}^2} f) (x) = \sum_{y \in \mathbb{Z}^2} \sum_{l=1}^L (w_l(y) ⊙ \psi_l(y))f(x+y)\]

<p>Furthermore, <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite> introduces a regularization term to impose equivariance on both relaxed models mentioned above. In our experiments, however, the best-performing models were those without this term.</p>

<h2 id="methodology">Methodology</h2>
<h3 id="datasets">Datasets</h3>
<h4 id="super-resolution">Super-Resolution</h4>

<p>The data consists of liquid flowing in 3D space and is produced by a high-resolution state-of-the-art simulation hosted by the John Hopkins University <d-cite key="jhtdb"></d-cite> . Importantly, this dataset is forced to be isotropic, i.e. fully equivariant to rotations, by design.</p>

<p>For the experiment, a subset of 50 timesteps are taken, each downsampled from $1024^3$ to $64^3$ and processed into a task suitable for learning. The model is given an input of 3 consecutive timesteps, $t, t+1, t+2$ (which are first downsampled to $16^3$), and is tasked to upsample timestep $t+1$ to $64^3$, see Figure 1 for a visualization.</p>

<p>We use the following $3$ models from <d-cite key="wang2023relaxed"></d-cite>’s experiment on the same dataset in <a href="#Results">Results</a>:</p>

<ul>
  <li>CNN.</li>
  <li>Regular G-CNN.</li>
  <li>Relaxed G-CNN.</li>
</ul>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/r1WCqrL4A.png" alt="Figure 1" style="max-width: 100%;" />
  <p>Figure 1: Super Resolution architecture, taken from [1].</p>
</div>

<h4 id="smoke-plume">Smoke Plume</h4>
<!-- For this experiment in [Wang et al. (2022)](#References), a specialized 2D smoke simulation was generated using PhiFlow [(Holl et al., 2020)](#References). -->
<p>This is a synthetic $64 \times 64$ 2D smoke simulation dataset generated by PhiFlow <d-cite key="phiflow"></d-cite>, where dispersion of smoke in a scene starting from an inflow position with a buoyant force is simulated (Figure 2).</p>

<p>The dataset we used has a fixed inflow with buoyant force only pointing in one of the following $4$ directions: upwards, downwards, left, or right. For our experiments we keep the buoyant force the same in all directions such that the data is fully equivariant w.r.t. $90$ degree rotations.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/S1RILeLE0.png" alt="Figure 2" style="max-width: 100%;" />
  <p>Figure 2: Example of a Smoke Plume sequence generated by PhiFlow.</p>
</div>

<p>The models trained on this dataset are tasked with predicting the upcoming frame based on the current one. We use the following $2$ models in <a href="#Results">Results</a>:</p>

<ul>
  <li>Relaxed steerable G-CNN from <d-cite key="wang2022approximatelyequivariantnetworksimperfectly"></d-cite> with relaxed equivariance w.r.t the C4 group.</li>
  <li>Steerable G-CNN from <d-cite key="weiler2021generale2equivariantsteerablecnns"></d-cite> with full equivariance w.r.t the C4 group.</li>
</ul>

<h3 id="training-dynamics-evaluation">Training Dynamics Evaluation</h3>

<p>To assess the training dynamics of a network, we are interested in the final performance and the generalizability of the learned parameters, which are quantified by the final RMSE, and the sharpness of the loss landscape near the final weight-point proposed in <d-cite key="zhao2024improvingconvergencegeneralizationusing"></d-cite>.</p>

<h4 id="sharpness">Sharpness</h4>

<p>To measure the sharpness of the loss landscape after training, we consider changes in the loss averaged over random directions. Let $D$ denote a set of vectors randomly drawn from the unit sphere, and $T$ a set of displacements, i.e. real numbers. Then, the sharpness of the loss $\mathcal{L}$ at a point $w$ is:</p>

\[\phi(w,D,T) = \frac{1}{|D||T|} \sum_{t \in T} \sum_{d \in D} |\mathcal{L}(w+dt)-\mathcal{L}(w)|\]

<p>This definition is an adaptation from the one in <d-cite key="zhao2024improvingconvergencegeneralizationusing"></d-cite>. A sharper loss landscape around the model’s final weights usually implies a greater generalization gap.</p>

<h4 id="hessian-eigenvalue">Hessian Eigenvalue</h4>

<p>Finally, the Hessian eigenvalue spectrum (<d-cite key="park2022visiontransformerswork"></d-cite>) sheds light on both the efficiency and efficacy of neural network training. Negative Hessian eigenvalues indicate a non-convex loss landscape, which can disturb the optimization process, whereas very large eigenvalues indicate training instability, sharp minima and consequently poor generalization.</p>

<h2 id="results">Results</h2>
<p>In this section, we study how equivariance imposed on a network influences the convexity of the loss landscape and generalization, answering all the subquestions posed in <a href="#Introduction">Introduction</a>.</p>

<h3 id="smoke-plume-with-full-equivariance">Smoke Plume with full Equivariance</h3>

<p>First, we examine the training, validation and test RMSE for the E2CNN and Rsteer models on the fully equivariant Smoke Plume dataset.</p>
<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/ByqLXIUEA.png" alt="Figure 5" style="max-width: 100%;" />
      <p align="center">Figure 5: Train RMSE curve for rsteer and E2CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/rJ58Q8LVC.png" alt="Figure 6" style="max-width: 100%;" />
      <p align="center">Figure 6: Validation RMSE curve for rsteer and E2CNN models</p>
    </td>
  </tr>
</table>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/HyqIXLIEA.png" alt="Figure 7" style="max-width: 100%;" />
  <p>Figure 7: Test RMSE for best models, averaged over five seeds</p>
</div>

<p>Figures 5 and 6 show the train and validation RMSE curves. While rsteer and E2CNN perform similarly on the training data, rsteer has lower RMSE on the validation data, indicating better generalization. Figure 7 confirms that rsteer performs best on the test set, consistent with results on the Isotropic Flow dataset in <d-cite key="wang2023relaxed"></d-cite>.</p>

<p>To understand why relaxed equivariant models outperform fully equivariant ones, we examine the sharpness of the loss and the Hessian spectra.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/S14l5XZ4A.png" alt="Figure 10" style="max-width: 100%;" />
  <p>Figure 8: Sharpness at early and best epochs for rsteer and E2CNN models. On the equivariant Smokeplume dataset</p>
</div>

<p>Figure 10 shows that the rsteer model has much lower sharpness of the loss landscape compared to E2CNN for both checkpoints. This indicates a lower generalization gap, and thus more effective learning. This matches the lower validation RMSE curve we saw earlier.</p>

<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/rJJQKAyNC.png" alt="Epoch 3" style="max-width: 100%;" />
      <p align="center">Figure 9: Hessian spectra at an early epoch for rsteer and E2CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/S1jQF0JN0.png" alt="Epoch best" style="max-width: 100%;" />
      <p align="center">Figure 10: Hessian spectra at the best epoch for rsteer and E2CNN models</p>
    </td>
  </tr>
</table>

<p>Figures 9 and 10 show Hessian spectra for the same checkpoints as the previous analysis. Regarding loss landscape flatness, both plots indicate that E2CNN has much larger eigenvalues than rsteer, potentially leading to training instability, less flat minima, and poor generalization for E2CNN.</p>

<p>To evaluate the convexity of the loss landscape, we examine the negative eigenvalues in the Hessian spectra. Neither model shows any negative eigenvalues, suggesting that both E2CNN and rsteer encounter convex loss landscapes. Therefore, convexity does not seem to significantly impact performance in this case.</p>

<h3 id="super-resolution-1">Super Resolution</h3>

<p>Similarly, we also analyze the training dynamics of the superresolution models on the isotropic Super-Resolution dataset.</p>

<p>First, we examine the training and validation MAE curves for the Relaxed Equivariant (RGCNN), Fully Equivariant (GCNN), and non-equivariant (CNN) models (run on 6 different seeds).</p>

<table>
  <tr>
    <td>
      <img src="https://hackmd.io/_uploads/HJrVE8IEA.png" alt="Figure 8" style="max-width: 100%;" />
      <p align="center">Figure 11: Training MAE curve for RGCNN, GCNN and CNN models</p>
    </td>
    <td>
      <img src="https://hackmd.io/_uploads/HJrV48UN0.png" alt="Figure 9" style="max-width: 100%;" />
      <p align="center">Figure 12: Validation MAE curve for RGCNN, GCNN and CNN models</p>
    </td>
  </tr>
</table>

<p>Here, we observe that early in the training (around epoch $3$), RGCNN starts outperforming the other two models and keeps this lead until its saturation at around $0.1$ MAE. For this reason, we take a checkpoint for each model on epoch $3$ (early) and on its best epoch (Best), to examine the corresponding sharpness values.</p>

<div style="text-align: center;">
  <img src="https://hackmd.io/_uploads/SkpgREzV0.png" alt="Figure 10" style="max-width: 100%;" />
  <p>Figure 13: Sharpness of the loss landscape on the super resolution dataset. Ran over 6 seeds, error bars represent the standard deviation. For early, the third epoch was chosen, while for best the epoch with the best validation loss was chosen.</p>
    
</div>

<p>Figure 13 shows that the relaxed model has the lowest sharpeness in both cases. This indicates that the relaxed steerable GCNN has better generalisability during its training and at its convergence, matching our findings on the previous dataset.</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>We reproduced and extended the relevant findings in <d-cite key="wang2023relaxed"></d-cite> reaffirming the effectiveness of relaxed equivariant models and demonstrating that they are able to outperform fully equivariant models even on perfectly equivariant datasets.</p>

<p>We furthermore investigated the authors’ speculation that this superior performance could be due to relaxed models having enhanced training dynamics. Our experiments empirically support this hypothesis, showing that relaxed models exhibit lower validation error, a flatter loss landscape around the final weights, and smaller Hessian eigenvalues, all of which are indicators of improved training dynamics and better generalization.</p>

<p>Our results suggest that replacing fully equivariant networks with relaxed equivariant networks could be advantageous in all application domains where some level of model equivariance is desired, including those where full equivariance is beneficial. For future research, we should investigate different versions of the relaxed model to find out which hyperparameters, like the number of filter banks, correlate with sharpness. Additionally, the method should be applied to different types of data to see if the same observations can be made there.</p>

<!---
## References

[1] Wang, R., Walters, R., & Smidt, T. E. (2023). Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. arXiv preprint arXiv:2310.02299.

[2] Gruver, N., Finzi, M., Goldblum, M., & Wilson, A. G. (2022). The lie derivative for measuring learned equivariance. arXiv preprint arXiv:2210.02984.

[3] Park, N., & Kim, S. (2022). How do vision transformers work?. arXiv preprint arXiv:2202.06709.

[4] Zhao, B., Gower, R. M., Walters, R., & Yu, R. (2023). Improving Convergence and Generalization Using Parameter Symmetries. arXiv preprint arXiv:2305.13404.

[5] Wang, R., Walters, R., & Yu, R. (2022, June). Approximately equivariant networks for imperfectly symmetric dynamics. In International Conference on Machine Learning (pp. 23078-23091). PMLR.

[6] Holl, P., Koltun, V., Um, K., & Thuerey, N. (2020). phiflow: A differentiable pde solving framework for deep learning via physical simulations. In NeurIPS workshop (Vol. 2).

[7]  Y. Li, E. Perlman, M. Wan, Y. Yang, C. Meneveau, R. Burns, S. Chen, A. Szalay & G. Eyink. "A public turbulence database cluster and applications to study Lagrangian evolution of velocity increments in turbulence". Journal of Turbulence 9, No. 31, 2008.

[8] E. Perlman, R. Burns, Y. Li, and C. Meneveau. "Data Exploration of Turbulence Simulations using a Database Cluster". Supercomputing SC07, ACM, IEEE, 2007.

[9] Super-resolution of Velocity Fields in Three-dimensional Fluid Dynamics: https://huggingface.co/datasets/*******/jhtdb

[10] Weiler, M. and Cesa, G. General E(2)-equivariant steerable CNNs. In Advances in Neural Information Processing Systems (NeurIPS), pp. 14334–14345, 2019b.

[11] Turbulence SuperResolution Replication W&B Report: https://api.wandb.ai/links/*******/hxj68bs1

[12] Equivariance and Training Stability W&B Report: https://api.wandb.ai/links/*******/yu9a85jn

[13] Rotation SmokePlume Replication W&B Report: https://api.wandb.ai/links/*******/hjsmj1u7

[14] `gconv` library for regular group convnets: https://github.com/*****/gconv

[15] Bekkers, E. J., Vadgama, S., Hesselink, R. D., van der Linden, P. A., & Romero, D. W. (2023). Fast, Expressive SE $(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. arXiv preprint arXiv:2310.02970.

[16] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016
-->]]></content><author><name>Anon</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[Can relaxing equivariance help in finding better minima?]]></summary></entry><entry><title type="html">Applications of TopoX to Topological Deep Learning</title><link href="http://localhost:4000/blog/2024/smpn/" rel="alternate" type="text/html" title="Applications of TopoX to Topological Deep Learning" /><published>2024-06-30T00:00:00+02:00</published><updated>2024-06-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/smpn</id><content type="html" xml:base="http://localhost:4000/blog/2024/smpn/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>Representation learning using Graph Neural Networks (GNNs) is a rapidly growing approach to complex tasks in chemistry <d-cite key="ballester2024attending,bekkers2023fast,eijkelboom2023n,battiloro2024n"></d-cite>. Notably, in a subset of these tasks, a crucial aspect is maintaining equivariance to transformations such as <em>translation</em>, <em>rotation</em> and <em>reflection</em>. Learning representations such that <strong>equivariance</strong> or <strong>invariance</strong> can be applied has proved very helpful <d-cite key="bekkers2023fast,eijkelboom2023n"></d-cite>. Additionally, incorporating higher-order relations in GNNs such that they encode more complex topological spaces is a recent effort to increase the expressivity of GNNs <d-cite key="hajij2022topological,eijkelboom2023n,giusti2024topological"></d-cite>.</p>

<p>This blogpost aims to draw attention to Topological Deep Learning (TDL) by using the suite of Python packages TopoX <d-cite key="hajij2024topox"></d-cite> to replicate the work of <d-cite key="eijkelboom2023n"></d-cite> and show how much simpler development is in this framework. Additionally, we experimented with different topological spaces with more geometric information and compared the results with those of the original work.</p>

<hr />

<h1 id="higher-order-networks-and-why-topology-is-useful">Higher-order networks and why topology is useful</h1>
<p>Our regular and beloved graphs, as functional as they are, have a bound on the expressive power under <em>message passing networks</em> (MPN) that operate over them <d-cite key="xu2018powerful"></d-cite>. For example, they can learn higher dimensional graph structures, such as <em>cliques</em>. The 1-WL <d-cite key="xu2018powerful"></d-cite> test is one of the measurements that characterize expressivity in distinguishing non-isomorphic graphs. While this measurement has its own set of limitations, it is the standard in characterizing expressivity.</p>

<p>In search of more expressive structures, the exploration of higher-dimensional topological spaces where higher-dimensional features, such as <em>cliques</em>, can be represented (and learned). Let’s start with what the space of a graph encodes. Let  \(G = (V, E)\) be a graph. We can interpret it as encoding relationships between nodes \(u, v  \in V\) as a tuple represented by an edge \((u, v) \in E\). What would relations between <em>pairs</em> of nodes and other pairs \((u,v) \rightarrow (x, y)\) or between <em>pairs</em> and <em>triplets</em> \((u, v, w) \rightarrow (x, y)\) would look like and how can we represent them? One space that allows for this kind of relationship is <strong>Simplicial Complexes</strong>. Extended graphs with features for higher dimensional structures are subjected to constraints. Luckily, the combinatorial definition of these spaces is more straightforward.</p>

<hr />

<h1 id="simplicial-complex-what-is-it-">Simplicial Complex: What is it ?</h1>
<p>An <em>abstract simplicial complex</em> (ASC) is the combinatorial expression of a non-empty set of <em>simplices</em>.</p>

<p>Concretly, let \(\mathcal{P}(S)\) be the powerset of \(S\) and let \(\mathcal{K} \subset \mathcal{P}(S)\), then \(\mathcal{K}\) is an ASC if for every \(X \in \mathcal{K}\) and every non-empty \(Y \subseteq X\) it holds that \(Y \in \mathcal{K}\). Also, we define the cardinality as \(\mid\mathcal{K}\mid  = (\underset{X \in \mathcal{K}}{\operatorname{max} \mid X \mid }) - 1\), to be the highest cardinality of a simplex in an ASC minus 1. If the rank is \(r\), it holds \(\forall X \in \mathcal{K}: r \geq \mid X \mid\).</p>

<details><summary><strong>Click here</strong> for an <em>example</em></summary>
<p>Let \(S = \{0, 1, 2, 3\}\) and \(\mathcal{K} = \{0, 1, 2, 3,  \{0, 1\}, \{1, 2\}, \{0, 2\}, \{0, 1, 2\}\}\) where we could pick an arbitrary element \(X = \{0, 1, 2\}\) and check that \(\forall Y \subseteq X: Y \in \mathcal{K}\). Meaning that for any possible subset \(Y\) in \(X\), \(Y\) is contained in \(\mathcal{K}\). In this case, the set \(\mathcal{K}\) is an ASC.</p>

</details>

<h2 id="geometric-realization">Geometric realization</h2>
<p>Although an ASC is a purely combinatorial object, it always entails a <strong>geometric realization</strong>. A <strong>Simplicial Complex</strong> is the geometric realization of an ASC, constructed out of the underlying geometric points in \(\mathcal{K}\). Thus, <strong>Simplicial Complex</strong> of dimension $1$ is equivalent to a <em>geometric graph</em> (can you see why ?).</p>

<p>The procedure of transforming a structure to a topological domain is commonly called <strong>lifting</strong>. Thus, lifting can be from point clouds to graphs, graphs to simplicial complexes, or other pairs of domains, given that the properties of the target domain mentioned before hold.</p>

<details><summary><strong>Click here</strong> to see the ASC corresponding to Figure 1</summary>
<p>As an example, let \(S = \{1, 2, 3, 4, 5, 6, 7\}\) and \(\mathcal{K} = \{ \{5, 6, 7\}, \{5, 6\}, \{5, 7\}, \{6, 7\}, \cdot \cdot \cdot, \{1,2\},\{2, 3\}, \cdot \cdot \cdot, \{1\}, \{2\}, \{3\}, \cdot \cdot \cdot\}\)</p>
</details>

<div class="row mt-3">
    <div class="col-8 mt-3 mt-md-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/simplex_desc-480.webp 480w,/assets/img/2024-06-30-smpn/simplex_desc-800.webp 800w,/assets/img/2024-06-30-smpn/simplex_desc-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/simplex_desc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 1: Ilustration of the lifting procedure</figcaption>
  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        This shows the lifting procedure of a graph into a <b>simplicial complex</b> using the <b>clique lifting</b>, where nodes are <span style="color:green"> 0-simplices </span>, edges are <span style="color:red"> 1-simplices </span> and triangles are <span style="color:blue"> 2-simplices </span>
    </div>
</div>

<h3 id="clique-complex">Clique Complex</h3>

<p>Of the plethora of spaces, a relatively simple and intuitive one is the <strong>Clique Complex</strong> shown at the top left of Figure 1. To describe the <strong>Clique Complex</strong> we need to formally define a <em>clique</em>, something we skipped over before.</p>

<details><summary><strong>Click here</strong> for a formal definition of <em>clique</em></summary>
<p>Given a graph \(G = (V, E)\), a <em>clique</em> \(C \subseteq V\), is an induced graph on \(G\) such that \(C\) is complete. In other words, there is an edge between every pair of vertices in \(C\).</p>
</details>

<p>The <strong>Clique Complex</strong> is a <strong>Simplicial Complex</strong> of rank \(1\). Each clique will become an \(r\)-simplex depending on the cardinality of \(C\) such that \(r=\mid C \mid-1\). The number of cliques grows exponentially, and the problem of finding all the cliques is complex. For this reason, the naive  time complexity of this lift is \(\mathcal{O}(3^{n/3})\)</p>

<h3 id="vietoris-ripps-complex">Vietoris-Ripps Complex</h3>
<p>The Vietoris-Rips complex is a common way to form a topological space efficiently. The time complexity for generating the procedure depends on the parameter \(\delta\) and the number of points \(n\)  given by \(\mathcal{O}(n^{r+1})\), where \(\delta\) is the diameter of the balls grown around points to calculate relationships. This space is equivalent to a <strong>Clique Complex</strong> in a geometric graph. Below, you can see a visualization of the lifting process with varying values of \(\delta\). The <span style="color:green"> points  </span> are <span style="color:green"> 0-simplex </span>, the <span style="color:red"> lines </span> are <span style="color:red"> 1-simplex </span> and the <span style="color:blue"> triangles </span> are <span style="color:blue"> 2-simplex </span>. The <span style="color:yellow"> growing ball </span> is the disk relating to the \(\delta\) parameter.</p>

<div class="m-page">
  <iframe src="/assets/plotly/2024-06-30-smpn/vr_lift.html" frameborder="0" scrolling="no" height="600x" width="100%" style="border: 0px  grey;"></iframe>
</div>

<hr />

<h1 id="gnns-and-en-equivariant-gnns">GNNs and E(n) Equivariant GNNs</h1>
<p>GNNs come in many flavors, but to be concise, we will focus on the <em>Message Passing Networks</em> where messages are passed among <em>neighborhoods</em>, which update the node’s representation. At a given number of passes, we will take the node representations and perform classification or regression tasks or pool them to perform the task on the whole graph. Next, we will introduce the remainder of the MPN framework and equivariant GNNs.</p>

<h2 id="good-ol-message-passing">Good ol’ message passing</h2>
<p>Let \(G = (V,E)\) be a graph consisting of nodes \(V\) and edges \(E\). Then let each node \(v_i \in V\) and edge \(e_{ij} \in E\) have an associated node feature \(\mathbf{f}_i \in \mathbb{R}^{c_n}\) and edge feature \(a_{ij} \in \mathbb{R}^{c_e}\), with dimensionality \(c_n, c_e \in \mathbb{N}_{&gt;0}\). Then, we define a <em>message passing layer</em> as:</p>

\[\begin{equation}\label{compute_message}
\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \mathbf{a}_{i j}\right)
\end{equation}\]

<p>\(\begin{equation}\label{aggregate_messages}
    \mathbf{m}_i=\underset{j \in \mathcal{N}(i)}{\operatorname{Agg}} \mathbf{m}_{i j}
\end{equation}\)
\(\begin{equation}\label{update_hidden}
    \mathbf{h}_i^{l+1}=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{equation}\)</p>

<h2 id="en-equivariant-gnn">E(n) equivariant GNN</h2>

<p>Using <em>inductive biases</em> to steer the training of GNNs towards a particular domain is common. There is a specific class of problems where symmetries are an intrinsic part of their representation. Examples of these are 3D molecular structures and N-body systems. To enhance the learning procedure, we restrict the families of learned functions by guaranteeing equivariance with the action of a transformation from a particular symmetry group. Of of those groups is the \(E(n)\) group, which encodes <em>rotation</em>, <em>translation</em>, <em>reflection</em> and <em>scaling</em> ( <strong>Euclidean</strong> group of dimension \(n\) ).</p>

<h3 id="tasks-qm9-and-n-body">Tasks: QM9 and N-Body</h3>
<p>QM9 is a dataset of 134K small molecules (up to \(9\) atoms without counting hydrogen) with \(19\) regression tasks representing the molecule’s properties. Each atom has a set of features and a position in 3D space. Pytorch geometric makes this dataset available in a convenient graph representation. The <em>N-body</em> problem extends the “<em>Charged particle N-body system</em>” to 3D where \(5\) particles have a positive and negative charge, position, and velocity. Then, the prediction is used to estimate the particle’s position after the steps of \(n\).</p>

<h3 id="equivariance-and-invariance">Equivariance and Invariance</h3>
<p><strong>Invariance</strong> is when an object or set of objects remain the same after a transformation. In contrast, <strong>equivariance</strong> is a symmetry concerning a function and a transformation. At first glance, these definitions are complicated to picture; however, with some group theory, they will become more apparent.</p>

<p>Let \(G\) be a group and let \(X\), \(Y\) be sets on which \(G\) acts. A function \(f: X \rightarrow Y\) is called equivariant with respect to \(G\) if it commutes with the group action. Equation \ref{eq:equi} expresses this notion formally.</p>

\[\begin{equation}\label{eq:equi}
f(g \cdot x)=g \cdot f(x)
\end{equation}\]

<p>Conversly, Equation \ref{eq:inv} shows that <strong>invariance</strong> is when the application of the transformation \(g \in G\) does not affect the output of the map \(f\),</p>

\[\begin{equation}\label{eq:inv}
f(g \cdot x)=f(x) 
\end{equation}\]

<h3 id="do-invariances-hold-in-gnns-">Do invariances hold in GNNs ?</h3>

<p>Equation \ref{eq:msg_eq} comes to replace Equation \ref{compute_message} with our invariant function. To make the network equivariant, we introduce feature vector  \(x\), which contains the positional coordinates in Euclidean space. Equation \ref{eq:pos_update} refers to the update in the position embedding of the node. The proof that with this condition, equivariance holds can be found in <d-cite key="satorras2021n"></d-cite>.
\(\begin{equation}\label{eq:msg_eq}
 \mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \operatorname{inv}\left(\mathbf{x}_i^l, \mathbf{x}_j^l\right), \mathbf{a}_{i j}\right)
 \end{equation}\)</p>

\[\begin{equation}\label{eq:pos_update}
    \mathbf{x}_i^{l+1}=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right)
\end{equation}\]

<h2 id="message-passing-simplicial-networks-mpsn">Message Passing Simplicial Networks (MPSN)</h2>

<p>In <d-cite key="eijkelboom2023n"></d-cite> authors generalize the \(E(n)\) equivariant GNN from <d-cite key="satorras2021n"></d-cite> to operate on <strong>Simplicial Complexes</strong>. First, the authors show how relationships between simplices are established via <a href="# higher-order-neighborhoods">higher-order neighborhoods</a>. Then, they pick a set of equivariant features on the geometric shapes embedded in the topological space up to relations involving \(2\)-simplices (triangles). Finally, they show how equivariance holds under message passing among simplices. We will go over each of these.</p>

<h3 id="higher-order-neighborhoods">Higher-order neighborhoods</h3>
<p>We establish some definitions to define proximity relations, such as graph adjacencies in \(r\)-simplex. We will work with only two types of adjacencies as they have proven to be as expressive as using all of them. First, let \(\sigma\) and \(\tau\) be two simplices, we say that \(\tau\)  <strong>is on the bound of</strong> \(\sigma\) or \(\tau \prec \sigma\) if :</p>
<ol>
  <li>
\[\tau \subset \sigma\]
  </li>
  <li>
\[\nexists \delta: \tau  \subset \delta \subset \sigma\]
  </li>
</ol>

<p>Equation \ref{eq:bound_adj} referes to the relation between a \(r\)-simplex and the \((r-1)\)-simplex that compose it. Equation \ref{eq:bound_up} referes to the relationship between  \((r-1)\)-simplex and other \((r-1)\)-simplex that are a part of a higher \(r\)-simplex. They are also referred to as <strong>cofaces</strong> in the literature <d-cite key="hajij2022topological"></d-cite>.</p>

\[\begin{equation}\label{eq:bound_adj}
\mathcal{B}(\sigma) = \{\tau \mid \tau \prec \sigma\}
\end{equation}\]

\[\begin{equation}\label{eq:bound_up}
\mathcal{N}_{\uparrow}(\sigma) = \{\tau \mid \exists \delta, \tau \prec \delta \land \sigma \prec \delta\}
\end{equation}\]

<h3 id="equivariant-relations">Equivariant relations</h3>

<p>We will make use of previous definitions in the following small section. <a href="#equivariance-and-invariance">Recall</a> that we showed that we can choose any \(f\) such that when applying to a certain class of objects <strong>invariant</strong>, it is invariant with respect to the group action of any \(g \in G\) for  \(G = E(n)\) in this case. The authors of the paper choose four types of message passing. Each of these will have a set of geometric features depending on \(f\). In general \(p_i\) and \(p_j\) mean a <strong>shared</strong> point, \(a\) and \(b\) are points <strong>not shared</strong>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Num</th>
      <th style="text-align: center">\(\bullet \rightarrow \bullet\)</th>
      <th style="text-align: center">\(\bullet \rightarrow |\)</th>
      <th style="text-align: center">\(| \rightarrow |\)</th>
      <th style="text-align: center">\(| \rightarrow \triangle\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{a}} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{a}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{a}}\parallel\)</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{a}}\parallel\)</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{a}} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(V(S_2)\)</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{a}}\parallel\)</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{a}}\parallel\)</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">\(\parallel x_{\mathbf{p}_i} - x_{\mathbf{b}}\parallel\)</td>
      <td style="text-align: center">\(\angle \mathbf{p}_i + \angle \mathbf{p}_j\)</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">\(\angle \mathbf{p}_i\)</td>
      <td style="text-align: center">\(\angle \mathbf{a}\)</td>
    </tr>
  </tbody>
</table>

<div class="row mt-3">
    <div class="col-8 mt-3 mt-md-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/shapes-480.webp 480w,/assets/img/2024-06-30-smpn/shapes-800.webp 800w,/assets/img/2024-06-30-smpn/shapes-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/shapes.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 2: Geometric relations</figcaption>
  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        This ilustrations represent how geometric relations are interpreted in terms of <span style="font-weight:bold"> boundries</span> (right) and <span style="font-weight:bold">upper-adjacencies</span> (left). In the <span style="font-wieght:bold">boundry</span> relationships the figures share all but one point and in the <span style="font-weight:bold">upper-adjacencies</span> they might share some points given that they are part of a higher-order simplex.
    </div>
</div>

<h3 id="does-it-work-on-higher-order-networks-">Does it work on higher-order networks ?</h3>

<p>Using the previous definitions of neighborhoods, <d-cite key="eijkelboom2023n"></d-cite> defines a message for each neighborhood as Equation \ref{eq:msg_boundary} and Equation \ref{eq:msg_ua} and replaces the hidden representation update to take these messages into account in Equation \ref{eq:update_sc}.</p>

\[\begin{equation}\label{eq:msg_boundary}
m_{\mathcal{B}}(\sigma) = \underset{\tau \in \mathcal{B}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{B}}(h^l_{\sigma}, h^l_{\tau})
\end{equation}\]

\[\begin{equation}\label{eq:msg_ua}
m_{\mathcal{N}_{\uparrow}}(\sigma) = \underset{\tau \in \mathcal{N}_{\uparrow}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{N}_{\uparrow}}(h^l_{\sigma}, h^l_{\tau}))
\end{equation}\]

\[\begin{equation}\label{eq:update_sc}
h_{\sigma}^{l+1} = \phi_{h} (h_{\sigma}^l, m_{\mathcal{B}}(\sigma), m_{\mathcal{N}_{\uparrow}}(\sigma))
\end{equation}\]

<p>Finally, they define a graph embedding as Equation \ref{eq:agg_simp} where the simplices \(\mathcal{K}\) of each dimension \(r\) will be aggregated, and the final embedding of the complex will be the concatenation of the embedding of each dimension.</p>

\[\begin{equation}\label{eq:agg_simp}

h_{\mathcal{K}} = \bigoplus_{i=0}^{r} \underset{\sigma \in \mathcal{K}, |\sigma|=i+1}{\operatorname{Agg}} h_\sigma
\end{equation}\]

<hr />

<h1 id="the-new-standard-topox">The new standard: TopoX</h1>

<p><strong>TopoX</strong> is a suite of Python packages that aims to fill the need for accessible and open-source software libraries to handle earning in higher-order domains. In the words of the team behind it, one of its goals is:</p>
<blockquote>
  <p>facilitate research in topological domains by providing foundational code to understand concepts and offer a platform to disseminate algorithms <d-cite key="hajij2024topox"></d-cite></p>
</blockquote>

<p>Given the rapid theoretical advancements in TDL, the need for a solid experimental playground is clear. Many of the development setups would benefit in terms of development efficacy and replication capabilities by sharing a set of standards and practices. <strong>TopoX</strong> is compromised of four modules: <strong>TopoModelX</strong>, <strong>TopoNetX</strong>, <strong>TopoEmbbedX</strong> and <strong>TopoBenchmarkX</strong></p>

<ul>
  <li><strong>TopoModelX</strong>: Variety of models based on higher-order message passing built on top of <em>Pytorch</em></li>
  <li><strong>TopoNetX</strong>: Similar to <em>NetworkX</em> it provides utilities to handle nodes, edge, higher-order cells and the calculation of adjacencies, incidences and hodge laplacians over complexes</li>
  <li><strong>TopoEmbeddX</strong>: Embedding of topological domains in euclidean domains</li>
  <li><strong>TopoBenchmarkX</strong> : Addition of datasets, transform such as <em>lifts</em> and new deep learning models</li>
</ul>

<p>Next, we illustrate the development process and reproduction of <d-cite key="eijkelboom2023n"></d-cite> in the TopoX suite. Additionally, as a base project, we use the <a href="https://github.com/pyt-team/challenge-icml-2024">ICML TDL Challenge 2024</a> repo for development, which has a very similar structure to the following package in the suite <strong>TopoBenchmarkX</strong> <d-cite key="telyatnikov2024topobenchmarkx">. To attempt a result reproduction, we tackle the QM9 dataset regression tasks.</d-cite></p>

<h2 id="1-building-structure">1. Building structure</h2>

<p>We are first concerned with the <strong>lifting</strong> of our initial graph or set of points. To perform that task, we will make use of GHUDI <d-cite key="gudhi:urm"></d-cite>, a Python library with many methods mainly used for Topological Data Analysis. We will lift from the <strong>graph</strong> domain to the <strong>simplicial complex</strong> domain. Each <strong>lifting</strong> procedure is a Pytorch <em>BaseTransform</em> with a <code class="language-plaintext highlighter-rouge">forward</code> that looks like this.</p>

<d-code block="" language="python">
    def forward(self, data: torch_geometric.data.Data) -&gt; torch_geometric.data.Data:
        r"""Applies the full lifting (topology + features) to the input data.

        Parameters
        ----------
        data : torch_geometric.data.Data
            The input data to be lifted.

        Returns
        -------
        torch_geometric.data.Data
            The lifted data.
        """
        initial_data = data.to_dict()
        lifted_topology = self.lift_topology(data)
        lifted_topology = self.feature_lifting(lifted_topology)
        return torch_geometric.data.Data(**initial_data, **lifted_topology)
</d-code>

<p>In essence, we only need to define a subclass of the source and target domain of our choice  (in this case <code class="language-plaintext highlighter-rouge">Graph2SimplicialLifting</code>) and override the <code class="language-plaintext highlighter-rouge">lift_topology</code> method and the apply <a href="#giving-meaning-to-structure">feature lifting</a>.</p>

<d-code block="" language="python">
 def lift_topology(self, data: torch_geometric.data.Data) -&gt; dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)
</d-code>
<p>In this case, we introduced <code class="language-plaintext highlighter-rouge">rips_lift</code>, which is going to do the actual computation of the lift, and <code class="language-plaintext highlighter-rouge">_get_lifted_topology</code> which will transform our <code class="language-plaintext highlighter-rouge">SimplicialComplex</code>into a Pytorch <code class="language-plaintext highlighter-rouge">Data</code> object.</p>

<d-code block="" language="python">
def rips_lift(graph: torch_geometric.data.Data, dim: int, dis: float,
                    fc_nodes: bool = True) -&gt; SimplicialComplex:
    x_0, pos = graph.x, graph.pos

    points = [pos[i].tolist() for i in range(pos.shape[0])]

    rips_complex = gudhi.RipsComplex(points=points, max_edge_length=dis)
    simplex_tree: SimplexTree  = rips_complex.create_simplex_tree(max_dimension=dim)

    if fc_nodes:
        nodes = [i for i in range(x_0.shape[0])]
        for edge in combinations(nodes, 2):
            simplex_tree.insert(edge)

    return SimplicialComplex.from_gudhi(simplex_tree)
</d-code>

<p>Note that optionally, nodes are fully connected, per the implementation of <d-cite key="eijkelboom2023n"></d-cite>. On <code class="language-plaintext highlighter-rouge">_lifted_topology</code>, we build the matrix representation of our complex. The library provides the <code class="language-plaintext highlighter-rouge">get_complex_connectivity</code>and constructs the connectivity matrices.</p>

<d-code block="" language="python">
    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -&gt; dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            # Convert to edge_index format
            lifted_topology[f'adjacency_{r}'] = 
            lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()

            lifted_topology[f'incidence_{r}'] = 
            lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()


        for r in range(0, simplicial_complex.dim+1):
            # Returns the list of `r`-simplex as `r`-tuples and convert to tensor
            lifted_topology[f'x_idx_{r}'] = 
            torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology

</d-code>

<p>Note that we transform the <strong>adjacency</strong> and <strong>incidence</strong> matrices to their <em>edge_index</em> form by using <code class="language-plaintext highlighter-rouge">nonzero().t().contigous().</code> This transformation is to be able to perform mini-batching during training.</p>

<h2 id="2-giving-meaning-to-structure">2. Giving meaning to structure</h2>

<p>Now that we have a higher-order topological space, we are missing only one thing. <em>What should the embeddings of the \(r\)-simplex higher than \(0\) be ?</em></p>

<p>There is still the question of what a good feature-lifting technique constitutes and what information it should take from the underlying representation. Authors in <d-cite key="eijkelboom2023n"></d-cite> perform an element-wise mean of the components of lower \(r\)-simplex, but the field is open to experimentation. Other alternatives are also being explored. As part of our contribution, we leverage TopoX. This tool allows us to accelerate these calculations by vectorizing this part of the process.</p>

<p>Formally, if \(\mid \sigma \mid &gt; 1\), then:</p>

\[f(\sigma) = mean(\{ f(\tau) \; | \; \tau \prec \sigma \wedge \nexists \delta:   \tau \prec \delta \prec \sigma \})\]

<details><summary><strong>Click here</strong> for an <em>example</em></summary>
<p>Let \(\sigma = \{a, b, c\}\) be a <em>simplex</em> where \(a,b,c \in R^d\) and denote the feature embedding of \(\sigma\) with \(f(\sigma)\). Then, given those simplices on the boundary are given by the powerset such that \(\tau_1 = \{a, b\}, \tau_2=\{b, c\}, \tau_3=\{a, c\}\) and \(\forall i \in \{1, 2, 3\}: \tau_i \prec \sigma\) and \(\delta_1=\{a\}, \delta_2=\{b\}\) we have that \(f(\tau_1) = mean(f(\delta_1), f(\delta_2))\) and \(f(\sigma) = mean(f(\tau_1), f(\tau_2), f(\tau_3))\).</p>
</details>

<h2 id="3-the-model">3. The Model</h2>

<p>On the model side, the most interesting addition is by <strong>TopoModelX</strong> and the <code class="language-plaintext highlighter-rouge">Conv</code> class. This class represents the <em>convolution operator</em> on the graph, which is necessary for aggregating messages over the neighborhood. We show our implementation of the <code class="language-plaintext highlighter-rouge">Conv</code> class to allow <strong>equivariant</strong> information to pass inside the messages. Everything else is standard Pytorch (unless we use one of the provided models)</p>

<d-code block="" language="python">
    def forward(self, x_source, edge_index, x_weights, x_target=None) -&gt; torch.Tensor:
        # Construct the edge index tensor of size (2, n_boundaries)
        # x_weights is indexed with send_idx because there might be more relationships
        # than r-cells, in that case the weights are not aligned 
        x_message = torch.cat((x_source[send_idx], x_target[recv_idx], x_weights), dim=1) 

        if self.weight_1 is not None:
            x_message = torch.mm(x_message, self.weight_1)
            if self.biases_1 is not None:
                x_message += self.biases_1
            x_message = self.update(x_message)
        if self.weight_2 is not None:
            x_message = torch.mm(x_message, self.weight_2)
            if self.biases_2 is not None:
                x_message += self.biases_2
            x_message = self.update(x_message)
        if self.weight_3 is not None:
            x_message_weights = torch.mm(x_message, self.weight_3)
            if self.biases_3 is not None:
                x_message_weights += self.biases_3
            x_message_weights = torch.nn.functional.sigmoid(x_message_weights)
        else:
            x_message_weights = torch.ones_like(x_message)

        # Weight the message by the learned weights
        x_message = x_message * x_message_weights

        return x_message
</d-code>

<h2 id="4-application">4. Application</h2>

<p>Now, on to the most important part. We con now very easily execute our model. To load our dataset, it is straightforward</p>
<d-code block="" language="python">
dataset_name = "manual_dataset"
dataset_config = load_dataset_config(dataset_name)
loader = GraphLoader(dataset_config)
dataset = loader.load()
</d-code>

<p>We can also visualize the information in <strong>point cloud</strong>, <strong>simplicial complex</strong>, or <strong>cell complex</strong> domains. Figure 3 is one such of test <strong>Simplicial Complex</strong></p>
<d-code block="" language="python">
describe_data(dataset)
</d-code>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/lifted_clique_complex-480.webp 480w,/assets/img/2024-06-30-smpn/lifted_clique_complex-800.webp 800w,/assets/img/2024-06-30-smpn/lifted_clique_complex-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/lifted_clique_complex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>After setting the configurations, using a lifting procedure is as easy as defining it’s name.</p>
<d-code block="" language="python">

# Define transformation type and id
transform_type = "liftings"
# If the transform is a topological lifting, 
it should include both the type of the lifting and the identifier
transform_id = "graph2simplicial/empsn_lifting"

# Read yaml file
transform_config = {
    "lifting": load_transform_config(transform_type, transform_id)
    # other transforms (e.g. data manipulations, feature liftings) can be added here
}
lifted_dataset = PreProcessor(dataset, transform_config, loader.data_dir)
</d-code>
<p>Finally, we load our model and execute it.</p>
<d-code block="" language="python">
from modules.models.simplicial.empsn import EMPSNModel

model_type = "simplicial"
model_id = "empsn"
model_config = load_model_config(model_type, model_id)

model = EMPSNModel(model_config, dataset_config)oy_hat = model(lifted_dataset.get(0))
y_hat = model(lifted_dataset.get(0))

</d-code>

<hr />

<h1 id="experiments">Experiments</h1>

<p>We performed experiments with the implementation in the TopoX framework. Additionally, we vectorized and improved the following sections: 1) the lifting procedure now results in a smaller file size, and the vectorization of the feature embeddings and adjacency/incidence matrix calculation with the help of TopoX is faster; 2) we organized the computation of the invariants, and further vectorized some operations.</p>

<p>Next, we present a comparison of the lifting times of the whole QM9 dataset, the size of the pre-processed file (after lifting), and the time it takes to run a forward pass.</p>

<h2 id="lifting-times">Lifting times</h2>

<p>The lifting procedure has two challenging areas optimized: 1) calculation of the adjacency and incidence relationships for which we rely on TopoX, and 2) feature lifting, which we manually optimized. We brought down the lifting time from about 1.5 hours to about 28 minutes on the same hardware. Next, we show the lifting time of each graph in the dataset and compare the total lifting time. We see that our optimization could be much faster on the heavier graphs. However, there is a set that is reduced to the baseline.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/lift_time_exp-480.webp 480w,/assets/img/2024-06-30-smpn/lift_time_exp-800.webp 800w,/assets/img/2024-06-30-smpn/lift_time_exp-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/lift_time_exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/total_lift_time-480.webp 480w,/assets/img/2024-06-30-smpn/total_lift_time-800.webp 800w,/assets/img/2024-06-30-smpn/total_lift_time-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/total_lift_time.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h2 id="file-size">File size</h2>
<p>We observed a reduction in the size of the preprocessed dataset after the lifting procedure. This behavior is due to the way incidences and adjacencies are stored. Instead of having the invariance information directly, we can store the relationships, such as boundaries, upper adjacencies, and embeddings, which makes it enough. We could have also stored these as sparse tensors; however, handling the mini-batching proved cumbersome on that representation. Ultimately, we reduced the size from 8.7GB to 6.3 GB.</p>

<h2 id="forward-pass">Forward pass</h2>

<p>One of these models’ bottlenecks is the time they take to train for an epoch. The original version took close to 70 hours in the cluster we had access to. Some of this time is related to the number of messages passing and some to calculating the invariances, which must be done each forward pass. We managed to optimize this calculation and thus speed up the execution of the model almost ten-fold. These results are calculated over the forward on a batch size of \(96\), as per the original implementation, and take into account \(96\) batches.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/total_forward_pass_time-480.webp 480w,/assets/img/2024-06-30-smpn/total_forward_pass_time-800.webp 800w,/assets/img/2024-06-30-smpn/total_forward_pass_time-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/total_forward_pass_time.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h2 id="replication-of-results">Replication of results</h2>

<p>On the side of replication, we executed the original, publicly available code shown in Figure 4. Figure 5 compares our implementation and execution using the same hyperparameters set in the code. Due to computing constraints, we could not run experiments on the number of epochs set in the original codebase. For that reason, we report on two particular experiments:</p>

<ol>
  <li>Execution of the original implementation up to epoch ~700.</li>
</ol>

<div class="row mt-3">
    <div class="col-md mt-3 mt-md-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/rep_validation-480.webp 480w,/assets/img/2024-06-30-smpn/rep_validation-800.webp 800w,/assets/img/2024-06-30-smpn/rep_validation-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/rep_validation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 4: Base code - Validation MAE</figcaption>
  
</figure>

    </div>
</div>

<ol>
  <li>Execution of our implementation up to epoch ~100.</li>
</ol>

<div class="row mt-3">
    <div class="col-md mt-3 mt-md-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/mae-480.webp 480w,/assets/img/2024-06-30-smpn/mae-800.webp 800w,/assets/img/2024-06-30-smpn/mae-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/mae.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
    <figcaption class="caption">Figure 5: Our code - Validation MAE</figcaption>
  
</figure>

    </div>
</div>
<p>Our scores are not near SOTA or the reported scores in the original work. Nevertheless, thanks to the optimizations mentioned above, we were able to execute different tests during the procedure for a reduced number of epochs. Varying batch sizes and learning rates, as well as using other common weight initialization techniques, did not improve the results.</p>

<hr />

<h1 id="conclusions">Conclusions</h1>

<p>In this post, we superficially introduced the field of topological deep learning and placed it in the field of graph neural networks. Additionally, we investigated the novel development suite for Topological Deep Learning (TopoX) and how it can be used to tackle a particular problem. We review concepts in <strong>geometric deep learning</strong> and show why they work and how we can leverage topological representations to better learn in message-passing networks. Using the unified TopoX framework allows for ease of development and standardization regarding reproducibility. Additionally, optimization for computationally heavy procedures such as the ones inherent to TDL is more straightforward.  Additionally, we replicate the work of <d-cite key="eijkelboom2023n"></d-cite>. Based on our tests, we could not reach the reported results. However, we took the original repository and replicated the implementation in TopoX. Thus, we cannot know the configurations that achieved the presented results.</p>]]></content><author><name>Martin Carrasco</name></author><category term="TDL," /><category term="GDL" /><summary type="html"><![CDATA[Studying the properties of message passing accross topological neural networks using the TopoX Suite]]></summary></entry><entry><title type="html">Do Transformers Really Perform Bad for Graph Representation?</title><link href="http://localhost:4000/blog/2024/graphormer/" rel="alternate" type="text/html" title="Do Transformers Really Perform Bad for Graph Representation?" /><published>2024-06-30T00:00:00+02:00</published><updated>2024-06-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/graphormer</id><content type="html" xml:base="http://localhost:4000/blog/2024/graphormer/"><![CDATA[<!-- abcd -->

<h2 id="introduction">Introduction</h2>
<p>The Transformer architecture has revolutionized sequence modelling. Its versatility is demonstrated by its application in various domains, from natural language processing to computer vision to even reinforcement learning. With its strong ability to learn rich representations across domains, it seems natural that the power of the transformer can be adapted to graphs.</p>

<p>The main challenge with applying a transformer to graph data is that there is no obvious sequence-based representation of graphs. Graphs are commonly represented by adjacency matrices or lists, which lack inherent order and are thus unsuitable for transformers.</p>

<p>The primary reason for finding a sequence-based representation of a graph is to combine the advantages of a transformer (such as its high scalability) with the ability of graphs to capture non-sequential and multidimensional relationships. Graph Neural Networks (GNNs) employ various constraints during training, such as enforcing valency limits when generating molecules. However, choosing such constraints may not be as straightforward for other problems. With Graphormer, we can apply these very constraints in a simpler manner, analogous to applying a causal mask in a transformer. This can also aid in discovering newer ways to apply constraints in GNNs by presenting existing concepts in an intuitive manner.</p>

<p>Graphormer introduces Centrality Encoding to capture the node importance, Spatial Encoding to capture the structural relations, and Edge Encoding to capture the edge features. In addition to this, Graphormer makes other architectures easier to implement by making various existing architecture special cases of Graphormer, with the performance to boot.</p>

<hr />

<h2 id="preliminaries">Preliminaries</h2>

<ul>
  <li><strong>Graph Neural Networks (GNNs)</strong>: Consider a graph \(G = \{V, E\}\) where \(V = \{v_1, v_2, \cdots, v_n\}\) and \(n = |V|\) is the number of nodes. Each node \(v_i\) has a feature vector \(x_i\). Modern GNNs update node representations iteratively by aggregating information from neighbours. The representation of node \(v_i\) at layer \(l\) is \(h^{(l)}_i\), with \(h_i^{(0)} = x_i\). The aggregation and combination at layer \(l\) are defined as: 
\(a_{i}^{(l)}=\text{AGGREGATE}^{(l)}\left(\left\{h_{j}^{(l-1)}: j \in \mathcal{N}(v_i)\right\}\right)\) 
\(h_{i}^{(l)}=\text{COMBINE}^{(l)}\left(h_{i}^{(l-1)}, a_{i}^{(l)}\right)\) 
where \(\mathcal{N}(v_i)\) is the set of first or higher-order neighbours of \(v_i\). Common aggregation functions include MEAN, MAX, and SUM. The COMBINE function fuses neighbor information into the node representation. For graph-level tasks, a READOUT function aggregates node features \(h_i^{(L)}\) from the final iteration into a graph representation \(h_G\):
\(h_{G}=\operatorname{READOUT}\left(\left\{h_{i}^{(L)} \mid v_i \in G \right\}\right)\)
READOUT can be a simple summation or a more complex pooling function.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/gnn-480.webp 480w,/assets/img/2024-06-30-graphormer/gnn-800.webp 800w,/assets/img/2024-06-30-graphormer/gnn-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/gnn.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A message-passing neural network. Note how the node states flow from outer to inner layers, with pooling at each step to update states.<d-cite key="GoogleResearch"></d-cite>
</div>

<ul>
  <li><strong>Transformer</strong>: The Transformer architecture comprises layers with two main components: a self-attention module and a position-wise feed-forward network (FFN). Let \(H = [h_1^\top, \cdots, h_n^\top]^\top\in ℝ^{n\times d}\) be the input to the self-attention module, where \(d\) is the hidden dimension and \(h_i\in ℝ^{1\times d}\) is the hidden representation at position \(i\). The input \(H\) is projected using matrices \(W_Q\inℝ^{d\times d_K}, W_K\inℝ^{d\times d_K}\), and \(W_V\inℝ^{d\times d_V}\) to obtain representations \(Q, K, V\). Self-attention is computed as:
\(Q = HW_Q,\ K = HW_K,\ V = HW_V,\ A = \frac{QK^\top}{\sqrt{d_K}},\ Attn(H) = \text{softmax}(A)V\)
where \(A\) captures the similarity between queries and keys. This self-attention mechanism allows the model to understand relevant information in the sequence comprehensively.</li>
</ul>

<!-- For simplicity of illustration, we consider the single-head self-attention and assume $$d_K = d_V = d$$. The extension to the multi-head attention is standard and straightforward, and we omit bias terms for simplicity. xxxx One of the main properties of the Transformer that makes it so effective in processing sequences is its ability to model long-range dependencies and contextual information with its receptive field. Specifically, each token in the input sequence can interact with (or pay “attention” to) every other token in the sequence when transforming its representation xxxx. -->

<!-- ![ [Source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)](Spatial%20Encoding%20d515dd50b6354ab19b8310fab3005464/Untitled.png) -->
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/head-view-480.webp 480w,/assets/img/2024-06-30-graphormer/head-view-800.webp 800w,/assets/img/2024-06-30-graphormer/head-view-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/head-view.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    An illustration of the attention mechanism. Notice how each word(or token) can attend to different parts of the sequence, forward or backward.<d-cite key="Vig2024"></d-cite>.
</div>

<!-- An illustration of attention mechanism at play for a translation task. Notice how each word(or token) can attend to different parts of the sequence, forward or backward. [Source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) -->

<hr />
<h2 id="graphormer">Graphormer</h2>
<h3 id="centrality-encoding">Centrality Encoding</h3>

<p>In a sequence modelling task, Attention captures the semantic correlations between the nodes (tokens).
The goal of this encoding is to capture the most important nodes in the graph.
Let’s take an example.
Say we want to compare airports and find which one is the largest.
We need a common metric to compare them, so we take the sum of the total daily incoming and outgoing flights, giving us the busiest airports. This is what the algorithm is doing logically to identify the ‘busiest’ nodes.
Additionally, the learnable vectors allow the Graphormer to ‘map’ out the nodes. All this culminates in better performance for graph-based tasks such as molecule generation.</p>

<p>This is the Centrality Encoding equation, given as:</p>

\[h_{i}^{(0)} = x_{i} + z^{-}_{deg^{-}(v_{i})} + z^{+}_{deg^{+}(v_{i})}\]

<p>Let’s analyse this term by term:</p>

<ul>
  <li>\(h_{i}^{(0)}\) - Representation (\(h\)) of vertice i (\(v_{i}\)) at the 0th layer (first input)</li>
  <li>\(x_{i}\) - Feature vector of vertice i (\(v_{i}\))</li>
  <li>\(z^{-}_{deg^{-}(v_{i})}\) - Learnable embedding vector (\(z\)) of the indegree (\(deg^{-}\)) of vertice i (\(v_{i}\))</li>
  <li>\(z^{+}_{deg^{+}(v_{i})}\) - Learnable embedding vector (\(z\)) of the outdegree (\(deg^{+}\)) of vertice i (\(v_{i}\))</li>
</ul>

<p>This is an excerpt of the code used to compute the Centrality Encoding</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">in_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_in_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="n">self</span><span class="p">.</span><span class="n">out_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_out_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Intial node feature computation.
</span><span class="n">node_feature</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_feature</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">in_degree_encoder</span><span class="p">(</span><span class="n">in_degree</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_degree_encoder</span><span class="p">(</span><span class="n">out_degree</span><span class="p">))</span>
</code></pre></div></div>

<!-- num_in_degree is the indegree and hidden_dim is the size of the embedding vector - the Embedding function call converts this number (indegree) to a learnable vector of size hidden_dim, which is then added to the node_feature. A similar procedure is done with num_out_degree, resulting in the implementation of Equation 5. -->

<!-- <put simple explanation first then equations and code> - talk about graph based example -->

<hr />

<h3 id="spatial-encoding">Spatial Encoding</h3>

<p>There are several methods for encoding the position information of the tokens in a sequence.
In a graph, however, there is a problem. Graphs consist of nodes (analogous to tokens) connected with edges in a non-linear, multi-dimensional space. There’s no inherent notion of an “ordering” or a “sequence” in its structure, but as with positional information, it’ll be helpful if we inject some sort of structural information when we process the graph.</p>

<!-- A naive solution would be to learn the encodings themselves. Another would be to perform some operation on the graph structure, such as a random walk, or components from the feature matrix. The intuition is to perform an operation on the graph to extract some “structural” information.  -->

<p>The authors propose a novel encoding called <em>Spatial Encoding</em>. Take a pair of nodes (analogous to tokens) as input and output a scalar value as a function of the shortest path distance (SPD) between the nodes. This scalar value is then added to the element corresponding to the operation between the two nodes in the Query-Key product matrix.</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i, v_j)}\]

<p>The above equation shows the modified computation of the Query-Key Product matrix. Notice that the additional term \(b_{\phi(v_i, v_j)}\)  is a learnable scalar value and acts like a bias term. Since this structural information is independent of which layer of our model is using it, we share this value across all layers.</p>

<p>The benefits of using such an encoding are:</p>
<ol>
  <li>Our receptive field has effectively increased, as we are no longer limited to the information from our neighbours, as is what happens in conventional message-passing networks.</li>
  <li>The model determines the best way to adaptively attend to the structural information. For example, if the scalar valued function is a decreasing function for a given node, we know that the nodes closer to our node are more important than the ones farther away.</li>
</ol>

<hr />

<h3 id="edge-encoding">Edge Encoding</h3>

<p>Graphormer’s edge encoding method significantly enhances the way the model incorporates structural features from graph edges into its attention mechanism. The prior approaches either add edge features to node features or use them during aggregation, propagating the edge information only to associated nodes. Graphormer’s approach ensures that edges play a vital role in the overall node correlation.</p>

<p>Initially, node features \((h_i, h_j)\) and edge features \((x_{e_n})\) from the shortest path between nodes are processed. For each pair of nodes \((v_i, v_j)\), the edge features on the shortest path \(SP_{ij}\) are averaged after being weighted by learnable embeddings \((w^E_n)\), this results in the edge encoding \(c_{ij}\):</p>

\[c_{ij} = \frac{1}{N} \sum_{n=1}^{N} x_{e_n} (w^E_n)^T\]

<p>This is then incorporated as the edge features into the attention score between nodes via a bias-like term. After incorporating the edge and spatial encodings, the value of \(A_{ij}\) is now:</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i,v_j)} + c_{ij}\]

<p>This ensures that edge features directly contribute to the attention score between any two nodes, allowing for a more nuanced and comprehensive utilization of edge information. The impact is significant, and it greatly improves the performance, as proven empirically in the Experiments section.</p>

<hr />
<h3 id="vnode">VNode</h3>

<p>The \([VNode]\) (or a Virtual Node) is arguably one of the most important contributions from the work. It is an artificial node that is connected to <b>all</b> other nodes. The authors cite this paper<d-cite key="gilmer2017neuralmessagepassingquantum"></d-cite> as an empirical motivation, but a better intuition behind the concept is as a generalization of the [CLS] token widely used in NLP and Vision. 
<!-- The sharp reader will notice that this has an important implication on $b$ and $\phi$, because the $$[VNode]$$ is connected to every node, --></p>

\[\phi([VNode], v) = 1, \forall v \in G\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> <!--Dummy divs to take up space, need to do this because height, width tags don't work with the given image class-->
    </div>
    <div class="col-sm-6 mt-3 mt-md-0"> <!-- Note  this is a trick to make the image small keep it center but also not too small (using -6)-->
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/vnode3-480.webp 480w,/assets/img/2024-06-30-graphormer/vnode3-800.webp 800w,/assets/img/2024-06-30-graphormer/vnode3-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/vnode3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<p>Since this is not a <b>physical connection</b>, \(b_{\phi([VNode], v)}\) is set to be a <b>distinct</b> learnable vector (for all \(v\)) to provide the model with this important geometric information.</p>

<p>[CLS] tokens are often employed as “summary” tokens for text and provide a global context to the model. With graphs and text being different modalities, the \([VNode]\) also helps in <b>relaying</b> global information to distant or non-connected clusters in a graph. This is significantly important to the model’s expressivity, as this information might otherwise never propagate. In fact, the \([VNode]\) becomes a learnable and dataset-specific READOUT function.</p>

<!-- As we pointed out, \[CLS\] tokens are used for varied downstream tasks, in a similar way, $$[VNode]$$ can be (and is) used as the final representation of the Graph, i.e., this becomes a learnable and dataset-specfic READOUT function! -->

<p>This can be implemented as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Initialize the VNode
</span>    <span class="n">self</span><span class="p">.</span><span class="n">v_node</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="c1"># one per head (different from CLS)
</span>    <span class="bp">...</span>
    <span class="c1"># During forward pass (suppose VNode is the first node)
</span>    <span class="bp">...</span>
    <span class="n">headed_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">v_node</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">headed_emb</span>
        <span class="c1">#(n_graph, n_heads, n_nodes + 1, n_nodes + 1)
</span>    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">headed_emb</span>
    <span class="bp">...</span>
</code></pre></div></div>

<p>Again, we emphasise that the information-relay point of view is much more important to the model than the summary-token view. The design choice of one \([VNode]\) per head reflects that.</p>

<hr />

<h2 id="theoretical-aspects-of-expressivity">Theoretical aspects of expressivity</h2>

<p>These are the three main facts from the paper,</p>

<ol>
  <li>With appropriate weights and \(\phi\), GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GraphSAGE<d-cite key="hamilton2018inductiverepresentationlearninglarge"></d-cite>, and GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite> are all <b>special cases</b> of a Graphormer.</li>
  <li>Graphormer is better than architectures that are limited by the 1-WL test. (so <b>all</b> traditional GNNs!)</li>
  <li>With appropriate weights, <b>every node</b> representation in the output can be MEAN-READOUT.</li>
</ol>

<p>The <a href="#spatial-encoding">spatial encoding</a> provides the model with important geometric information. Observe that with an appropriate \(b_{\phi(v_i, v_j)}\), the model can <b>find (learn)</b> neighbours for any \(v_i\) and thus easily implement <b>mean-statistics (GCN!)</b>. By knowing the degree (some form of <a href="#centrality-encoding">centrality encoding</a>), mean-statistics can be transformed into sum-statistics; it (indirectly) follows that various statistics can be learned by different heads, which leads to varied representations and allows GraphSAGE, GIN or GCN to be modeled as a Graphormer.</p>

<p>Fact 2 follows from Fact 1, with GIN being the most powerful traditional GNN, which can theoretically identify all graphs distinguishable by the 1-WL test, as it is now a special case of Graphormer. The latter can do the same (&amp; more!).</p>

<p>More importantly, Fact 3 implies that Graphormer allows the flow of <i>Global</i> (and Local) information within the network. This truly sets the network apart from traditional GNNs, which can only aggregate local information up to a fixed radius (or depth).</p>

<p>Traditional GNNs are <i>designed</i> to prevent this type of flow, as with their architecture, this would lead to over-smoothening. However, the clever design around \([VNode]\) prevents this from happening in Graphormer. The addition of a supernode along with Attention and the learnable \(b_{\phi(v_i, v_j)}\) facilitate this, the \([VNode]\) can relay global information, and the attention mechanism can selectively choose from there.</p>

<hr />
<h2 id="experiments">Experiments</h2>

<p>The researchers conducted comprehensive experiments to evaluate Graphormer’s performance against state-of-the-art models like GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite>, DeeperGCN<d-cite key="li2020deepergcnneedtraindeeper"></d-cite>, and the Transformer-based GT<d-cite key="dwivedi2021generalizationtransformernetworksgraphs"></d-cite>.</p>

<p>Two variants of Graphormer, <em>Graphormer</em> (L=12, d=768) and a smaller <em>GraphormerSMALL</em> (L=6, d=512), were evaluated on the <a href="https://ogb.stanford.edu/docs/lsc/">OGB-LSC</a> quantum chemistry regression challenge (PCQM4M-LSC), one of the largest graph-level prediction dataset with over 3.8 million graphs.</p>

<p>The results, as shown in Table 1, demonstrate Graphormer’s significant performance improvements over previous top-performing models such as GIN-VN, DeeperGCN-VN, and GT.</p>

<p>Table 1: Results on PCQM4M-LSC</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Train MAE</th>
      <th>Validate MAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GIN-VN</td>
      <td>6.7M</td>
      <td>0.1150</td>
      <td>0.1395</td>
    </tr>
    <tr>
      <td>DeeperGCN-VN</td>
      <td>25.5M</td>
      <td>0.1059</td>
      <td>0.1398</td>
    </tr>
    <tr>
      <td>GT</td>
      <td>0.6M</td>
      <td>0.0944</td>
      <td>0.1400</td>
    </tr>
    <tr>
      <td>GT-Wide</td>
      <td>83.2M</td>
      <td>0.0955</td>
      <td>0.1408</td>
    </tr>
    <tr>
      <td>GraphormerSMALL</td>
      <td>12.5M</td>
      <td>0.0778</td>
      <td>0.1264</td>
    </tr>
    <tr>
      <td>Graphormer</td>
      <td>47.1M</td>
      <td>0.0582</td>
      <td>0.1234</td>
    </tr>
  </tbody>
</table>

<p>Notably, Graphormer did not encounter over-smoothing issues, with both training and validation errors continuing to decrease as model depth and width increased, thereby going beyond the <a href="https://web.stanford.edu/class/cs224w/slides/06-theory.pdf#page=46">1-WL test</a>. Additionally, Graph Transformer (GT) showed no performance gain despite a significant increase in parameters from GT to GT-Wide, highlighting Graphormer’s scaling capabilities.</p>

<p>Further experiments for graph-level prediction tasks were performed on datasets from popular leaderboards like <a href="https://ogb.stanford.edu/docs/graphprop/#ogbg-mol">OGBG</a> (MolPCBA, MolHIV) and <a href="https://paperswithcode.com/paper/benchmarking-graph-neural-networks">benchmarking-GNNs</a> (ZINC), which also showed Graphormer consistently outperforming top-performing GNNs.</p>

<p>By using the ensemble with ExpC<d-cite key="yang2020breakingexpressivebottlenecksgraph"></d-cite>, Graphormer was able to reach a 0.1200 MAE and win the graph-level track in the OGB Large-Scale Challenge.</p>

<h3 id="comparison-against-state-of-the-art-molecular-representation-models">Comparison against State-of-the-Art Molecular Representation Models</h3>

<p>Let’s first take a look at GROVER<d-cite key="rong2020selfsupervisedgraphtransformerlargescale"></d-cite>, a transformer-based GNN boasting 100 million parameters and pre-trained on a massive dataset of 10 million unlabeled molecules.</p>

<p>The authors further fine-tune GROVER on MolHIV and MolPCBA to achieve competitive performance along with supplying additional molecular features such as morgan fingerprints and other 2D features. Note that the Random Forest model fitted on these features alone outperforms the GNN model, showing the huge boost in performance granted by the same.</p>

<p>Table 2: Comparison between Graphormer and GROVER on MolHIV</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th># param.</th>
      <th>AUC (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Morgan Finger Prints + Random Forest</td>
      <td>230K</td>
      <td>80.60±0.10</td>
    </tr>
    <tr>
      <td>GROVER</td>
      <td>48.8M</td>
      <td>79.33±0.09</td>
    </tr>
    <tr>
      <td>GROVER (LARGE)</td>
      <td>107.7M</td>
      <td>80.32±0.14</td>
    </tr>
    <tr>
      <td>Graphormer-FLAG</td>
      <td>47.0M</td>
      <td>80.51±0.53</td>
    </tr>
  </tbody>
</table>

<p>However, as evident in Table 2, Graphormer manages to outperform it consistently on the benchmarks without even using the additional features (known to boost performance), which showcases it increases the expressiveness of complex information.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Graphormer presents a novel way of applying Transformers to graph representation using the three structural encodings. While it has demonstrated strong performance across various benchmark datasets, significant progress has been made since the original paper. Structure-Aware Transformer <d-cite key="chen2022structureawaretransformergraphrepresentation"></d-cite> improves on the initial Transformer by incorporating structural information by extracting subgraph representations. DeepGraph <d-cite key="zhao2023layersbeneficialgraphtransformers"></d-cite> explores the benefits of deeper graph transformers by enhancing global attention with substructure tokens and local attention. Despite the success of these architectures, some challenges still remain; for example, the quadratic complexity of the self-attention module limits its use on large graphs. Therefore, the future development of efficient sequence-based graph-processing networks and the imposing of such constraints for geometric learning are open research areas.</p>]]></content><author><name>Anonymized</name></author><category term="graph" /><category term="representation" /><category term="learning" /><summary type="html"><![CDATA[A first-principles blog post to understand the Graphormer.]]></summary></entry><entry><title type="html">Cancer Immunotherapy Design with Geometric Deep Learning</title><link href="http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI/" rel="alternate" type="text/html" title="Cancer Immunotherapy Design with Geometric Deep Learning" /><published>2024-06-13T00:00:00+02:00</published><updated>2024-06-13T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI</id><content type="html" xml:base="http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI/"><![CDATA[<h1 id="cancer-immunotherapy-design-with-geometric-deep-learning">Cancer Immunotherapy Design with Geometric Deep Learning</h1>

<p>Cancer remains one of the most formidable challenges in medicine, but recent advancements in immunotherapy and artificial intelligence (AI) are opening new frontiers in treatment. This blog post explores the intersection of cancer immunotherapy and AI.</p>

<h2 id="what-is-cancer">What is Cancer?</h2>

<p>Cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. It begins when genetic mutations in a cell’s DNA disrupt the normal process of cell growth, division, and death. This leads to the formation of tumors, which can be benign (non-cancerous) or malignant (cancerous). Malignant tumors can invade nearby tissues and metastasize, spreading to other parts of the body. There are various types of cancer, classified by the type of cell initially affected <d-cite key="hanahan2011hallmarks"></d-cite>. Treatment options vary and may include surgery, radiation therapy, chemotherapy, targeted therapy, and immunotherapy.</p>

<h2 id="why-is-cancer-so-difficult-to-cure">Why is Cancer So Difficult to Cure?</h2>

<p>Cancer is challenging to cure due to its genetic diversity and constant mutation, leading to treatment resistance <d-cite key="siegel2020colorectal"></d-cite>. Tumors consist of heterogeneous cells, making it difficult for a single therapy to target all cancer cells effectively. Additionally, cancer cells can invade nearby tissues and metastasize, forming secondary tumors that are hard to detect and treat. The ability of cancer cells to evade the immune system and the protective nature of the tumor microenvironment further complicate treatment efforts. Moreover, many cancer treatments, such as chemotherapy and radiation, can harm healthy cells, causing severe side effects and limiting safe dosage levels. Late detection often means the disease is more advanced and harder to treat successfully. Personalized treatment plans are necessary but complex to develop, as each patient’s cancer is unique. These factors underscore the need for ongoing research to develop more effective and targeted therapies.</p>

<h2 id="what-is-cancer-immunotherapy-and-what-makes-it-different">What is Cancer Immunotherapy and What Makes It Different?</h2>

<p>Cancer immunotherapy is a treatment that enhances the body’s immune response to fight cancer. It involves mechanisms such as the Major Histocompatibility Complex (MHC) and T-cell response. MHC is a protein complex found on almost all nucleated cells in the body that presents fragments of proteins (antigens) on their surface. These antigens can be normal self-antigens or, in the case of cancer cells, abnormal or mutated antigens. T-cells, a type of white blood cell, patrol the body searching for these antigens. When a T-cell recognizes an abnormal antigen presented by the MHC on a cancer cell, it becomes activated and can kill cancer cells directly or recruit other immune cells to help eliminate the cancer.</p>

<p>Cancer immunotherapy is relatively new compared to traditional treatments like surgery, chemotherapy, and radiation. The concept dates back over a century, but significant advancements have been made in the past few decades. The first immune checkpoint inhibitor, ipilimumab (Yervoy), was approved by the FDA in 2011 <d-cite key="pardoll2012blockade"></d-cite>.</p>

<p>The main types of cancer immunotherapy include immune checkpoint inhibitors, CAR-T cell therapy, and cancer vaccines. Immune checkpoint inhibitors block proteins that prevent the immune system from attacking cancer cells more effectively. CAR-T cell therapy involves modifying a patient’s T cells to target and kill cancer cells <d-cite key="june2018car"></d-cite>. Cancer vaccines stimulate the immune system to attack cancer cells by presenting them with specific antigens found on cancer cells <d-cite key="melief2015therapeutic"></d-cite>.</p>

<p>Immunotherapy differs from traditional treatments in several ways. It enhances the body’s immune response to identify and attack cancer cells, potentially leading to long-lasting protection against cancer recurrence. Immunotherapy can be highly specific, targeting only cancer cells and sparing normal cells, reducing side effects compared to traditional therapies. Some patients experience long-term remission with immunotherapy, as the immune system can continue to recognize and attack cancer cells even after treatment ends. Additionally, immunotherapy can be combined with other cancer treatments to enhance overall effectiveness.</p>

<p>These distinctive features make cancer immunotherapy a promising and rapidly evolving area in cancer treatment. Understanding the interaction between the immune system and cancer is key to overcoming the challenges and improving the effectiveness of these therapies. Therefore, it is essential to explore how AI can enhance the development and effectiveness of cancer immunotherapy.</p>

<h2 id="ai-for-cancer-immunotherapy">AI for Cancer Immunotherapy</h2>

<p>The two most important interactions are peptide-MHC interaction, to predict which peptides will be shown at the cell surface, and MHC-T-Cell interaction. In this blog post, I will focus on peptide-MHC structure modeling. Accurate modeling of the 3D structure and properties of p-MHC can lead to better-targeted therapies and improved immune responses against cancer cells. Additionally, speed is extremely important due to the massive number of peptide-MHC pairs for each patient and the diversity of MHCs. While there is a very good physical simulation model called Pandora <d-cite key="marzella2022pandora"></d-cite>, it is much slower than potential neural network-based models. One major challenge for using neural networks here is that there is very limited data available, making it challenging to generalize from.</p>

<h2 id="peptide-mhc-structure-modeling-with-diffusion-models">Peptide-MHC Structure Modeling with Diffusion Models</h2>

<p>We need to choose a framework for modeling p-MHC. AI for structural biology has developed quickly in the last few years. The first major breakthroughs in using AI for biology came with non-generative models like AlphaFold 2 <d-cite key="jumper2021highly"></d-cite> and RoseTTAFold <d-cite key="baek2021accurate"></d-cite>. AlphaFold 2 and RoseTTAFold revolutionized protein structure prediction by accurately modeling the three-dimensional shapes of proteins from their amino acid sequences. Both models have significantly advanced our understanding of protein folding, paving the way for new therapeutic discoveries. However, generative models represent the next frontier in AI for biology. Unlike non-generative models that predict static outcomes, generative models can create new data instances, offering more dynamic and flexible solutions. Diffusion models <d-cite key="yang2023diffusion"></d-cite>, flow matching <d-cite key="lipman2022flow"></d-cite>, and GFlowNets <d-cite key="bengio2023gflownet"></d-cite> are prominent examples of generative models. Diffusion models, in particular, have gained popularity for their ability to generate high-quality molecular structures by simulating the gradual process of denoising from a random state.</p>

<p>Work using Diffusion Models such as DiffSBDD <d-cite key="schneuing2022structure"></d-cite> has shown that it is possible to create small molecules for protein pockets using diffusion models. RF-Diffusion has demonstrated the ability to design the backbone structure of proteins with these models. Very recently, AlphaFold 3 <d-cite key="abramson2024accurate"></d-cite> has put diffusion models in the spotlight again by replacing AlphaFold 2’s structure prediction model, extending the structure prediction capabilities from proteins to any multi-component complex in biology. Given their flexibility and power, we choose a diffusion model for our task.</p>

<h3 id="choosing-an-input-representation">Choosing an Input Representation</h3>

<p>For p-MHC we are dealing with a large number of degrees of freedom. For this reason, our first aim is to reduce the degrees of freedom with prior knowledge as much as possible. It has been shown that modeling peptides and proteins at their amino acid level is sufficient to model the full structure. This means that instead of modelling every single atom of the peptide and the MHC pocket we only model a single node for each amino acid. We can later add back the remaining atoms using a simple non-generative regression model <d-cite key="dauparas2022robust"></d-cite>. For the amino acid, we can use several representations:</p>

<ul>
  <li><strong>Frame-based representation:</strong> Captures the structure of the entire amino acid.</li>
  <li><strong>C-alpha only representation:</strong> Uses only the alpha carbon of the amino acid, simplifying the structure.</li>
  <li><strong>C-alpha + side chain orientation representation:</strong> Includes both the alpha carbon and the side chain orientation, providing more detailed structural information.</li>
</ul>

<p>In this blog post, we will discuss C-alpha only representation and C-alpha + side chain orientation representation. Since only the protein pocket is relevant for the binding process and its structure is largely fixed, we can use AlphaFold2 to predict the structure of the MHC and its pocket. There are a lot more peptides than there are MHCs for each person, which makes this approach computationally feasible.</p>

<p>First, we use the 3D position of the alpha carbon (C-alpha only representation) and a one hot encoding of the amino acid type as node feature.</p>

<h3 id="concept-of-equivariance-and-equivariant-neural-networks">Concept of Equivariance and Equivariant Neural Networks</h3>

<p>For the neural network, we can use our knowledge of geometric representations. The joint translation and rotation of the peptide-MHC complex does not matter for its function. But if we use a normal graph neural network, then it would have to learn to treat a translated or rotated complex the same as the original complex. This usually requires more data and is generally harder. For our task, we want our neural network to be equivariant to rotation and translation but not to reflection, hence SE(3) normally. An equivariant function with respect to a group transforms the input in a way that applying a group action to the input is equivalent to applying the same group action to the output. There are different ways to encode equivariance into a network’s architecture. The most straightforward way is to only let the network use inherently equivariant information.</p>

<p>For a graph in (\mathbb{R}^3), the weighted sum of pairwise differences between nodes is inherently equivariant with respect to rotation and translation. This is the key idea behind the construction of the EGNN network <d-cite key="satorras2021n"></d-cite>, which is the default choice for equivariant diffusion models.</p>

<p>Using amino acids as nodes is very efficient but it does lose a lot of potential information, for instance, the amino acid sidechain orientation (C-alpha + side chain orientation representation). While the neural network might be able to learn this implicitly from enough data, we can also give the orientation as further input without increasing the number of nodes. This, however, changes the space that the SE(3) group acts on to (\mathbb{R}^3 \times S^2) or (\mathbb{R}^3 \times S^3). The advantage of using (\mathbb{R}^3 \times S^2) is that it is a lot more computationally efficient than indexing on (\mathbb{R}^3 \times S^3). Further, (\mathbb{R}^3 \times S^2) is sufficient to fully represent information on (\mathbb{R}^3 \times S^3).</p>

<p>Ponita <d-cite key="bekkers2023fast"></d-cite> is a new, very lean and fast architecture. It achieves equivariance on (\mathbb{R}^3 \times S^2) by defining a bijective mapping from any point pair to an invariant attribute such that any point pair in the equivalence class of the group G is mapped to the same attribute and any attribute maps only to one such equivalence class.</p>

\[\mathbb{R}^3 \times S^2 : \quad [(\mathbf{p}_i, \mathbf{o}_i), (\mathbf{p}_j, \mathbf{o}_j)] \quad \mapsto \quad a_{ij} = \left( \begin{array}{c} 
\| (\mathbf{p}_j - \mathbf{p}_i) - \mathbf{o}_i^\top (\mathbf{p}_j - \mathbf{p}_i) \mathbf{o}_i \| \\
\mathbf{o}_i^\top (\mathbf{p}_j - \mathbf{o}_i) \\
\arccos (\mathbf{o}_i^\top \mathbf{o}_j)
\end{array} \right)\]

<p>Ponita doesn’t have the high computational overhead of using specialized Clebsch-Gordan tensor products and is very efficient due to the ability to separate group convolutions over different group parts. It also achieves state-of-the-art performance on QM9.</p>

<p>Both using the C-alpha atom only representation with an EGNN and using the C-alpha atom + sidechain orientation representation with Ponita have shown great promise for modeling molecules and for conditional generation in my experiments. Peptide-MHC is difficult as the data we are working with is often not very diverse, which makes it hard to learn for generative models. There are numerous more implementation details involved in building a model like this, which exceed the scope of this blog post. The two most significant, which I want to mention, are encoding the location of the amino acids within the peptide chain as well as using energy guidance to guide the denoising process during sampling. When constructed correctly, a diffusion model can effectively predict the joint structure of peptide-MHCs.</p>

<p><img src="pMHC-Diffusion.png" alt="Diffusion Model generates peptide-MHC structure from random noise: Round points represent C-alpha atoms, while the rest forms the MHC protein pocket. Peptide nodes start as random noise and are progressively denoised by the trained diffusion model into the correct structure." /></p>

<h2 id="open-challenges">Open Challenges</h2>

<p>Beyond peptide-MHC interaction, many open problems in cancer immunotherapy can potentially be solved with AI. Modeling the structure of T-cells interacting with p-MHCs is one such problem. The regions of the T-cell receptors are highly variable, making predicting their structure even more challenging than that of p-MHC. Additionally, designing new TCRs (T-cell receptors) with feature diffusion is highly relevant for enhanced T-cell therapy, where TCRs are modified to target a patient’s specific cancer. Adding time dynamics to the structure modeling is also an intriguing direction, as biological complexes are always dynamic and change their structure to fulfill different functions at different times. Other promising methods for molecule generation include Diffusion Models, Flow Matching, and GFlow Networks. Finally, there is a significant need for better evaluation metrics. Current metrics often result in molecules that look good theoretically but fail in biological experiments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>AI has immense potential to revolutionize cancer immunotherapy by providing advanced tools to model and predict complex biological interactions. While significant progress has been made, many challenges remain. Continued advancements in AI and machine learning will be critical in overcoming these challenges and developing more effective, personalized cancer treatments.</p>

<h2 id="references">References</h2>

<p>[1] Josh Abramson et al. “Accurate structure prediction of biomolecular interactions with AlphaFold 3”. In: Nature (2024), pp. 1–3.<br />
[2] Minkyung Baek et al. “Accurate prediction of protein structures and interactions using a three-track neural network”. In: Science 373.6557 (2021), pp. 871–876.<br />
[3] Erik J Bekkers et al. “Fast, Expressive SE (n) Equivariant Networks through Weight-Sharing in Position-Orientation Space”. In: arXiv preprint arXiv:2310.02970 (2023).<br />
[4] Yoshua Bengio et al. “Gflownet foundations”. In: The Journal of Machine Learning Research 24.1 (2023), pp. 10006–10060.<br />
[5] Justas Dauparas et al. “Robust deep learning–based protein sequence design using ProteinMPNN”. In: Science 378.6615 (2022), pp. 49–56.<br />
[6] Douglas Hanahan and Robert A Weinberg. “Hallmarks of cancer: the next generation”. In: cell 144.5 (2011), pp. 646–674.<br />
[7] John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: nature 596.7873 (2021), pp. 583–589.<br />
[8] Carl H June et al. “CAR T cell immunotherapy for human cancer”. In: Science 359.6382 (2018), pp. 1361–1365.<br />
[9] Yaron Lipman et al. “Flow matching for generative modeling”. In: arXiv preprint arXiv:2210.02747 (2022).<br />
[10] Dario F Marzella et al. “PANDORA: a fast, anchor-restrained modelling protocol for peptide: MHC complexes”. In: Frontiers in Immunology 13 (2022), p. 878762.<br />
[11] Cornelis JM Melief et al. “Therapeutic cancer vaccines”. In: The Journal of clinical investigation 125.9 (2015), pp. 3401–3412.<br />
[12] Drew M Pardoll. “The blockade of immune checkpoints in cancer immunotherapy”. In: Nature reviews cancer 12.4 (2012), pp. 252–264.<br />
[13] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. “E (n) equivariant graph neural networks”. In: International conference on machine learning. PMLR. 2021, pp. 9323–9332.<br />
[14] Arne Schneuing et al. “Structure-based drug design with equivariant diffusion models”. In: arXiv preprint arXiv:2210.13695 (2022).<br />
[15] Rebecca L Siegel et al. “Colorectal cancer statistics, 2020”. In: CA: a cancer journal for clinicians 70.3 (2020), pp. 145–164.<br />
[16] Ling Yang et al. “Diffusion models: A comprehensive survey of methods and applications”. In: ACM Computing Surveys 56.4 (2023), pp. 1–39.</p>]]></content><author><name>Anonymous</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[Cancer Immunotherapy Design with Geometric Deep Learning]]></summary></entry><entry><title type="html">Learning Embedding Spaces with Metrics via Contrastive Learning</title><link href="http://localhost:4000/blog/2024/contrast-learning/" rel="alternate" type="text/html" title="Learning Embedding Spaces with Metrics via Contrastive Learning" /><published>2024-06-13T00:00:00+02:00</published><updated>2024-06-13T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/contrast-learning</id><content type="html" xml:base="http://localhost:4000/blog/2024/contrast-learning/"><![CDATA[<p>Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired properties, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo <d-cite key="he_momentum_2020"></d-cite>, <d-cite key="chen_empirical_2021"></d-cite>, SimCLR <d-cite key="chen_simple_2020"></d-cite>, <d-cite key="chen_big_2020"></d-cite>, and BYOL <d-cite key="grill_bootstrap_2020"></d-cite>, as well as supervised methods such as SupCon <d-cite key="khosla_supervised_2020"></d-cite> and SINCERE <d-cite key="feeney_sincere_2024"></d-cite>.</p>

<p>In contrastive learning, there are two components that determine the constraints on the learned embedding space: the similarity function and the contrastive loss. The similarity function takes a pair of embedding vectors and quantifies how similar they are as a scalar. The contrastive loss determines which pairs of embeddings have similarity evaluated and how the resulting set of similarity values are used to measure error with respect to a task, such as classification. Backpropagating to minimize this error causes a model to learn embeddings that best satisfy the constraints induced by the similarity function and contrastive loss.</p>

<p>This blog post examines how similarity functions and contrastive losses affect the learned embedding spaces. We first examine the different choices for similarity functions and contrastive losses. Then we conclude with a brief case study investigating the effects of different similarity functions on supervised contrastive learning.</p>

<h2 id="similarity-functions">Similarity Functions</h2>

<p>A similarity function \(s(z_1, z_2): \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\) maps a pair of \(d\)-dimensional embedding vectors \(z_1\) and \(z_2\) to a real similarity value, with greater values indicating greater similarity. A temperature hyperparameter \(0 &lt; \tau \leq 1\) is often included, via \(\frac{s(z_1, z_2)}{\tau}\), to scale a similarity function. If the similarity function has a range that is a subset of \(\mathbb{R}\), then \(\tau\) can increase that range. \(\tau\) is omitted for simplicity here.</p>

<h3 id="cosine-similarity">Cosine Similarity</h3>

<p>A common similarity function is cosine similarity:</p>

\[s(z_1, z_2) = \frac{z_1 \cdot z_2}{||z_1|| \cdot ||z_2||}\]

<p>This function measures the cosine of the angle between \(z_1\) and \(z_2\) as a scalar in \([-1, 1]\). Cosine similarity violates the triangle inequality, making it the only similarity function discussed here that is not derived from a distance metric.</p>

<h3 id="negative-arc-length">Negative Arc Length</h3>

<p>The recently proposed negative arc length similarity function <d-cite key="koishekenov_geometric_2023"></d-cite> provides an analogue for cosine similarity that is a distance metric:</p>

\[s(z_1, z_2) = 1 - \frac{\text{arccos}(z_1 \cdot z_2)}{\pi}\]

<p>This function assumes that 
\(||z_1|| = ||z_2|| = 1\)
which is a common normalization <d-cite key="le-khac_contrastive_2020"></d-cite> that restricts the embeddings to a hypersphere. The arc length \(\text{arccos}(z_1 \cdot z_2)\) is a natural choice for comparing such vectors as it is the geodesic distance, or the length of the shortest path between \(z_1\) and \(z_2\) on the hypersphere. Subtracting the arc length converts the distance metric into a similarity function with range \([0, 1]\). Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses.</p>

<h3 id="negative-euclidean-distance">Negative Euclidean Distance</h3>

<p>The negative Euclidean distance similarity function is simply:</p>

\[s(z_1, z_2) = -||z_1 - z_2||_2\]

<p>Euclidean distance measures the shortest path in Euclidean space, making it the geodesic distance when \(z_1\) and \(z_2\) can take any value in \(\mathbb{R}^d\). In this case the similarity function has range \([-\infty, 0]\).</p>

<p>The negative Euclidean distance can also be used with embeddings restricted to a hypersphere, resulting in range \([-2, 0]\). However, this is not the geodesic distance for the hypersphere as the path being measured is inside the sphere. The Euclidean distance will be less than the arc length unless \(z_1 = z_2\), in which case they both equal 0.</p>

<h2 id="contrastive-losses">Contrastive Losses</h2>

<p>A contrastive loss function maps a set of embeddings and a similarity function to a scalar value. Losses are written such that derivatives for backpropagation are taken with respect to the embedding \(z\).</p>

<h3 id="margin-losses">Margin Losses</h3>

<p>The original contrastive loss <d-cite key="chopra_learning_2005"></d-cite> maximizes similarity for examples \(z^+\) and minimizes similarity for examples \(z^-\) until the similarity is below margin hyperparameter \(m\):</p>

\[L(z, z^+) = s(z, z^+); L(z, z^-) = \max( 0, m - s(z, z^-) )\]

<p>The structure of this loss implies that \(z_1\) and \(z_2\) share a class if \(s(z_1, z_2) &lt; m\) and otherwise they do not share a class. This margin hyperparameter can be challenging to tune for efficiency throughout the training process because it needs to be satisfiable but also provide \(z^-\) samples within the margin in order to backpropagate the error.</p>

<p>The triplet loss <d-cite key="schroff_facenet_2015"></d-cite> avoids this by using a margin between similarity values:</p>

\[L(z, z^+, z^-) = \max( 0, s(z, z^+) - s(z, z^-) + m)\]

<p>The triplet loss only updates a network when its loss is positive, so finding triplets satisfying that condition are important for learning efficiency.</p>

<p>Lifted Structured Loss <d-cite key="oh_song_deep_2016"></d-cite> handles this by precomputing similarities for all pairs in a batch then selecting the \(z^-\) with maximal similarity:</p>

\[L(z, z^+) = \max( 0, s(z, z^+) + m - \max [ \max_{z^-} s(z, z^-), \max_{z^-} s(z^+, z^-) ] )\]

<p>The Batch Hard loss <d-cite key="hermans_defense_2017"></d-cite> takes this even further by selecting \(z^+\) with minimal similarity:</p>

\[L(z, z^+) = \max( 0, \min_{z^+} [ s(z, z^+) ] + m - \max_{z^-} [ s(z, z^-) ] )\]

<p>The decision to compute the loss based on comparisons between \(z\), a single \(z^+\), and a single \(z^-\) comes with advantages and disadvantages. These methods can be easier to adapt for learning with varying levels of supervision because complete knowledge of whether similarity should be maximized or minimized for each pair in the dataset is not required. However, these methods also make training efficiently difficult and provide relatively loose constraints on the embedding space.</p>

<h3 id="cross-entropy-losses">Cross Entropy Losses</h3>

<p>A common contrastive loss is the Information Noise Contrastive Estimation (InfoNCE) <d-cite key="oord_representation_2019"></d-cite> loss:</p>

\[L(z, z^+, z^-_1, z^-_2, \ldots, z^-_n) = -\log \frac{ e^{s(z, z^+)} }{ e^{s(z, z^+)} + \sum_{i=1}^n e^{s(z, z^-_i)} }\]

<p>InfoNCE is a cross entropy loss whose logits are similarities for \(z\). \(z^+\) is a single embedding whose similarity with \(z\) should be maximized while \(z^-_1, z^-_2, \ldots, z^-_n\) are a set of \(n\) embeddings whose similarity with \(z\) should be minimized. The structure of this loss implies that \(z_1\) shares a class with \(z_2\) if no other embedding has greater similarity with \(z_1\).</p>

<p>The choice of \(z^+\) and \(z^-\) sets varies across methods. The self-supervised InfoNCE loss chooses \(z^+\) to be an embedding of an augmentation of the input that produced \(z\) and \(z^-\) to be the other inputs and augmentations in the batch. This is called instance discrimination because only augmentations of the same input instance have their similarity maximized.</p>

<p>Supervised methods expand the definition of \(z^+\) to also include embeddings which share a class with \(z\). The expectation of InfoNCE loss over choices of \(z^+\) is used to jointly maximize their similarity to \(z\). The Supervised Contrastive (SupCon) loss <d-cite key="khosla_supervised_2020"></d-cite> uses all embeddings not currently set as \(z\) or \(z^+\) as \(z^-\), including embeddings that share a class with \(z\) and therefore will also be used as \(z^+\). This creates loss terms that would minimize similarity between embeddings that share a class. Supervised Information Noise-Contrastive Estimation REvisited (SINCERE) loss <d-cite key="feeney_sincere_2024"></d-cite> removes embeddings that share a class with \(z\) from \(z^-\), leaving only embeddings with different classes. An additional margin hyperparameter can also be added to these losses <d-cite key="barbano_unbiased_2022"></d-cite>, which allows for interpolation between the original losses and losses with the \(e^{s(z, z^+)}\) term removed from the denominator.</p>

<p>Considering a set of similarities during loss calculation allows the loss to implicitly perform hard negative mining <d-cite key="khosla_supervised_2020"></d-cite>, avoiding the challenge of selecting triplets required by a margin loss. The lack of a margin places strict constraints on the embedding space, as similarities are always being pushed towards the maximum or minimum. This enables analysis of embedding spaces that minimize the loss. For example, InfoNCE and SINCERE losses with cosine similarity are minimized by embedding spaces with clusters of inputs mapped to single points (maximizing similarity) that are uniformly distributed on the unit sphere (minimizing similarity) <d-cite key="wang_understanding_nodate"></d-cite>.</p>

<h2 id="case-study-contrastive-learning-on-a-hypersphere">Case Study: Contrastive Learning on a Hypersphere</h2>

<p>Many modern contrastive learning techniques build off of the combination of cosine similarity and cross entropy losses. However, few papers have explored changing similarity functions and losses outside of the context of a more complex model.</p>

<p>Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses. This change is motivated by the desire to use the geodesic distance on the embedding space, which in this case is a unit hypersphere. We investigate whether replacing cosine similarity with negative arc length similarity can improve performance with the SINCERE loss, which is supervised, and how each similarity affects the learned embedding space.</p>

<h3 id="supervised-learning-accuracy">Supervised Learning Accuracy</h3>

<p>We utilize the methodology of Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite> to evaluate if the results of Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> generalize to supervised cross entropy losses. Specifically, we train models with SINCERE loss and each similarity function then evaluate the models with nearest neighbor classifiers on the test set.</p>

<table>
  <tr>
   <td>
   </td>
   <td colspan="2"><strong>CIFAR-10</strong>
   </td>
   <td colspan="2"><strong>CIFAR-100</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Similarity</strong>
   </td>
   <td><strong>1NN</strong>
   </td>
   <td><strong>5NN</strong>
   </td>
   <td><strong>1NN</strong>
   </td>
   <td><strong>5NN</strong>
   </td>
  </tr>
  <tr>
   <td>Cosine
   </td>
   <td>95.88
   </td>
   <td>95.91
   </td>
   <td>76.23
   </td>
   <td>76.13
   </td>
  </tr>
  <tr>
   <td>Negative Arc Length
   </td>
   <td>95.66
   </td>
   <td>95.65
   </td>
   <td>75.81
   </td>
   <td>76.41
   </td>
  </tr>
</table>

<p>We find no statistically significant difference based on the 95% confidence interval of the accuracy difference <d-cite key="foody_classification_2009"></d-cite> from 1,000 iterations of test set bootstrapping. This aligns with the results in Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite>, which used different loss functions with cosine similarity and found a similar lack of statistically significant results across choices of supervised contrastive cross entropy losses. This suggests that supervised learning accuracy is similar across choices of reasonable similarity functions and contrastive losses.</p>

<h3 id="supervised-learning-embedding-space">Supervised Learning Embedding Space</h3>

<p>We also visualize the learned embedding space for each CIFAR-10 model. For each test set image, the similarity value is plotted for the closest training set image that shares a class (“Target”) and that does not share a class (“Noise”). This visualizes the 1-nearest neighbor decision process. Both similarity functions are plotted for each model, with the title denoting the similarity function used during training.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with cosine similarity maximizes the similarity to target images well. There are a small number of noise images with near maximal similarity, but the majority are below 0.3 cosine similarity. Interestingly, the peaks seen in the noise similarity reflects the fact that individual classes will have different modes of their noise histograms.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with negative arc length similarity does a better job of forcing target similarity values very close to 1 negative arc length similarity, but also has a notable number of target similarities near 0.5 negative arc length similarity. The noise distribution also reflects the fact that individual classes have different modes for their noise histograms, but in this case the modes are spread across more similarity values. Notably the peak for the horse class is very close to the max similarity due to a high similarity to the dog class, although they are still separated enough from the target similarities to not have an impact on accuracy.</p>

<h3 id="discussion">Discussion</h3>

<p>The choice of similarity function clearly has an effect on the learned embedding space despite a lack of statistically significant changes in accuracy. The cosine similarity histogram most cleanly aligns with the intuition that contrastive losses should be maximizing and minimizing similarities. In contrast, the negative arc length similarity histogram suggests similarity minimization is sacrificed for very consistent maximization, producing small differences in similarity between some target classes and noise examples. I hypothesize that this change in behavior arises from the difference in similarity function behavior with small angles described in Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite>.</p>

<p>These differences in the learned embedding spaces could affect performance on downstream tasks such as transfer learning. I hypothesize that the larger difference between target and noise similarities seen in the cosine similarity model would improve transfer learning performance, similar to the improvement of SINCERE over SupCon loss reported in Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite>.</p>]]></content><author><name>Patrick Feeney</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired properties, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo , , SimCLR , , and BYOL , as well as supervised methods such as SupCon and SINCERE .]]></summary></entry><entry><title type="html">Equivariant Neural Fields - continuous representations grounded in geometry</title><link href="http://localhost:4000/blog/2024/equivariant-neural-fields/" rel="alternate" type="text/html" title="Equivariant Neural Fields - continuous representations grounded in geometry" /><published>2024-06-01T00:00:00+02:00</published><updated>2024-06-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/equivariant-neural-fields</id><content type="html" xml:base="http://localhost:4000/blog/2024/equivariant-neural-fields/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Neural fields (NeFs) <d-cite key="xie2022neural"></d-cite> have emerged as a promising paradigm for representing continuous signals in a variety of domains. 
Recently, they have been used as a continuous alternative for classical discrete signal representations - showing promising results especially in higher dimensional settings where traditional grid-based methods often fall short <d-cite key="dupont2022data"></d-cite>.</p>

<p>A major limitation of NeFs as representation is their lack of interpretibility and preservation of geometric information. In this blog post, we delve into the recent advancements presented in the paper “Grounding Continuous Representations in Geometry: Equivariant Neural Fields” <d-cite key="wessels2024ENF"></d-cite>, and explore how Equivariant Neural Fields (ENFs) enhance the capabilities of NeFs through geometric grounding and equivariance properties. We then elaborate upon their use as a representation by discussing the paper “Space-Time Continuous PDE Forecasting using Equivariant Neural Fields” <d-cite key="knigge2024pde"></d-cite>, which demonstrates the use of ENFs in modelling spatiotemporal dynamics. An important upcomming field of research, in which the geometric grounding of NeFs is crucial.</p>

<h3 id="the-evolution-of-neural-fields">The Evolution of Neural Fields</h3>

<p>Neural fields are functions that map spatial coordinates to feature representations. For instance, a neural field \(f_{\theta}: \mathbb{R}^d \rightarrow \mathbb{R}^c\) can map pixel coordinates \(x\) to RGB values to represent images. These fields are typically parameterized by neural networks, which are optimized to approximate a target signal \(f_\theta\) within a reconstruction task. Although this gives rise to continuous representations, for multiple signals, the weights \(\theta_f\) are optimized separately for each signal \(f\), leading to a lack of shared structure across different signals and the need to train different seperate models.</p>

<p>Neural fields are functions that map spatial coordinates to feature representations. Specifically, a neural field \(f_{\theta}: \mathbb{R}^d \rightarrow \mathbb{R}^c\) can map pixel coordinates \(x\) to RGB values, thereby representing images. These fields are typically parameterized by neural networks and optimized to approximate a target signal \(f_\theta\) within a reconstruction task.</p>

<p>While this approach results in continuous representations, it also presents a significant drawback. For multiple signals, the weights \(\theta_f\) must be optimized separately for each signal \(f\) This leads to a lack of shared structure across different signals and necessitates training separate models for each individual signal.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/nf-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/nf-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/nf-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/nf.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Neural Fields. when applied to images, a neural field $f_\theta$ maps coordinates $x \in \mathbb{R}^2$ to pixel values $I(x) \in \mathbb{R}^3$.
</div>

<p>Conditional Neural Fields (CNFs) extend this concept of neural fields by introducing a conditioning variable \(z_f\) that modulates the neural field for a specific signal \(f\). This enhancement allows CNFs to effectively represent an entire dataset of signals \(f \in \mathcal{D}\) using a single, shared set of weights \(\theta\) along with a set of unique conditioning variables \(z_f\). Since these representations are signal-specific, they latents can be used as a representation in downstream tasks. This approach has been successful in various tasks, including classification <d-cite key="dupont2022data"></d-cite>, segmentation <d-cite key="zhang20233dshape2vecset"></d-cite>, and even solving partial differential equations<d-cite key="yin2022continuous"></d-cite> <d-cite key="knigge2024pde"></d-cite>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/cnf-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/cnf-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/cnf-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/cnf.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Conditional Neural Fields. Conditional neural fields extend neural fields by introducing a conditioning variable $z$ that modulates the shared base field $f_\theta$.
</div>

<p>However, conventional CNFs often lack geometric interpretability, they are able to capture textures and appearances which is shown by their performance in reconstruction. However, they do struggle to encode explicit geometric information necessary for tasks requiring spatial reasoning. Think for example of simple geometric transformations like rotations or translations, which are not inherently captured by CNFs; it is unclear how these transformations would manifest in the latent space.</p>

<h3 id="introducing-equivariant-neural-fields">Introducing Equivariant Neural Fields</h3>

<p>Equivariant Neural Fields (ENFs) address this limitation by grounding neural field representations in geometry. ENFs use latent point clouds as conditioning variables, where each point is a tuple consisting of a pose and a context vector. This grounding ensures that transformations in the field correspond to transformations in the latent space, a property known as equivariance.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/ENF_latents.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Equivariant Neural Fields parameterize the conditioning variable $z$ as an attributed point-cloud of poses $p_i$ and corresponding context vectors $\mathbf{c}_i$: $z = \{ (p_i, \mathbf{c}_i \}_{i=0}^N$, explicitly grounding the latent space in geometry. 
</div>

<h3 id="key-properties-of-enfs">Key Properties of ENFs</h3>

<ul>
  <li><strong>Equivariance</strong>: If the field transforms, the latent representation transforms accordingly. This property ensures that the latent space preserves geometric patterns, enabling better geometric reasoning.</li>
  <li><strong>Weight Sharing</strong>: ENFs utilize shared weights over similar local patterns, leading to more efficient learning.</li>
  <li><strong>Localized Representations</strong>: The latent point sets in ENFs enable localized cross-attention mechanisms, enhancing interpretability and allowing unique field editing capabilities.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-properties-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-properties.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Illustration of key properties of Equivariant Neural Fields. ENFs exhibit equivariance by weight-sharing over local patterns through a latent set of poses and context vectors. This enables localized representations and geometric reasoning in the latent space.
</div>

<h3 id="use-of-neural-fields-in-downstream-tasks">Use of Neural Fields in downstream tasks</h3>
<p>As brief interjection, we provide some background on how NeFs are used in downstream tasks. As (a subset of) model NeF parameters are optimized reconstruct specific samples, these parameters may be used as a representation of their corresponding signals. These representations serve as input to downstream models for tasks such as classification, segmentation or even solving partial differential equations.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/downstream-example-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/downstream-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Using NeFs in downstream tasks. For "conventional" NeFs, the weights $\theta_j$ are used as input to a downstream model that can operate on the computational graph of the neural field. For CNFs, the latent vectors $z_j$ are used as representation instead, allowing the use of simple MLPs.
    In ENFs instead the latent point sets $z_j$ are used as input to the downstream model, allowing for preservation of geometric information in the downstream task through the use of equivariant graph models.
</div>

<h2 id="methodology">Methodology</h2>
<p>We now delve into the technical details of ENF, focusing on the key components that enable the model to ground continuous representations in geometry.</p>

<h3 id="requirements-for-equivariant-neural-fields">Requirements for Equivariant Neural Fields</h3>
<p>The equivariance or steerability property of ENFs can be formally defined as:</p>

\[\forall g \in G : f_{\theta}(g^{-1}x, z) = f_{\theta}(x, gz).\]

<p>This property ensures that if the field as a whole transforms, the latent representation will transform in a consistent manner. This is crucial for maintaining geometric coherence in the latent space.
In order reason about the application of a group action on $z$, the authors equip the latent space with a group action by defining $z$ as a set of tuples \((p_i, \mathbf{c}_i)\), where $G$ acts on $z$ by transforming poses \(p_i: gz = \{(gp_i, \mathbf{c}_i)\}_{i=1}^N\).</p>

<p>For a neural field to satisfy the steerability property, the authors show it must be bi-invariant with respect to both coordinates and latents. 
This means that the field $ f_{\theta} $ must remain unchanged under group transformations applied to both the input coordinates and the latent point cloud, i.e.:</p>

\[\forall g \in G: f_\theta(gx, gz) = f_\theta(x, z).\]

<p>This observation is leveraged to define the architecture of ENFs, ensuring that the model is equivariant by design.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-comm-diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Commutative diagram illustrating the steerability of ENFs. 
</div>

<h3 id="equivariance-through-bi-invariant-cross-attention">Equivariance through Bi-invariant Cross-Attention</h3>
<p>ENFs utilize a bi-invariant cross-attention mechanism to parametrize the neural fields in order to achieve the aforementioned steerability property.
The cross-attention operation is defined as:
$ f_{\theta}(x, z) = \sum_{i=1}^{N} \text{att}(x, z) v(a(x, p_i), c_i) $
where $ a(x, p_i) $ is an invariant pair-wise attribute that captures the geometric relationship between the coordinate $ x $ and the latent pose $ p_i $, 
ensuring that the cross-attention operation respects the aforementioned bi-invariance condition.</p>

<p>Note that the choice of bi-invariant is related to the choice of group \(G\) and the specific application domain. For example,
in natural images \(G\) may be the group of 2D Euclidean transformations, while in 3D shape representations \(G\) may be the group of 3D rigid transformations,
leading to different choices of bi-invariant \(a(x, p_i)\). For a better understanding of bi-invariant properties we refer to <d-cite key="bekkers2023fast"></d-cite> which shows optimal invariant attributes in terms of expressivity for different groups.</p>

<h3 id="enforcing-locality-in-equivariant-neural-fields">Enforcing Locality in Equivariant Neural Fields</h3>
<p>To enforce locality, ENFs incorporate a Gaussian window into the attention mechanism. 
This ensures that each coordinate receives attention primarily from nearby latents, akin to the localized kernels in convolutional networks. 
This locality improves the interpretability of the latent representations, as specific features can be related to specific latent points $(p_i, \mathbf{c}_i)$.
Moreover, locality also improves parameter-efficiency by allowing for weight sharing over similar patterns.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-summ-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-summ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fitting different signals with ENFs. A field $f_j$ denoting a specific signal is represented by a set of localized latent points $z_j=\{ (p_i, \mathbf{c}_i)\}_{i=1}^N$. In the case of images (left), the latent points are distributed over the image plane. In the case of shapes (right), the latent points are distributed over 3D space.
</div>

<h2 id="experimental-validation">Experimental Validation</h2>
<p>The authors validate the properties of ENFs through various experiments on image and shape datasets, providing 
metrics for reconstruction and downstream classification. Moreover, authors play around with the ENFs latent space
to demonstrate the benefits of having a geometrically grounded latent space. A
separate study by Knigge et al. demonstrates the use of ENFs in modelling spatiotemporal dynamics.</p>

<h3 id="image-and-shape-reconstruction-and-classification">Image and Shape Reconstruction and Classification</h3>
<p>ENFs were evaluated on several image datasets, including CIFAR-10, CelebA, and STL-10. 
The results show that ENFs achieve higher peak signal-to-noise ratio (PSNR) in image reconstruction tasks compared to CNFs. 
This improvement is attributed to the geometric grounding and weight-sharing properties of ENFs.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Symmetry</strong></th>
      <th><strong>Cifar10</strong></th>
      <th><strong>CelebA</strong></th>
      <th><strong>STL-10</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa       <d-cite key="dupont2022data"></d-cite></td>
      <td>x</td>
      <td>31.9</td>
      <td>28.0</td>
      <td>20.7</td>
    </tr>
    <tr>
      <td><strong>ENF - abs pos</strong></td>
      <td>x</td>
      <td>31.5</td>
      <td>16.8</td>
      <td>22.8</td>
    </tr>
    <tr>
      <td><strong>ENF - rel pos</strong></td>
      <td>$\mathbb{R}^2$</td>
      <td><strong>34.8</strong></td>
      <td><strong>34.6</strong></td>
      <td><strong>26.8</strong></td>
    </tr>
    <tr>
      <td><strong>ENF - abs rel pos</strong></td>
      <td>SE(2)</td>
      <td>32.8</td>
      <td>32.4</td>
      <td>23.9</td>
    </tr>
    <tr>
      <td><strong>ENF - ponita</strong></td>
      <td>$\rm SE(2)$</td>
      <td>33.9</td>
      <td>32.9</td>
      <td>25.4</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Reconstruction accuracy for ENFs compared to CNFs on CIFAR-10, CelebA and STL10 for different choices of bi-invariant $a$.
</div>

<p>For classification, the authors used the latent point sets extracted from the trained ENF models. 
The classification accuracy on CIFAR-10 shows a significant improvement over conventional CNFs, 
highlighting the superior representation capabilities of ENFs.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Symmetry</strong></th>
      <th><strong>Cifar10</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa <d-cite key="dupont2022data"></d-cite></td>
      <td>x</td>
      <td>68.3</td>
    </tr>
    <tr>
      <td><strong>ENF - abs pos</strong></td>
      <td>x</td>
      <td>68.7</td>
    </tr>
    <tr>
      <td><strong>ENF - rel pos</strong></td>
      <td>$\mathbb{R}^2$</td>
      <td><strong>82.1</strong></td>
    </tr>
    <tr>
      <td><strong>ENF - abs rel pos</strong></td>
      <td>SE(2)</td>
      <td>70.9</td>
    </tr>
    <tr>
      <td><strong>ENF - ponita</strong></td>
      <td>SE(2)</td>
      <td>81.5</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Classification accuracy for ENFs compared to CNFs on CIFAR-10 for different choices of bi-invariant $a$.
</div>

<p>The authors also tested ENFs on shape datasets using Signed Distance Functions (SDFs). 
The results indicate that ENFs can effectively represent geometric shapes with high fidelity, 
further validating the geometric interpretability of the latent representations.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Reconstruction IoU (voxel)</strong></th>
      <th><strong>Reconstruction IoU (SDF)</strong></th>
      <th><strong>Classification</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Functa <d-cite key="dupont2022data"></d-cite></td>
      <td>99.44</td>
      <td>-</td>
      <td>93.6</td>
    </tr>
    <tr>
      <td><strong>ENF</strong></td>
      <td>-</td>
      <td>55</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

<div class="caption">
    Shape reconstruction and classification metrics for ENFs compared to CNFs on ShapeNet.
</div>

<h3 id="latent-space-editing">Latent Space Editing</h3>
<p>The authors demonstrate the benefits of the geometrically grounded latent space in ENFs by performing latent space editing.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/enf-carduck.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Latent space editing with ENFs. By construction, ENF representations can be stitched together to create new fields. Here, the authors demonstrate the ability to create a "car-duck" by combining the latent representations of reconstructions of a car and a duck.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/interpolation-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/interpolation-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/interpolation-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/interpolation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Localized latent space interpolation. The authors demonstrate the ability to interpolate between two neural fields by interpolating between their latent point sets. This allows for localized editing of the fields.
</div>

<h3 id="spatiotemporal-dynamics-modelling">Spatiotemporal Dynamics Modelling</h3>

<p>Another usecase for ENFs is highlighted in the paper “Space-Time Continuous PDE Forecasting using Equivariant Neural Fields” <d-cite key="knigge2024pde"></d-cite>.
Authors use the ENF as a continuous state representation for solving partial differential equations; learning to forecast dynamics by modelling them with a Neural ODE as a
flow in the latent space of the ENF. Since PDEs are often defined over continuous domains in terms of local differential operators, ENFs are well-suited to model these dynamics, as
they provide localized continuous representations.
This approach allows for symmetry-preserving continuous forecasting of spatiotemporal dynamics,
showing promising results on a variety of PDEs defined over different geometries. An initial state \(\nu_0:\mathcal{X}\rightarrow \mathbb{R}\) representing the current state of the PDE
is fit with a corresponding latent \(z_0\), which is unrolled in latent space with an equivariant graph-based neural ODE \(F_\psi\).</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/fig-enf-pde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    In Equivariant Neural Fields $f_\theta$, a field $\nu_t$ is represented by a set of latents 
$z^\nu_t = \{(p_{i}^\nu,\mathbf{c}_{i}^\nu)\}_{i=1}^N$ consisting of a pose $p_{i}$ and context vector $\mathbf{c}_{i}$. 
Using meta-learning, the initial latent $z^\nu_0$ is fit in only 3 SGD steps, after which an equivariant neural ODE $F_\psi$ models the solution as a latent flow.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/shallow-water-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/shallow-water.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
Due to its continuous nature, the ENF forecasting model is able to natively handle zero-shot super-resolution, as demonstrated on the shallow water equations. 
Top: low resolution test sample at train resolution. Middle: high resolution test sample at test resolution. Bottom: ENF forecast at test resolution. 
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-480.webp 480w,/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-800.webp 800w,/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-15-equivariant-neural-fields/internally-heated-convection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
Because of its geometric grounding, the model is able to handle complicated geometries, as demonstrated on internally heated convection equations in the ball.
</div>

<iframe src="/assets/plotly/2024-06-15-equivariant-neural-fields/navier-stokes.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
<div class="caption">
    The authors demonstrate the use of ENFs in modelling spatiotemporal dynamics by solving the Navier-Stokes 
equations over a 2D domain with periodic boundary conditions. The ENF respects the corresponding translational symmetries. Left: ground truth dynamics. Middle: ENF forecast. Right: Absolute forecast error. A test-sample is visualized, i.e. the model is unrolled from the initial state $\nu_0$. During training the model is supervised for 10 timesteps.
</div>

<h2 id="conclusion">Conclusion</h2>

<p>Equivariant Neural Fields leverage geometric grounding and equivariance properties to provide continuous signal representations preserving geometric information.
This approach opens up new possibilities for tasks that require geometric reasoning and localized representations, such as image and shape analysis, and shows promising results in 
forecasting spatiotemporal dynamics.</p>

<p>This blog post has explored the foundational concepts and the significant advancements brought forward by Equivariant Neural Fields. By grounding neural fields in geometry and incorporating equivariance properties, ENFs pave the way for more robust and interpretable continuous signal representations. As research in this area progresses, we can expect further innovations that leverage the geometric and localized nature of these fields, unlocking new potentials across diverse applications.</p>]]></content><author><name>David R. Wessels*</name></author><category term="equivariance" /><category term="geometry" /><category term="neural-fields" /><summary type="html"><![CDATA[An intro to geometry-grounded continuous signal representations and their use in modelling spatio-temporal dynamics.]]></summary></entry></feed>