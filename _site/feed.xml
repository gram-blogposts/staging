<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-04T17:11:32+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>GRaM Workshop
</subtitle><entry><title type="html">Accelerating Equivariant Graph Neural Networks with JAX</title><link href="http://localhost:4000/blog/2024/egnn-jax/" rel="alternate" type="text/html" title="Accelerating Equivariant Graph Neural Networks with JAX" /><published>2024-06-30T00:00:00+03:00</published><updated>2024-06-30T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/egnn-jax</id><content type="html" xml:base="http://localhost:4000/blog/2024/egnn-jax/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This blogpost serves as a tutorial for the fast and scalable training of Equivariant Neural Networks, which are slower to train due to the handling of more complex data. We propose leveraging JAX’s capabilities to address these challenges. In this work, we analyze the benefits of utilizing JAX and provide a detailed breakdown of the steps needed to achieve a fully JIT-compatible framework. This approach not only enhances the performance of Neural Networks but also opens the door for future research in developing fully equivariant transformers using JAX.</p>

<p>This blogpost serves three purposes:</p>
<ol>
  <li>Explain the ideas of equivariance in networks while also explaining some of the methods used.</li>
  <li>Give an overview of the performance tests conducted on the two approaches.</li>
  <li>Provide an overview of reproduction results for the Equivariant Graph Neural Network.</li>
</ol>

<h3 id="recap-of-equivariance">Recap of Equivariance</h3>

<p>As equivariance is prevalent in the natural sciences <d-cite key="balaban1985applications"></d-cite><d-cite key="gupta2013wtf"></d-cite><d-cite key="miller1995wordnet"></d-cite><d-cite key="thölke2022equivariant"></d-cite><d-cite key="maron2020learning"></d-cite>, it makes sense to utilize them for our neural networks, especially given the evidence suggesting that it significantly improves performance through increasing the network’s generalizability <d-cite key="bronstein2021geometric"></d-cite>. One large area within this subfield of deep learning is learning 3D translation and rotation symmetries, where various techniques have been created such as Graph Convolutional Neural Networks <d-cite key="cohen2016group"></d-cite> and Tensor Field Networks <d-cite key="thomas2018tensor"></d-cite>.</p>

<p>Following these works, more efficient implementations have emerged, with the first being the Equivariant Graph Neural Network (EGNN) <d-cite key="satorras2021en"></d-cite>. Based on the GNN <d-cite key="gori2005new"></d-cite><d-cite key="kipf2018neural"></d-cite><d-cite key="bruna2014spectral"></d-cite>, which follows a message passing scheme, it innovates by inputting the relative squared distance between two coordinates into the edge operation and to make the output equivariant, updates the coordinates of the nodes per layer. This specific method bypasses any expensive computations/approximations relative to other, similar methods while retaining high performance levels, making it preferable compared to most other GNN architectures.</p>

<p>More recently, transformer architectures have been utilized within the field of equivariant models. While not typically used for these types of problems due to how they were originally developed for sequential tasks <d-cite key="devlin2019bert"></d-cite><d-cite key="baevski2020wav2vec"></d-cite>, recent work has suggested their effectiveness for tackling such issues <d-cite key="thölke2022equivariant"></d-cite><d-cite key="fuchs2020se"></d-cite><d-cite key="liao2023equiformer"></d-cite>. This is possible through the incorporation of domain-related inductive biases, allowing them to model geometric constraints and operations. In addition, one property of transformers is that they assume full adjacency by default, which is something that can be adjusted to better match the local connectivity of GNN approaches. These additions further increase the complexity of the framework, strongly highlighting the need for a more efficient alternative.</p>

<h3 id="equivariant-graph-neural-networks">Equivariant Graph Neural Networks</h3>

<p>Given a set of \(T_g\) transformations on a set \(X\) (\(T_g: X \rightarrow X\)) for an element \(g \in G\), where \(G\) is a group acting on \(X\), a function \(\varphi: X \rightarrow Y\) is equivariant to \(g\) iff an equivalent transformation \(S_g: Y \rightarrow Y\) exists on its output space \(Y\), such that:</p>

<div style="text-align: center;">
$$
\varphi(T_g(x)) = S_g(\varphi(x)). \qquad \qquad \text{(Equation 1)}
$$
</div>

<p>In other words, translating the input set \(T_g(x)\) and then applying \(\varphi(T_x(x))\) on it yields the same result as first running the function \(y = \varphi(x)\) and then applying an equivalent translation to the output \(S_g(y)\) such that Equation 1 is fulfilled and \(\varphi(x+g) = \varphi(x) + g\) <d-cite key="satorras2021en"></d-cite>.</p>

<h3 id="equivariant-graph-neural-networks-1">Equivariant Graph Neural Networks</h3>

<p>For a given graph \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\) with nodes \(v_i \in \mathcal{V}\) and edges
\(=e_{ij} \in \mathcal{E}\), we can define a graph convolutional layer as the following:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, a_{ij}), \qquad \qquad \text{(Equation 2)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 3)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i), \qquad \qquad \text{(Equation 4)}
$$
</div>

<p>where \(\mathbf{h}\_i^l \in \mathbb{R}^{nf}\) is the nf-dimensional embedding of node \(v_i\) at layer \(l\), \(a_{ij}\) are the edge attributes, \(\mathcal{N}\_i\) is the set of neighbors of node \(v_i\), and \(\varphi_e\) and \(\varphi_h\) are the
edge and node operations respectively, typically approximated by Multilayer Perceptrons (MLPs).</p>

<p>To make this implementation equivariant, <d-cite key="satorras2021en"></d-cite> introduced the inputting of the relative squared distances between two points and updating of the node positions at each time step, leading to the following formulae:</p>

<div style="text-align: center;">
$$
\mathbf{m}\_{ij} = \varphi_e (\mathbf{h}\_i^l, \mathbf{h}\_j^l, ||\mathbf{x}\_i^l - \mathbf{x}\_j^l||^2, a_{ij}), \qquad \qquad \text{(Equation 5)}
$$
$$
x_i^{l+1} = x_i^l + C \sum_{j \neq i} (\mathbf{x}\_i^l - \mathbf{x}\_j^l) \varphi_x(\mathbf{m}\_{ij}), \qquad \qquad \text{(Equation 6)}
$$
$$
\mathbf{m}\_{i} = \sum_{j \in \mathcal{N}\_i } \mathbf{m}\_j, \qquad \qquad \text{(Equation 7)}
$$
$$
\mathbf{h}\_i^{l+1} = \varphi_h (\mathbf{h}\_i^l, \mathbf{m}\_i). \qquad \qquad \text{(Equation 8)}
$$
</div>

<p>This idea of using the distances during computation forms one of the bases of our proposed transformer architecture, as it is a simple yet effective way to impose geometric equivariance within a system.</p>

<h3 id="why-jax">Why JAX?</h3>

<p>JAX is a high-performance numerical computing library that provides several advantages over traditional frameworks. By default, JAX automatically compiles library calls using just-in-time (JIT) compilation, ensuring optimal execution. It utilizes XLA-optimized kernels, allowing for sophisticated algorithm expression without leaving Python. Furthermore, JAX also excels in utilizing multiple GPU or TPU cores and automatically evaluating gradients through differentiation transformations, making it ideal for high-compute scenarios.</p>

<p>This is partially caused by how JAX often uses pointers to reference elements in memory instead of copying them, which has several advantages:</p>

<ul>
  <li><strong>Efficiency:</strong> Through pointers, JAX avoids the unnecessary copying of data, resulting in faster computations and lower memory usage.</li>
  <li><strong>Functionally Pure:</strong> Since JAX functions are pure (i.e., contain no side effects), using pointers ensures that the data is not accidentally modified, maintaining the integrity of all operations.</li>
  <li><strong>Automatic Differentiation:</strong> JAX’s efficient gradient computation relies on its functional programming model. Pointers allow JAX to track operations and dependencies without data duplication.</li>
</ul>

<hr />

<h2 id="experiments">Experiments</h2>

<h3 id="n-body-dataset">N-Body dataset</h3>

<p>In this dataset, a dynamical system consisting of 5 atoms is modeled in 3D space. Each atom has a positive and negative charge, a starting position and a starting velocity. The task is to predict the position of the particles after 1000 time steps. The movement of the particles follow the rules of physics: Same charges repel and different charges attract. The task is equivariant in the sense, that translating and rotating the 5-body system on the input space is the same as rotating the output space.</p>

<h3 id="qm9-dataset">QM9 dataset</h3>

<p>This dataset consists of small molecules and the task is to predict a chemical property. The atoms of the molecules have 3 dimensional positions and each atom is one hot encoded to the atom type. This task is an invariant task, since the chemical property does not depend on position or rotation of the molecule. In addition, larger batch sizes were also experimented with due to smaller sizes causing bottlenecks during training.</p>

<h3 id="data-preparation">Data Preparation</h3>

<p>Here, we introduce a straightforward method for preprocessing data from a PyTorch-compatible format to one suitable for JAX. Our approach handles node features, edge attributes, indices, positions, and target properties. The key step would be converting the data to jax numpy (jnp) arrays, ensuring compatibility with JAX operations. For usage examples, refer to <code class="language-plaintext highlighter-rouge">qm9\utils.py</code> or <code class="language-plaintext highlighter-rouge">n_body\utils.py</code>.</p>

<h3 id="training">Training</h3>

<p>We now address the key differences and steps in adapting the training loop, model saving, and evalution functions for JAX (refer to <code class="language-plaintext highlighter-rouge">main_qm9.py</code> and <code class="language-plaintext highlighter-rouge">nbody_egnn_trainer.py</code>).</p>

<p>JAX uses a functional approach to define and update the model parameters. We use <code class="language-plaintext highlighter-rouge">jax.jit</code> via the <code class="language-plaintext highlighter-rouge">partial</code> decorator for JIT compilation, which ensures that our code runs efficiently by compiling the functions once and then executing them multiple times. We also utilize <code class="language-plaintext highlighter-rouge">static_argnames</code> as decorators for the loss and update functions, which specify the arguments to treat as static. By doing this, JAX can assume these arguments will not change and optimize the function accordingly.</p>

<p>Moreover, model initialization in JAX requires knowing the input sizes beforehand. We extract features to get their shapes and initialize the model using <code class="language-plaintext highlighter-rouge">model.init(jax_seed, *init_feat, max_num_nodes)</code>. This seed initializes the random number generators, which then produces the random number sequences used in virtually all processes. Also, this seed is created using the <code class="language-plaintext highlighter-rouge">jax.random.PRNGKey</code> function, which is used for all random operations. This ensures that they are all reproducible and can be split into multiple independent keys if needed.</p>

<p>The loss function is called through <code class="language-plaintext highlighter-rouge">jax.grad(loss_fn)(params, x, edge_attr, edge_index, pos, node_mask, edge_mask, max_num_nodes, target)</code>. <code class="language-plaintext highlighter-rouge">jax.grad</code> is a powerful tool in JAX for automatic differentiation, allowing us to compute gradients of scalar-valued functions with respect to their inputs.</p>

<hr />

<h2 id="evaluation">Evaluation</h2>

<h3 id="speed-comparison">Speed Comparison</h3>

<p>The EGNN authors <d-cite key="satorras2021en"></d-cite> note that while their approach is more computationally efficient, it is still slower than Linear and Graph Neural Networks. Thus, the aim is to preserve the properties of the model while also providing a fast alternative. We demonstrate the effectivity of building a JAX-based alternative by comparing the forward pass times of the original EGNN implementation with our version. The results of which can be seen in the following graph:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch32-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch64-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch64.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-480.webp 480w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-800.webp 800w,/assets/img/2024-06-30-egnn-jax/jaxvspytorch128-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-egnn-jax/jaxvspytorch128.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-12 text-center">
        <b>Figure 1.</b> EGNN speed comparison between JAX EGNN (ours) and the PyTorch EGNN <d-cite key="satorras2021en"></d-cite>. Benchmark results represent a single forward pass averaged over 100 tries. The batch sizes used here are 32, 64 and 128.
    </div>
</div>

<p>One notable observation is the consistency in performance. The JAX implementation exhibits less variance in duration values, resulting in more stable and predictable performances across runs. This is particularly important for large-scale applications where the performance consistency can impact overall system reliability and efficiency.</p>

<p>Additionally, as the number of nodes increases, the JAX implementation maintains a less steep increase in computation time compared to PyTorch. This indicates better scalability, making the JAX-based EGNN more suitable for handling larger and more complex graphs.</p>

<h3 id="reproduction-results">Reproduction Results</h3>

<p>To show that our implementation generally preserves the performance and characteristics of the base model, we perform a reproduction of the results reported in <d-cite key="satorras2021en"></d-cite> and display the results for several properties in both experiments. They can be found in the table below.</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th style="text-align: center">EGNN</th>
      <th style="text-align: center">EGNN (Ours)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>QM9 (ε<sub>HOMO</sub>) (meV)</td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">75</td>
    </tr>
    <tr>
      <td>N-Body (Position MSE)</td>
      <td style="text-align: center">0.0071</td>
      <td style="text-align: center">0.0025</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1.</strong> Reproduction results comparing <d-cite key="satorras2021en"></d-cite> with our JAX implementation.</p>

<p>Here, our EGNN implementation outperforms the original author’s implementation on the N-Body dataset. Moreover, other publicly available EGNN implementations also achieve a similar performance as our model on our data. We therefore argue that the increased performance stems from how the dataset is generated slightly differently compared to the one presented in <d-cite key="satorras2021en"></d-cite>.</p>

<hr />

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>Our EGNN comparisons reveal that the JAX-based model is faster than traditional PyTorch implementations, benefiting from JIT compilation to optimize runtime performance. In addition, we also demonstrate that these JAX-based models also achieve comparable performances to the aforementioned PyTorch ones, meaning that they are generally more suitable for equivariance tasks.</p>

<p>We also adapted the model for two well-known datasets: the QM9 dataset for molecule property prediction and the N-body dataset for simulating physical systems. This demonstrates the flexibility and potential of our JAX framework as a strong foundation for further development. Our work suggests that the JAX-based EGNN framework can be effectively extended to other applications, facilitating future research and advancements in equivariant neural networks and beyond.</p>

<hr />]]></content><author><name>Anonymous</name></author><category term="equivariance" /><summary type="html"><![CDATA[A Tutorial on How to Make EGNNs Faster]]></summary></entry><entry><title type="html">Equivariant Diffusion for Molecule Generation in 3D using Consistency Models</title><link href="http://localhost:4000/blog/2024/equivariant_diffusion/" rel="alternate" type="text/html" title="Equivariant Diffusion for Molecule Generation in 3D using Consistency Models" /><published>2024-06-30T00:00:00+03:00</published><updated>2024-06-30T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/equivariant_diffusion</id><content type="html" xml:base="http://localhost:4000/blog/2024/equivariant_diffusion/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this blog post, we discuss the paper <a href="https://arxiv.org/abs/2203.17003">“Equivariant Diffusion for Molecule Generation in 3D”</a> <d-cite key="hoogeboom2022equivariant"></d-cite>, 
which first introduced 3D molecule generation using diffusion models. Their Equivariant Diffusion Model (EDM) also
incorporated an Equivariant Graph Neural Network (EGNN) architecture, effectively grounding the model with inductive
priors about the symmetries in 3D space. EDM demonstrated strong improvement over other (non-diffusion) generative 
methods for molecules at the time and inspired many further influential works in the field <d-cite key="anstine2023generative"></d-cite><d-cite key="corso2023diffdock"></d-cite><d-cite key="igashov2024equivariant"></d-cite><d-cite key="xu2023geometric"></d-cite>.</p>

<p>Most diffusion models are unfortunately bottle-necked by the sequential denoising process, which can be slow and 
computationally expensive <d-cite key="song2023consistency"></d-cite>. Hence, we also introduce <a href="https://arxiv.org/abs/2303.01469">“Consistency Models”</a> <d-cite key="song2023consistency"></d-cite>
and demonstrate that an EDM can generate samples up to <em>24x faster</em> in this paradigm with as little as a single step.
However, we unfortunately found the quality of samples generated by the consistency model to be much worse 
than from the original EDM.</p>

<!---
Using Consistency Models can be a step towards enabling much larger GNN backbones, eventually observing 
similar scaling effects as other domains including language <d-cite key="brown2020language"></d-cite><d-cite key="kaplan2020scaling"></d-cite><d-cite key="touvron2023llama"></d-cite> 
or image and video generation <d-cite key="liu2024sora"></d-cite><d-cite key="ramesh2022hierarchical"></d-cite><d-cite key="rombach2022high"></d-cite><d-cite key="saharia2022photorealistic"></d-cite>.
Such improvement has been demonstrated in training Graph Neural Networks (GNN) <d-cite key="sriram2022towards"></d-cite>,
and scaling model parameters to take advantage of increasingly larger compute availability, is generally known to improve 
model performance <d-cite key="dosovitskiy2020image"></d-cite><d-cite key="kaplan2020scaling"></d-cite><d-cite key="krizhevsky2012imagenet"></d-cite>.
--->

<!--- 260 words --->

<p><br /></p>

<h4 id="briefly-on-equivariance-for-molecules">Briefly on Equivariance for molecules</h4>

<p>Equivariance is a property of certain functions, which ensures that their output transforms in a predictable manner under 
collections of transformations. This property is valuable in molecular modeling, where it can be used to ensure that the 
properties of molecular structures are consistent with their symmetries in the real world. Specifically, we are interested 
in ensuring that structure is preserved in the representation of a molecule under three types of transformations: 
<em>translation, rotation, and reflection</em>.</p>

<p>Formally, we say that a function \(f\) is equivariant to the action of a group \(G\) if:</p>

\[\begin{align}
T_g(f(x)) = f(S_g(x))
\end{align}\]

<p>for all \(g \in G\), where \(S_g,T_g\) are linear representations related to the group element \(g\) <d-cite key="serre1977linear"></d-cite>.</p>

<p>The three transformations: <em>translation, rotation, and reflection</em>, form the Euclidean group \(E(3)\), which is the group of all aforementioned isometries in three-dimensional space, for which \(S_g\) and 
\(T_g\) can be represented by a translation \(t\) and an orthogonal matrix $R$ that rotates or reflects coordinates.</p>

<p>A function \(f\) is then equivariant to a rotation or reflection \(R\) if transforming its input results in an equivalent transformation of its output <d-cite key="hoogeboom2022equivariant"></d-cite>:</p>

\[\begin{align}
Rf(x) = f(Rx)
\end{align}\]

<p><br /></p>

<!--- 330 words --->

<h4 id="introducing-equivariant-graph-neural-networks-egnns">Introducing Equivariant Graph Neural Networks (EGNNs)</h4>
<p>Molecules can very naturally be represented with graph structures, where the nodes are atoms and edges their bonds. 
The features of each atom, such as its element type or charge can be encoded into an embedding \(\mathbf{h}_i \in \mathbb{R}^d\) 
alongside with the atoms 3D position \(\mathbf{x}_i \in \mathbb{R}^3\).</p>

<p>To learn and operate on such structured inputs, Graph Neural Networks (GNNs) <d-cite key="zhou2021graphneuralnetworksreview"></d-cite> 
have been developed, falling under the message passing paradigm <d-cite key="gilmer2017neuralmessagepassingquantum"></d-cite>. 
This architecture consists of several layers, each of which updates the representation of each node, using the information 
in nearby nodes.</p>

<style>
.custom-img-size {
    width: 50%; /* Adjust width as needed */
    height: auto; /* Maintains aspect ratio */
    display: block;
    margin-left: auto;
    margin-right: auto;
}

.custom-img-size-2 {

    width: 60%; /* Adjust width as needed */
    height: auto; /* Maintains aspect ratio */
    display: block;
    margin-left: auto;
    margin-right: auto;
}
</style>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/message_passing-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/message_passing-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/message_passing-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/message_passing.png" class="img-fluid rounded z-depth-1 custom-img-size" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 1: visualization of a message passing network</figcaption>
        </figure>
    </div>
</div>

<p>The previously mentioned \(E(3)\) equivariance property of molecules can be injected as an inductive prior into to the model 
architecture of a message passing graph neural network, resulting in an \(E(3)\) EGNN. This property improves generalisation <d-cite key="hoogeboom2022equivariant"></d-cite> and also beats similar non-equivariant Graph Convolution Networks on 
the molecular generation task <d-cite key="verma2022modular"></d-cite>.</p>

<p>The EGNN is built with <em>equivariant</em> graph convolution layers (EGCLs):</p>

\[\begin{align}
\mathbf{x}^{l+1},\mathbf{h}^{l+1}=EGCL[ \mathbf{x}^l, \mathbf{h}^l ]
\end{align}\]

<p>An EGCL layer can be formally defined by:</p>

<div align="center">

$$
\begin{align}
\mathbf{m}_{ij} = \phi_e(\mathbf{h}_i^l, \mathbf{h}_j^l, d^2_{ij})
\end{align}
$$

$$
\begin{align}
\mathbf{h}_i^{l+1} = \phi_h\left(\mathbf{h}_i^l, \sum_{j \neq i} \tilde{e}_{ij} \mathbf{m}_{ij}\right) 
\end{align}
$$

$$
\begin{align}
\mathbf{x}_i^{l+1} = \mathbf{x}_i^l + \sum_{j \neq i} \frac{\mathbf{x}_i^l \mathbf{x}_j^l}{d_{ij} + 1} \phi_x(\mathbf{h}_i^l, \mathbf{h}_j^l, d^2_{ij})
\end{align}
$$

</div>

<p>where \(h_l\) represents the feature $h$ at layer \(l\), \(x_l\) represents the coordinate at layer \(l\) and 
\(d_{ij}= ||x_i^l-x^l_j||_2\) is the Euclidean distance between nodes \(v_i\) and \(v_j\).</p>

<p>A fully connected neural network is used to learn the functions \(\phi_e\), \(\phi_x\), and \(\phi_h\). 
At each layer, a message \(m_{ij}\) is computed from the previous layer’s feature representation. 
Using the previous feature and the sum of these messages, the model computes the next layer’s feature representation.</p>

<p>This architecture then satisfies translation and rotation equivariance. Notably, the messages depend on the distance 
between the nodes and these distances are not changed by isometric transformations.</p>

<!--- 600 words --->

<h2 id="equivariant-diffusion-models-edm">Equivariant Diffusion Models (EDM)</h2>
<p>This section introduces diffusion models and describes how their predictions can be made \(E(3)\) equivariant. 
The categorical properties of atoms are already invariant to \(E(3)\) transformations, hence, we are only 
interested in enforcing this property on the sampled atom positions.</p>

<h3 id="what-are-diffusion-models">What are Diffusion Models?</h3>

<p>Diffusion models <d-cite key="sohl2015deep"></d-cite><d-cite key="ho2020denoising"></d-cite> are inspired by the principles 
of diffusion in physics, and model the flow of a data distribution to pure noise over time. A neural network is then 
trained to learn a reverse process that reconstructs samples on the data distribution from pure noise.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/ddpm_figure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 2: The Markov process of forward and reverse diffusion <d-cite key="ho2020denoising"></d-cite></figcaption>
        </figure>
    </div>
</div>

<p>The “forward” noising process can be parameterized by a Markov process <d-cite key="ho2020denoising"></d-cite>, 
where transition at each time step \(t\) adds Gaussian noise with a variance of \(\beta_t \in (0,1)\):</p>

\[\begin{align}
q\left( x_t \mid x_{t-1} \right) := \mathcal{N}\left( x_t ; \sqrt{1-\beta_t} x_{t-1}, \beta_t \mathbf{I} \right) 
\end{align}\]

<p>The whole Markov process leading to time step \(T\) is given as a chain of these transitions:</p>

\[\begin{align}
q\left( x_1, \ldots, x_T \mid x_0 \right) := \prod_{t=1}^T q \left( x_t \mid x_{t-1} \right)
\end{align}\]

<p>The “reverse” process transitions are unknown and need to be approximated using a neural network parametrized by \(\theta\):</p>

\[\begin{align}
p_\theta \left( x_{t-1} \mid x_t \right) := \mathcal{N} \left( x_{t-1} ; \mu_\theta \left( x_t, t \right), \Sigma_\theta \left( x_t, t \right) \right)
\end{align}\]

<p>Because we know the dynamics of the forward process, the variance \(\Sigma_\theta \left( x_t, t \right)\) at time \(t\) is 
known and can be fixed to \(\beta_t \mathbf{I}\).</p>

<p>The predictions then only need to obtain the mean \(\mu_\theta \left( x_t, t \right)\), given by:</p>

\[\begin{align}
\mu_\theta \left( x_t, t \right) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta\_t}{\sqrt{1 - \bar{\alpha}\_t}} \epsilon\_\theta \left( x_t, t \right) \right)
\end{align}\]

<p>where \(\alpha_t = \Pi_{s=1}^t \left( 1 - \beta_s \right)\).</p>

<p>Hence, we can directly predict \(x_{t-1}\) from \(x_{t}\) using the network \(\theta\):</p>

\[\begin{align}
x_{t-1} = \frac{1}{\sqrt{1 - \beta_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta \left( x_t, t \right) \right) + \sqrt{\beta_t} v_t
\end{align}\]

<p>where \(v_T \sim \mathcal{N}(0, \mathbf{I})\) is a sample from the pure Gaussian noise.</p>

<!--- 850 words --->

<h3 id="enforcing-e3-equivariant-diffusion">Enforcing E(3) equivariant diffusion</h3>
<!--- check rotations and reflections or jsut rotations? --->
<p>Equivariance to rotations and reflections effectively means that if any orthogonal rotation matrix \(\mathbf{R}\) is 
applied to a sample \(\mathbf{x}_t\) at any given time step \(t\), we should still generate a correspondingly rotated 
“next best sample” \(\mathbf{R}\mathbf{x}_{t+1}\) at time \(t+1\).</p>

<p>In other words, the likelihood of this next best sample does not depend on the molecule’s rotation and the probability 
distribution for each transition in the Markov Chain is hence roto-invariant:</p>

\[\begin{align}
p(y|x) = p(\mathbf{R}y|\mathbf{R}x)
\end{align}\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure class="custom-figure">
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_gaus.png" class="img-fluid rounded z-depth-1 custom-image" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        </figure>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure class="custom-figure">
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/roto_symetry_donut.png" class="img-fluid rounded z-depth-1 custom-image" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

        </figure>
    </div>
</div>
<div class="row">
    <div class="col text-center mt-3">
        <p>Figure 3: Examples of 2D roto-invariant distributions</p>
    </div>
</div>

<style>
    .custom-figure .custom-image {
        height: 250px; /* Set a fixed height for both images */
        width: auto; /* Maintain aspect ratio and adjust width accordingly */
        max-width: 100%; /* Ensure the image doesn't exceed the container width */
    }
</style>

<p>Such an invariant distribution composed with an equivariant invertible function results in another invariant distribution <d-cite key="kohler2020equivariant"></d-cite>. 
Furthermore, if \(x \sim p(x)\) is invariant to a group, and the transition probabilities of a Markov chain \(y \sim p(y|x)\) 
are equivariant, then the marginal distribution of \(y\) at any time step \(t\) is also invariant to that group <d-cite key="xu2022geodiff"></d-cite>.</p>

<p>Since the underlying EGNN already ensures  this equivariance, the remaining constraint can easily be achieved by 
setting the initial sampling distribution to something roto-invariant, such as a simple mean zero Gaussian with a 
diagonal covariance matrix, as illustrated in Figure 3 (left).</p>

<p><em>Translation equivariance</em> requires a few tricks. It has been shown, that it is impossible to have non-zero distributions 
invariant to translations <d-cite key="satorras2021en"></d-cite>. Intuitively, the translation invariance property 
means that any point \(\mathbf{x}\) results in the same assigned \(p(\mathbf{x})\), leading to a uniform distribution, 
which, if stretched over an unbounded space, would be approaching zero-valued probabilities thus not integrating 
to one.</p>

<p>The EDM authors bypass this with a clever trick of always re-centering the generated samples to have center of gravity at
\(\mathbf{0}\) and further show that these \(\mathbf{0}\)-centered distributions lie on a linear subspace that can reliably be used 
for equivariant diffusion <d-cite key="hoogeboom2022equivariant"></d-cite><d-cite key="xu2022geodiff"></d-cite>.</p>

<!---
We hypothesize that, intuitively, moving a coordinate from e.g. 5 to 6 on any given axis is the same as moving from 
8 to 9. But EDM predicts the actual atom positions, not a relative change, hence the objective needs to adjusted. 
By constraining the model to this "subspace" of options where the center of the molecule is always at $\mathbf{0}$, 
the absolute positions are effectively turned into relative ones w.r.t. to the center of the molecule, hence the model 
can now learn relationships that do not depend on the absolute position of the whole molecule in 3D space.
--->

<!--- (below) 1100 words --->

<h3 id="how-to-train-the-edm">How to train the EDM?</h3>

<p>The training objective of diffusion-based generative models amounts to <strong>“maximizing the log-likelihood of the 
sample on the original data distribution.”</strong></p>

<p>During training, a diffusion model learns to approximate the parameters of a posterior distributions at the next time
step by minimizing the KL divergence between this estimate and the ground truth, which is equivalent
objective to minimizing the negative log likelihood.</p>

\[\begin{align}
L_{vlb} := L_{t-1} := D_{KL}(q(x_{t-1}|x_{t}, x_{0}) \parallel p_{\theta}(x_{t-1}|x_{t}))
\end{align}\]

<p>The EDM adds a caveat that the predicted distributions must be calibrated to have center of gravity at \(\mathbf{0}\), 
in order to ensure equivariance.</p>

<p>Using the KL divergence loss term with the EDM model parametrization simplifies the loss function to:</p>

\[\begin{align}
\mathcal{L}_t = \mathbb{E}_{\epsilon_t \sim \mathcal{N}_{x_h}(0, \mathbf{I})} \left[ \frac{1}{2} w(t) \| \epsilon_t - \hat{\epsilon}_t \|^2 \right]
\end{align}\]

<p>where 
\(w(t) = \left(1 - \frac{\text{SNR}(t-1)}{\text{SNR}(t)}\right)\) and \(\hat{\epsilon}_t = \phi(z_t, t)\).</p>

<p>The EDM authors found that the model performs best with a constant \(w(t) = 1\), thus effectively simplifying 
the loss function to an MSE. Since coordinates and categorical features are on different scales, it was also 
found that scaling the inputs before inference and then rescaling them back also improves performance.</p>

<!--- 1250 words --->

<h2 id="consistency-models">Consistency Models</h2>

<p>As previously mentioned, diffusion models are bottlenecked by the sequential denoising process <d-cite key="song2023consistency"></d-cite>.
Consistency Models reduce the number of steps during de-noising up to just a single step, significantly speeding up 
this costly process, while allowing for a controlled trade-off between speed and sample quality.</p>

<h3 id="modelling-the-noising-process-as-an-sde">Modelling the noising process as an SDE</h3>

<p>Song et al. <d-cite key="song2021score"></d-cite> have shown that the noising process in diffusion can be described with a Stochastic Differential Equation (SDE)
transforming the data distribution \(p_{\text{data}}(\mathbf{x})\) in time:</p>

\[\begin{align}
d\mathbf{x}_t = \mathbf{\mu}(\mathbf{x}_t, t) dt + \sigma(t) d\mathbf{w}_t
\end{align}\]

<p>Where \(t\) is the time-step, \(\mathbf{\mu}\) is the drift coefficient, \(\sigma\) is the diffusion coefficient,
and \(\mathbf{w}_t\) is the stochastic component denoting standard Brownian motion. This stochastic component effectively
represents the iterative adding of noise to the data in the forward diffusion process and dictates the shape of the final
distribution at time \(T\).</p>

<p>Typically, this SDE is designed such that \(p_T(\mathbf{x})\) at the final time-step \(T\) is close to a tractable Gaussian.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/bimodal_to_gaussian_plot.png" class="img-fluid rounded z-depth-1 custom-img-size-2" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 4: Illustration of a bimodal distribution evolving to a Gaussian over time</figcaption>
        </figure>
    </div>
</div>

<!--- 1400 words --->

<h3 id="existence-of-the-pf-ode">Existence of the PF ODE</h3>

<p>This SDE has a remarkable property, that a special ODE exists, whose trajectories sampled at \(t\) are distributed
according to \(p_t(\mathbf{x})\) <d-cite key="song2023consistency"></d-cite>:</p>

\[\begin{align}
d\mathbf{x}_t = \left[ \mathbf{\mu}(\mathbf{x}_t, t) - \frac{1}{2} \sigma(t)^2 \nabla \log p_t(\mathbf{x}_t) \right] dt
\end{align}\]

<p>This ODE is dubbed the Probability Flow (PF) ODE by Song et al. <d-cite key="song2023consistency"></d-cite> and corresponds to the different view of diffusion
manipulating probability mass over time we hinted at in the beginning of the section.</p>

<p>A score model \(s_\phi(\mathbf{x}, t)\) can be trained to approximate \(\nabla log p_t(\mathbf{x})\) via score matching <d-cite key="song2023consistency"></d-cite>.
Since we know the parametrization of the final distribution \(p_T(\mathbf{x})\) to be a standard Gaussian parametrized 
with \(\mathbf{\mu}=0\) and \(\sigma(t) = \sqrt{2t}\), this score model can be plugged into the equation (16) and the 
expression reduces itself to an empirical estimate of the PF ODE:</p>

\[\begin{align}
\frac{dx_t}{dt} = -ts\phi(\mathbf{x}_t, t)
\end{align}\]

<p>With \(\mathbf{\hat{x}}_T\) sampled from the specified Gaussian at time \(T\), the PF ODE can be solved backwards in time 
to obtain a solution trajectory mapping all points along the way to the initial data distribution at time $$\epsilon$.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/consistency_models_pf_ode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 5: Solution trajectories of the PF ODE. <d-cite key="dosovitskiy2020image"></d-cite></figcaption>
        </figure>
    </div>
</div>

<p>Given any off-the-shelf ODE solver (e.g. Euler) and a trained score model \(s_\phi(\mathbf{x}, t)\), we can solve this PF ODE.
The time horizon \([\epsilon, T]\) with \(\epsilon\) very close to zero is discretized into sub-intervals for improved performance <d-cite key="karras2022elucidating"></d-cite>. A solution trajectory, denoted \(\\{\mathbf{x}_t\\}\), 
is then given as a finite set of samples \(\mathbf{x}_t\) for every discretized time-step \(t\) between \(\epsilon\) and \(T\).</p>

<!--- 1600 words --->

<h3 id="consistency-function">Consistency Function</h3>

<p>Given a solution trajectory \({\mathbf{x}_t}\), we define the <em>consistency function</em> as:</p>

\[\begin{align}
f: (\mathbf{x}_t, t) \to \mathbf{x}_{\epsilon}
\end{align}\]

<p>In other words, a consistency function always outputs a corresponding datapoint at time $\epsilon$, i.e. very close to
the original data distribution for every pair (\(\mathbf{x}_t\), \(t\)).</p>

<p>Importantly, this function has the property of <em>self-consistency</em>: i.e. its outputs are consistent for arbitrary pairs of
\((x_t, t)\) that lie on the same PF ODE trajectory. Hence, we have \(f(x_t, t) = f(x_{t'}, t')\) for all \(t, t' \in [\epsilon, T]\).</p>

<p>The goal of a <em>consistency model</em>, denoted by \(f_\theta\), is to estimate this consistency function \(f\) from data by
being enforced with this self-consistency property during training.</p>

<!--- 1700 words --->

<!---
### Boundary Condition & Function Parametrization

For any consistency function $f(\cdot, \cdot)$, we must have $f(x_\epsilon, \epsilon) = x_\epsilon$, i.e., $f(\cdot, 
\epsilon)$ being an identity function. This constraint is called the _boundary condition_ <d-cite key="song2023consistency"></d-cite>.

The boundary condition has to be met by all consistency models, as we have hinted before that much of the training relies
on the assumption that $p_\epsilon$ is borderline identical to $p_0$. However, it is also a big architectural
constraint on consistency models.

For consistency models based on deep neural networks, there are two ways to implement this boundary condition almost
for free <d-cite key="song2023consistency"></d-cite>. Suppose we have a free-form deep neural network $F_\theta (x, t)$ whose output has the same dimensionality
as $x$.

1.) One way is to simply parameterize the consistency model as:

$$
f_\theta (x, t) =
\begin{cases}
x & t = \epsilon \\
F_\theta (x, t) & t \in (\epsilon, T]
\end{cases} \\
\qquad \text{(27)}
$$

2.) Another method is to parameterize the consistency model using skip connections, that is:

$$
f_\theta (x, t) = c_{\text{skip}} (t) x + c_{\text{out}} (t) F_\theta (x, t) \qquad \text{(28)}
$$

where $c_{\text{skip}} (t)$ and $c_{\text{out}} (t)$ are differentiable functions such that $c_{\text{skip}} (\epsilon) = 1$,
and $c_{\text{out}} (\epsilon) = 0$.

This way, the consistency model is differentiable at $t = \epsilon$ if $F_\theta (x, t)$, $c_{\text{skip}} (t)$, $c_{\text{out}} (t)$
are all differentiable, which is critical for training continuous-time consistency models.

In our work, we utilize the latter methodology in order to satisfy the boundary condition.
--->

<h3 id="sampling">Sampling</h3>

<p>With a fully trained consistency model \(f_\theta(\cdot, \cdot)\), we can generate new samples by simply sampling from the initial
Gaussian \(\hat{x_T}\) \(\sim \mathcal{N}(0, T^2I)\) and propagating this through the consistency model to obtain
samples on the data distribution \(\hat{x_{\epsilon}}\) \(= f_\theta(\hat{x_T}, T)\) with as little as one diffusion step.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>
            

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-480.webp 480w,/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-800.webp 800w,/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-equivariant_diffusion/consistency_on_molecules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

            <figcaption class="text-center mt-2">Figure 6: Visualization of PF ODE trajectories for molecule generation in 3D. <d-cite key="fan2023ecconf"></d-cite></figcaption>
        </figure>
    </div>
</div>

<!--- 1750 words --->

<h3 id="training-consistency-models">Training Consistency Models</h3>

<p>Consistency models can either be trained by “distillation” from a pre-trained diffusion model, or in “isolation” as a standalone generative model from scratch. In the context of our work, we focused only on the latter because the distillation approach has a hard requirement of using a pretrained score based diffusion. 
In order to train in isolation we need to leverage the following unbiased estimator:</p>

\[\begin{align}
\nabla \log p_t(x_t) = - \mathbb{E} \left[ \frac{x_t - x}{t^2} \middle| x_t \right]
\end{align}\]

<p>where \(x \sim p_\text{data}\) and \(x_t \sim \mathcal{N}(x; t^2 I)\).</p>

<p>That is, given \(x\) and \(x_t\), we can estimate \(\nabla \log p_t(x_t)\) with \(-(x_t - x) / t^2\).
This unbiased estimate suffices to replace the pre-trained diffusion model in consistency distillation
when using the Euler ODE solver in the limit of \(N \to \infty\) <d-cite key="song2023consistency"></d-cite>.</p>

<p>Song et al. <d-cite key="song2023consistency"></d-cite> justify this with a further theorem in their paper and show that the consistency training objective (CT loss)
can then be defined as:</p>

\[\begin{align}
\mathcal{L}_{CT}^N (\theta, \theta^-) &amp;= \mathbb{E}[\lambda(t_n)d(f_\theta(x + t_{n+1} \mathbf{z}, t_{n+1}), f_{\theta^-}(x + t_n \mathbf{z}, t_n))]
\end{align}\]

<p>where \(\mathbf{z} \sim \mathcal{N}(0, I)\).</p>

<p>Crucially, \(\mathcal{L}(\theta, \theta^-)\) only depends on the online network \(f_\theta\), and the target network
\(f_{\theta^-}\), while being completely agnostic to diffusion model parameters \(\phi\).</p>

<!--- ~1900 words --->

<h2 id="experiments">Experiments</h2>

<p>We replicate the original EDM set-up and evaluate on the QM9 dataset <d-cite key="ramakrishnan2014quantum"></d-cite>. 
Due to computational constraints and the demonstrational nature of this blogpost, we only trained models for 
130 epochs with the default hyperparameter settings given by the original EDM implementation <d-cite key="hoogeboom2022equivariant"></d-cite>
to illustrate the trade-offs in speed and quality of samples.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model / Sampling Time (seconds)</strong></th>
      <th><strong>Mean</strong></th>
      <th><strong>STD</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Default EDM</td>
      <td>0.6160</td>
      <td>0.11500</td>
    </tr>
    <tr>
      <td>Consistency Model (single step)</td>
      <td>0.0252</td>
      <td>0.00488</td>
    </tr>
  </tbody>
</table>

<div class="caption" style="text-align: center;">
    Table 1: EDM and Consistency Model inference speed
</div>

<p>As expected, we observed in table 1., that the consistency model in single-step mode is significantly faster than the EDM, 
providing up to a <em>24x speed-up</em> averaged over 5 sampling runs. This number represents the time it takes the model to generate
a sample on the data distribution from a pure Gaussian noise input, excluding other computational overheads shared by 
both models equally, such as logging.</p>

<table>
  <thead>
    <tr>
      <th><strong>Model / Metric</strong></th>
      <th><strong>Training NLL</strong></th>
      <th><strong>Validation NLL</strong></th>
      <th><strong>Best Cross-Validated Test NLL</strong></th>
      <th><strong>Best Atom Stability</strong></th>
      <th><strong>Best Molecule Stability</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Default EDM</td>
      <td>2.524</td>
      <td>-30.066</td>
      <td>-17.178</td>
      <td>0.873</td>
      <td>0.196</td>
    </tr>
    <tr>
      <td>Consistency Model (single step)</td>
      <td>2.482</td>
      <td>94176</td>
      <td>80363</td>
      <td>0.19</td>
      <td>0</td>
    </tr>
    <tr>
      <td>Consistency Model (multi-step)</td>
      <td>2.484</td>
      <td>166264</td>
      <td>179003</td>
      <td>0.12</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<div class="caption" style="text-align: center;">
    Table 2: EDM and Consistency Model results on the QM9 dataset after 130 epochs.
</div>

<p>We observed that the consistency models converge on the training set with similar rate as the regular EDM, even
achieving slightly lower training NLLs. However, they completely fail to generalize to the validation and test sets with
much lower atom stability and no molecule stability. These results are surprisingly poor, given that
the dataset is not particularly complicated, and consistency models have already shown promising results on 
images <d-cite key="song2023consistency"></d-cite> and reportedly, shows competitive results on QM9 as well <d-cite key="fan2023ecconf"></d-cite>.</p>

<p>To improve these results, we attempted to use multi-step sampling, which  should in theory allow us to replicate results 
close to the EDM with the same number of sampling steps. However, we observed no such improvement in our experiments. 
We tested multiple different amounts of steps and report results for 100, which performed best overall. 
Oddly, the multi-step sampling actually yields worse results than the single-step sampling most of the time,
which is highly unexpected and requires further investigation.</p>

<p>It should also be noted that the default EDM with more training is capable of achieving results much better than what we 
report in table 2. However, it still comfortably outperforms all consistency model variations on all metrics using equal
amounts of compute.</p>

<h2 id="discussion">Discussion</h2>

<p>Consistency models are able to reduce the number of steps during sampling up to just a single step, significantly 
speeding up the sampling process. We were able to successfully demonstrate this and train an EDM as a consistency 
model in isolation, achieving nearly identical training loss with up to 24x faster sampling times. However, using 
the single-step sampling only achieves up to 19% atom stability in best case scenario, compared with the default 
EDM which consistently reaches 87% or much more with further training. We suspect that a model trained in this 
set-up might be too prone to overfitting and struggles with generalization to anything outside the training data
distribution, compared to sequential de-noising predictions of the EDM, which arem uch more robust.</p>

<p>Using multi-step sampling should in theory yield competitive results, but we observed no such improvement. 
Since it cannot be conclusively ruled out that this was caused by a bug in our multi-step sampling code, we hope 
to continue investigating if the consistency model paradigm can reliably be used for molecule generation in the future
and show more competitive results as previous works suggest is possible <d-cite key="fan2023ecconf"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><category term="equivariance," /><category term="diffusion," /><category term="molecule" /><category term="generation," /><category term="consistency" /><category term="models" /><summary type="html"><![CDATA[Introduction to the seminal papers &quot;Equivariant Diffusion for Molecule Generation in 3D&quot; and &quot;Consistency Models&quot; with an adaptation fusing the two together for fast molecule generation.]]></summary></entry><entry><title type="html">Do Transformers Really Perform Bad for Graph Representation?</title><link href="http://localhost:4000/blog/2024/graphormer/" rel="alternate" type="text/html" title="Do Transformers Really Perform Bad for Graph Representation?" /><published>2024-06-30T00:00:00+03:00</published><updated>2024-06-30T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/graphormer</id><content type="html" xml:base="http://localhost:4000/blog/2024/graphormer/"><![CDATA[<!-- abcd -->

<h2 id="introduction">Introduction</h2>
<p>The Transformer architecture has revolutionized sequence modelling. Its versatility is demonstrated by its application in various domains, from natural language processing to computer vision to even reinforcement learning. With its strong ability to learn rich representations across domains, it seems natural that the power of the transformer can be adapted to graphs.</p>

<p>The main challenge with applying a transformer to graph data is that there is no obvious sequence-based representation of graphs. Graphs are commonly represented by adjacency matrices or lists, which lack inherent order and are thus unsuitable for transformers.</p>

<p>The primary reason for finding a sequence-based representation of a graph is to combine the advantages of a transformer (such as its high scalability) with the ability of graphs to capture non-sequential and multidimensional relationships. Graph Neural Networks (GNNs) employ various constraints during training, such as enforcing valency limits when generating molecules. However, choosing such constraints may not be as straightforward for other problems. With Graphormer, we can apply these very constraints in a simpler manner, analogous to applying a causal mask in a transformer. This can also aid in discovering newer ways to apply constraints in GNNs by presenting existing concepts in an intuitive manner.</p>

<p>Graphormer introduces Centrality Encoding to capture the node importance, Spatial Encoding to capture the structural relations, and Edge Encoding to capture the edge features. In addition to this, Graphormer makes other architectures easier to implement by making various existing architecture special cases of Graphormer, with the performance to boot.</p>

<hr />

<h2 id="preliminaries">Preliminaries</h2>

<ul>
  <li><strong>Graph Neural Networks (GNNs)</strong>: Consider a graph \(G = \{V, E\}\) where \(V = \{v_1, v_2, \cdots, v_n\}\) and \(n = |V|\) is the number of nodes. Each node \(v_i\) has a feature vector \(x_i\). Modern GNNs update node representations iteratively by aggregating information from neighbours. The representation of node \(v_i\) at layer \(l\) is \(h^{(l)}_i\), with \(h_i^{(0)} = x_i\). The aggregation and combination at layer \(l\) are defined as: 
\(a_{i}^{(l)}=\text{AGGREGATE}^{(l)}\left(\left\{h_{j}^{(l-1)}: j \in \mathcal{N}(v_i)\right\}\right)\) 
\(h_{i}^{(l)}=\text{COMBINE}^{(l)}\left(h_{i}^{(l-1)}, a_{i}^{(l)}\right)\) 
where \(\mathcal{N}(v_i)\) is the set of first or higher-order neighbours of \(v_i\). Common aggregation functions include MEAN, MAX, and SUM. The COMBINE function fuses neighbor information into the node representation. For graph-level tasks, a READOUT function aggregates node features \(h_i^{(L)}\) from the final iteration into a graph representation \(h_G\):
\(h_{G}=\operatorname{READOUT}\left(\left\{h_{i}^{(L)} \mid v_i \in G \right\}\right)\)
READOUT can be a simple summation or a more complex pooling function.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/gnn-480.webp 480w,/assets/img/2024-06-30-graphormer/gnn-800.webp 800w,/assets/img/2024-06-30-graphormer/gnn-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/gnn.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A message-passing neural network. Note how the node states flow from outer to inner layers, with pooling at each step to update states.<d-cite key="GoogleResearch"></d-cite>
</div>

<ul>
  <li><strong>Transformer</strong>: The Transformer architecture comprises layers with two main components: a self-attention module and a position-wise feed-forward network (FFN). Let \(H = [h_1^\top, \cdots, h_n^\top]^\top\in ℝ^{n\times d}\) be the input to the self-attention module, where \(d\) is the hidden dimension and \(h_i\in ℝ^{1\times d}\) is the hidden representation at position \(i\). The input \(H\) is projected using matrices \(W_Q\inℝ^{d\times d_K}, W_K\inℝ^{d\times d_K}\), and \(W_V\inℝ^{d\times d_V}\) to obtain representations \(Q, K, V\). Self-attention is computed as:
\(Q = HW_Q,\ K = HW_K,\ V = HW_V,\ A = \frac{QK^\top}{\sqrt{d_K}},\ Attn(H) = \text{softmax}(A)V\)
where \(A\) captures the similarity between queries and keys. This self-attention mechanism allows the model to understand relevant information in the sequence comprehensively.</li>
</ul>

<!-- For simplicity of illustration, we consider the single-head self-attention and assume $$d_K = d_V = d$$. The extension to the multi-head attention is standard and straightforward, and we omit bias terms for simplicity. xxxx One of the main properties of the Transformer that makes it so effective in processing sequences is its ability to model long-range dependencies and contextual information with its receptive field. Specifically, each token in the input sequence can interact with (or pay “attention” to) every other token in the sequence when transforming its representation xxxx. -->

<!-- ![ [Source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)](Spatial%20Encoding%20d515dd50b6354ab19b8310fab3005464/Untitled.png) -->
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/head-view-480.webp 480w,/assets/img/2024-06-30-graphormer/head-view-800.webp 800w,/assets/img/2024-06-30-graphormer/head-view-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/head-view.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    An illustration of the attention mechanism. Notice how each word(or token) can attend to different parts of the sequence, forward or backward.<d-cite key="Vig2024"></d-cite>.
</div>

<!-- An illustration of attention mechanism at play for a translation task. Notice how each word(or token) can attend to different parts of the sequence, forward or backward. [Source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) -->

<hr />
<h2 id="graphormer">Graphormer</h2>
<h3 id="centrality-encoding">Centrality Encoding</h3>

<p>In a sequence modelling task, Attention captures the semantic correlations between the nodes (tokens).
The goal of this encoding is to capture the most important nodes in the graph.
Let’s take an example.
Say we want to compare airports and find which one is the largest.
We need a common metric to compare them, so we take the sum of the total daily incoming and outgoing flights, giving us the busiest airports. This is what the algorithm is doing logically to identify the ‘busiest’ nodes.
Additionally, the learnable vectors allow the Graphormer to ‘map’ out the nodes. All this culminates in better performance for graph-based tasks such as molecule generation.</p>

<p>This is the Centrality Encoding equation, given as:</p>

\[h_{i}^{(0)} = x_{i} + z^{-}_{deg^{-}(v_{i})} + z^{+}_{deg^{+}(v_{i})}\]

<p>Let’s analyse this term by term:</p>

<ul>
  <li>\(h_{i}^{(0)}\) - Representation (\(h\)) of vertice i (\(v_{i}\)) at the 0th layer (first input)</li>
  <li>\(x_{i}\) - Feature vector of vertice i (\(v_{i}\))</li>
  <li>\(z^{-}_{deg^{-}(v_{i})}\) - Learnable embedding vector (\(z\)) of the indegree (\(deg^{-}\)) of vertice i (\(v_{i}\))</li>
  <li>\(z^{+}_{deg^{+}(v_{i})}\) - Learnable embedding vector (\(z\)) of the outdegree (\(deg^{+}\)) of vertice i (\(v_{i}\))</li>
</ul>

<p>This is an excerpt of the code used to compute the Centrality Encoding</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">in_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_in_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="n">self</span><span class="p">.</span><span class="n">out_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_out_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Intial node feature computation.
</span><span class="n">node_feature</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_feature</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">in_degree_encoder</span><span class="p">(</span><span class="n">in_degree</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_degree_encoder</span><span class="p">(</span><span class="n">out_degree</span><span class="p">))</span>
</code></pre></div></div>

<!-- num_in_degree is the indegree and hidden_dim is the size of the embedding vector - the Embedding function call converts this number (indegree) to a learnable vector of size hidden_dim, which is then added to the node_feature. A similar procedure is done with num_out_degree, resulting in the implementation of Equation 5. -->

<!-- <put simple explanation first then equations and code> - talk about graph based example -->

<hr />

<h3 id="spatial-encoding">Spatial Encoding</h3>

<p>There are several methods for encoding the position information of the tokens in a sequence.
In a graph, however, there is a problem. Graphs consist of nodes (analogous to tokens) connected with edges in a non-linear, multi-dimensional space. There’s no inherent notion of an “ordering” or a “sequence” in its structure, but as with positional information, it’ll be helpful if we inject some sort of structural information when we process the graph.</p>

<!-- A naive solution would be to learn the encodings themselves. Another would be to perform some operation on the graph structure, such as a random walk, or components from the feature matrix. The intuition is to perform an operation on the graph to extract some “structural” information.  -->

<p>The authors propose a novel encoding called <em>Spatial Encoding</em>. Take a pair of nodes (analogous to tokens) as input and output a scalar value as a function of the shortest path distance (SPD) between the nodes. This scalar value is then added to the element corresponding to the operation between the two nodes in the Query-Key product matrix.</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i, v_j)}\]

<p>The above equation shows the modified computation of the Query-Key Product matrix. Notice that the additional term \(b_{\phi(v_i, v_j)}\)  is a learnable scalar value and acts like a bias term. Since this structural information is independent of which layer of our model is using it, we share this value across all layers.</p>

<p>The benefits of using such an encoding are:</p>
<ol>
  <li>Our receptive field has effectively increased, as we are no longer limited to the information from our neighbours, as is what happens in conventional message-passing networks.</li>
  <li>The model determines the best way to adaptively attend to the structural information. For example, if the scalar valued function is a decreasing function for a given node, we know that the nodes closer to our node are more important than the ones farther away.</li>
</ol>

<hr />

<h3 id="edge-encoding">Edge Encoding</h3>

<p>Graphormer’s edge encoding method significantly enhances the way the model incorporates structural features from graph edges into its attention mechanism. The prior approaches either add edge features to node features or use them during aggregation, propagating the edge information only to associated nodes. Graphormer’s approach ensures that edges play a vital role in the overall node correlation.</p>

<p>Initially, node features \((h_i, h_j)\) and edge features \((x_{e_n})\) from the shortest path between nodes are processed. For each pair of nodes \((v_i, v_j)\), the edge features on the shortest path \(SP_{ij}\) are averaged after being weighted by learnable embeddings \((w^E_n)\), this results in the edge encoding \(c_{ij}\):</p>

\[c_{ij} = \frac{1}{N} \sum_{n=1}^{N} x_{e_n} (w^E_n)^T\]

<p>This is then incorporated as the edge features into the attention score between nodes via a bias-like term. After incorporating the edge and spatial encodings, the value of \(A_{ij}\) is now:</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i,v_j)} + c_{ij}\]

<p>This ensures that edge features directly contribute to the attention score between any two nodes, allowing for a more nuanced and comprehensive utilization of edge information. The impact is significant, and it greatly improves the performance, as proven empirically in the Experiments section.</p>

<hr />
<h3 id="vnode">VNode</h3>

<p>The \([VNode]\) (or a Virtual Node) is arguably one of the most important contributions from the work. It is an artificial node that is connected to <b>all</b> other nodes. The authors cite this paper<d-cite key="gilmer2017neuralmessagepassingquantum"></d-cite> as an empirical motivation, but a better intuition behind the concept is as a generalization of the [CLS] token widely used in NLP and Vision. 
<!-- The sharp reader will notice that this has an important implication on $b$ and $\phi$, because the $$[VNode]$$ is connected to every node, --></p>

\[\phi([VNode], v) = 1, \forall v \in G\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> <!--Dummy divs to take up space, need to do this because height, width tags don't work with the given image class-->
    </div>
    <div class="col-sm-6 mt-3 mt-md-0"> <!-- Note  this is a trick to make the image small keep it center but also not too small (using -6)-->
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/vnode3-480.webp 480w,/assets/img/2024-06-30-graphormer/vnode3-800.webp 800w,/assets/img/2024-06-30-graphormer/vnode3-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/vnode3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<p>Since this is not a <b>physical connection</b>, \(b_{\phi([VNode], v)}\) is set to be a <b>distinct</b> learnable vector (for all \(v\)) to provide the model with this important geometric information.</p>

<p>[CLS] tokens are often employed as “summary” tokens for text and provide a global context to the model. With graphs and text being different modalities, the \([VNode]\) also helps in <b>relaying</b> global information to distant or non-connected clusters in a graph. This is significantly important to the model’s expressivity, as this information might otherwise never propagate. In fact, the \([VNode]\) becomes a learnable and dataset-specific READOUT function.</p>

<!-- As we pointed out, \[CLS\] tokens are used for varied downstream tasks, in a similar way, $$[VNode]$$ can be (and is) used as the final representation of the Graph, i.e., this becomes a learnable and dataset-specfic READOUT function! -->

<p>This can be implemented as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Initialize the VNode
</span>    <span class="n">self</span><span class="p">.</span><span class="n">v_node</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="c1"># one per head (different from CLS)
</span>    <span class="bp">...</span>
    <span class="c1"># During forward pass (suppose VNode is the first node)
</span>    <span class="bp">...</span>
    <span class="n">headed_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">v_node</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">headed_emb</span>
        <span class="c1">#(n_graph, n_heads, n_nodes + 1, n_nodes + 1)
</span>    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">headed_emb</span>
    <span class="bp">...</span>
</code></pre></div></div>

<p>Again, we emphasise that the information-relay point of view is much more important to the model than the summary-token view. The design choice of one \([VNode]\) per head reflects that.</p>

<hr />

<h2 id="theoretical-aspects-of-expressivity">Theoretical aspects of expressivity</h2>

<p>These are the three main facts from the paper,</p>

<ol>
  <li>With appropriate weights and \(\phi\), GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GraphSAGE<d-cite key="hamilton2018inductiverepresentationlearninglarge"></d-cite>, and GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite> are all <b>special cases</b> of a Graphormer.</li>
  <li>Graphormer is better than architectures that are limited by the 1-WL test. (so <b>all</b> traditional GNNs!)</li>
  <li>With appropriate weights, <b>every node</b> representation in the output can be MEAN-READOUT.</li>
</ol>

<p>The <a href="#spatial-encoding">spatial encoding</a> provides the model with important geometric information. Observe that with an appropriate \(b_{\phi(v_i, v_j)}\), the model can <b>find (learn)</b> neighbours for any \(v_i\) and thus easily implement <b>mean-statistics (GCN!)</b>. By knowing the degree (some form of <a href="#centrality-encoding">centrality encoding</a>), mean-statistics can be transformed into sum-statistics; it (indirectly) follows that various statistics can be learned by different heads, which leads to varied representations and allows GraphSAGE, GIN or GCN to be modeled as a Graphormer.</p>

<p>Fact 2 follows from Fact 1, with GIN being the most powerful traditional GNN, which can theoretically identify all graphs distinguishable by the 1-WL test, as it is now a special case of Graphormer. The latter can do the same (&amp; more!).</p>

<p>More importantly, Fact 3 implies that Graphormer allows the flow of <i>Global</i> (and Local) information within the network. This truly sets the network apart from traditional GNNs, which can only aggregate local information up to a fixed radius (or depth).</p>

<p>Traditional GNNs are <i>designed</i> to prevent this type of flow, as with their architecture, this would lead to over-smoothening. However, the clever design around \([VNode]\) prevents this from happening in Graphormer. The addition of a supernode along with Attention and the learnable \(b_{\phi(v_i, v_j)}\) facilitate this, the \([VNode]\) can relay global information, and the attention mechanism can selectively choose from there.</p>

<hr />
<h2 id="experiments">Experiments</h2>

<p>The researchers conducted comprehensive experiments to evaluate Graphormer’s performance against state-of-the-art models like GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite>, DeeperGCN<d-cite key="li2020deepergcnneedtraindeeper"></d-cite>, and the Transformer-based GT<d-cite key="dwivedi2021generalizationtransformernetworksgraphs"></d-cite>.</p>

<p>Two variants of Graphormer, <em>Graphormer</em> (L=12, d=768) and a smaller <em>GraphormerSMALL</em> (L=6, d=512), were evaluated on the <a href="https://ogb.stanford.edu/docs/lsc/">OGB-LSC</a> quantum chemistry regression challenge (PCQM4M-LSC), one of the largest graph-level prediction dataset with over 3.8 million graphs.</p>

<p>The results, as shown in Table 1, demonstrate Graphormer’s significant performance improvements over previous top-performing models such as GIN-VN, DeeperGCN-VN, and GT.</p>

<p>Table 1: Results on PCQM4M-LSC</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Train MAE</th>
      <th>Validate MAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GIN-VN</td>
      <td>6.7M</td>
      <td>0.1150</td>
      <td>0.1395</td>
    </tr>
    <tr>
      <td>DeeperGCN-VN</td>
      <td>25.5M</td>
      <td>0.1059</td>
      <td>0.1398</td>
    </tr>
    <tr>
      <td>GT</td>
      <td>0.6M</td>
      <td>0.0944</td>
      <td>0.1400</td>
    </tr>
    <tr>
      <td>GT-Wide</td>
      <td>83.2M</td>
      <td>0.0955</td>
      <td>0.1408</td>
    </tr>
    <tr>
      <td>GraphormerSMALL</td>
      <td>12.5M</td>
      <td>0.0778</td>
      <td>0.1264</td>
    </tr>
    <tr>
      <td>Graphormer</td>
      <td>47.1M</td>
      <td>0.0582</td>
      <td>0.1234</td>
    </tr>
  </tbody>
</table>

<p>Notably, Graphormer did not encounter over-smoothing issues, with both training and validation errors continuing to decrease as model depth and width increased, thereby going beyond the <a href="https://web.stanford.edu/class/cs224w/slides/06-theory.pdf#page=46">1-WL test</a>. Additionally, Graph Transformer (GT) showed no performance gain despite a significant increase in parameters from GT to GT-Wide, highlighting Graphormer’s scaling capabilities.</p>

<p>Further experiments for graph-level prediction tasks were performed on datasets from popular leaderboards like <a href="https://ogb.stanford.edu/docs/graphprop/#ogbg-mol">OGBG</a> (MolPCBA, MolHIV) and <a href="https://paperswithcode.com/paper/benchmarking-graph-neural-networks">benchmarking-GNNs</a> (ZINC), which also showed Graphormer consistently outperforming top-performing GNNs.</p>

<p>By using the ensemble with ExpC<d-cite key="yang2020breakingexpressivebottlenecksgraph"></d-cite>, Graphormer was able to reach a 0.1200 MAE and win the graph-level track in the OGB Large-Scale Challenge.</p>

<h3 id="comparison-against-state-of-the-art-molecular-representation-models">Comparison against State-of-the-Art Molecular Representation Models</h3>

<p>Let’s first take a look at GROVER<d-cite key="rong2020selfsupervisedgraphtransformerlargescale"></d-cite>, a transformer-based GNN boasting 100 million parameters and pre-trained on a massive dataset of 10 million unlabeled molecules.</p>

<p>The authors further fine-tune GROVER on MolHIV and MolPCBA to achieve competitive performance along with supplying additional molecular features such as morgan fingerprints and other 2D features. Note that the Random Forest model fitted on these features alone outperforms the GNN model, showing the huge boost in performance granted by the same.</p>

<p>Table 2: Comparison between Graphormer and GROVER on MolHIV</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th># param.</th>
      <th>AUC (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Morgan Finger Prints + Random Forest</td>
      <td>230K</td>
      <td>80.60±0.10</td>
    </tr>
    <tr>
      <td>GROVER</td>
      <td>48.8M</td>
      <td>79.33±0.09</td>
    </tr>
    <tr>
      <td>GROVER (LARGE)</td>
      <td>107.7M</td>
      <td>80.32±0.14</td>
    </tr>
    <tr>
      <td>Graphormer-FLAG</td>
      <td>47.0M</td>
      <td>80.51±0.53</td>
    </tr>
  </tbody>
</table>

<p>However, as evident in Table 2, Graphormer manages to outperform it consistently on the benchmarks without even using the additional features (known to boost performance), which showcases it increases the expressiveness of complex information.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Graphormer presents a novel way of applying Transformers to graph representation using the three structural encodings. While it has demonstrated strong performance across various benchmark datasets, significant progress has been made since the original paper. Structure-Aware Transformer <d-cite key="chen2022structureawaretransformergraphrepresentation"></d-cite> improves on the initial Transformer by incorporating structural information by extracting subgraph representations. DeepGraph <d-cite key="zhao2023layersbeneficialgraphtransformers"></d-cite> explores the benefits of deeper graph transformers by enhancing global attention with substructure tokens and local attention. Despite the success of these architectures, some challenges still remain; for example, the quadratic complexity of the self-attention module limits its use on large graphs. Therefore, the future development of efficient sequence-based graph-processing networks and the imposing of such constraints for geometric learning are open research areas.</p>]]></content><author><name>Anonymized</name></author><category term="graph" /><category term="representation" /><category term="learning" /><summary type="html"><![CDATA[A first-principles blog post to understand the Graphormer.]]></summary></entry><entry><title type="html">Applications of TopoX to Topological Deep Learning</title><link href="http://localhost:4000/blog/2024/smpn/" rel="alternate" type="text/html" title="Applications of TopoX to Topological Deep Learning" /><published>2024-06-30T00:00:00+03:00</published><updated>2024-06-30T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/smpn</id><content type="html" xml:base="http://localhost:4000/blog/2024/smpn/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>Representation learning using Graph Neural Networks (GNNs) is rapidly growing approach to complex tasks in chemistry <d-cite key="ballester2024attending,bekkers2023fast,eijkelboom2023n,battiloro2024n"></d-cite>. Particularly, in a subset of these tasks a crucial aspect is maintaining equivariance to different transformations such as <em>translation</em>, <em>rotation</em> and <em>reflection</em>. Learning representations such that <strong>equivariance</strong> or <strong>invariance</strong> can be applied has proved very helpful <d-cite key="bekkers2023fast,eijkelboom2023n"></d-cite>. Additionally, incorporating higher-order relations in GNNs such that they encode more complex topological spaces is a recent effort to increase the expressivity of GNNs <d-cite key="hajij2022topological,eijkelboom2023n,giusti2024topological"></d-cite>.</p>

<p>The aim of this blogpost is to draw attention to Topological Deep Learning (TDL) by using the suite of Python packages TopoX <d-cite key="hajij2024topox"></d-cite> to replicate the work of <d-cite key="eijkelboom2023n"></d-cite> and show how much simpler development is in this framework. Additionally, we experiment with a different topological spaces with more geometric information and compare the results with the original work.</p>

<!--
 We will introduce the concepts needed to address **equivariance** and **invariance** as well as definitions of what exactly constitute this *higher-order* structures. Then, we will introduce TopoX and the benefits of development in this framework, show some examples and present the results. Our results show that development in this platform is beneficial for the investigator conducting research as well as the scientific community. New architectures and experiments developed using TopoX allow a standardaized way to share this among people interested in TDL.
 -->
<hr />

<h1 id="message-passing-in-gnns">Message passing in GNNs</h1>
<p>Let \(G = (V,E)\) be a graph consisting of nodes \(V\) and edges \(E\). Then let each node \(v_i \in V\) and edge \(e_{ij} \in E\) have an associated node feature \(\mathbf{f}_i \in \mathbb{R}^{c_n}\) and edge feature \(a_{ij} \in \mathbb{R}^{c_e}\), with dimensionality \(c_n, c_e \in \mathbb{N}_{&gt;0}\). Then, we define a <em>message passing layer</em> as:</p>

\[\begin{equation}\label{compute_message}
\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \mathbf{a}_{i j}\right)
\end{equation}\]

<p>\(\begin{equation}\label{aggregate_messages}
    \mathbf{m}_i=\underset{j \in \mathcal{N}(i)}{\operatorname{Agg}} \mathbf{m}_{i j}
\end{equation}\)
\(\begin{equation}\label{update_hidden}
    \mathbf{h}_i^{l+1}=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{equation}\)</p>

<hr />

<h1 id="equivariance-and-invariance">Equivariance and Invariance</h1>

<p><strong>Invariance</strong> is when an object or set of objects remain the same after a transformation. In contrast, <strong>equivariance</strong> is a symmetry with respect to a function and a transformation. At first glance this definitions might be hard to picture, however with some group theory they will become more clear.</p>

<p>Let \(G\) be a group and let \(X\), \(Y\) be sets on which \(G\) acts. A function \(f: X \rightarrow Y\) is called equivariant with respect to \(G\) if it commutes with the group action. Equation \ref{eq:equi} expresses this notion formally.</p>

\[\begin{equation}\label{eq:equi}
f(g \cdot x)=g \cdot f(x)
\end{equation}\]

<p>Conversly, Equation \ref{eq:inv} shows that <strong>invariance</strong> is when the application of the transformation \(g \in G\) does not affect the output of the map \(f\),</p>

\[\begin{equation}\label{eq:inv}
f(g \cdot x)=f(x) 
\end{equation}\]

<hr />

<h1 id="higher-order-networks-and-why-topology-is-useful">Higher-order networks and why topology is useful</h1>
<p>Regular graph relations fall short of modelling multi-interacions, as such we turn to higher order networks. An <em>abstract simplicial complex</em> (ASC) is the combinatorial expression of a non-empty set of simplices.</p>

<p>Concretly, let \(\mathcal{P}(S)\) be the powerset of \(S\) and another set \(\mathcal{K} \subset \mathcal{P}(S)\), then \(\mathcal{K}\) is an ASC if for every \(X \in \mathcal{K}\) and every non-empty \(Y \subseteq X\) it holds that \(Y \in \mathcal{K}\). Also, we define \(\mid\mathcal{K}\mid\) to be the highest cardinality of a simplex in an ASC minus 1. If the rank is \(r\) then it holds \(\forall X \in \mathcal{K}: r \geq \mid X \mid\).</p>

<!-- 
## Geometric realization

Although an ASC is a purely combinatorial object, it always entails a **geometric realization**. For the case of $$r=1$$ then it can be represented as a graph. A **simplicial complex** is the geometric realization of an ASC, constructed out of the underlying geometric of the points in $$ \mathcal{K}$$. As such, they can be constructed using the set of verticies $$ V $$ of a graph as disjoint points in space or even taking a graph $$ G = (V, E) $$ where $$ V $$ is the set of 0-cells and $$ E $$ is the set of 1-cells. Transforming a set of points to a simplicial complex is called **lifting**. 
 
As an example, we may consider the clique lifting procedure. A *clique* $$ C \subseteq V $$, such that $$ C $$ is complete. In other words, there is an edge between every pair of vertices. In this lifting procedure each clique will become an r-cell and have it's own set of neighbours. Note that the time complexity of this lift is $$ \mathcal{O}(3^{n/3}) $$
-->

<h2 id="higher-order-neighbourhoods">Higher-order neighbourhoods</h2>
<p>To define proximite relations such as graph adjacencies in \(r\)-simplex we establish some definitions. We will work with only two types of adjacencies as they have proven to be as expressive as using all of them. First, let \(\sigma\) and \(\tau\) be two simplices, we say that \(\sigma \text{ is on the bound of } \tau\) as \(\sigma \prec \tau\) and: 1) \(\sigma \subset \tau\), 2) \(\nexists \delta: \sigma  \subset \delta \subset \tau\)</p>

<p>Equation \ref{eq:bound_adj} referes to the relation between a \(r\)-simplex and the \((r-1)\)-simplex that compose it. Equation \ref{eq:bound_up} referes to the relationship between  \((r-1)\)-simplex and other \((r-1)\)-simplex that are a part of a higher \(r\)-simplex. They are also refered to as <strong>cofaces</strong> in the literature <d-cite key="hajij2022topological"></d-cite>.</p>

\[\begin{equation}\label{eq:bound_adj}
\mathcal{B}(\sigma) = \{\tau \mid \tau \prec \sigma\}
\end{equation}\]

\[\begin{equation}\label{eq:bound_up}
\mathcal{N}_{\uparrow}(\sigma) = \{\tau \mid \exists \delta, \tau \prec \delta \land \sigma \prec \delta\}
\end{equation}\]

<hr />
<h1 id="lifting-techniques">Lifting techniques</h1>
<p>How a higher-order representation of points in the space or a particular graph is to be constructed  depends on the properties that want to be attained and, as always, how efficient is to compute. Let’s go over some alternatives.</p>

<h2 id="vietoris-ripps-complex">Vietoris-Ripps Complex</h2>
<p>The Vietoris-Ripps complex is a common way to form a topological space. The time complexity for generating of the procedure depends on the maximum dimension of the simplices in the complex \(r\) and number of points \(n\)  given by \(\mathcal{O}(n^{r+1})\). If the points are embedded in Euclidean space then it is an approximation of a larger and richer complex called the <em>Cech Complex</em>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/rips-lift-480.webp 480w,/assets/img/2024-06-30-smpn/rips-lift-800.webp 800w,/assets/img/2024-06-30-smpn/rips-lift-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/rips-lift.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h2 id="alpha-complex">Alpha Complex</h2>
<p>The alpha complex is a fundamental data structure from computational geometry given by a subset \(\sigma = \{x_{i0},...,x_{ik} \}\subset S\) belongs to \(\operatorname{Alpha}(S,r)\) if there exists a point \(y \in \mathbb{R}^m\) that is equidistant from every member of \(\sigma\), so that</p>

<p>\(\begin{align*}
    \rho := || y- x_{i0}||=...=||y-x_{ik}|| \leq r
\end{align*}\)
and thus \(||y-x|| \leq r\ \ \  \forall x \in S\). 
Formally, the alpha complex is defined as the collection:</p>

\[\begin{align*}
    \operatorname{Alpha}(S, r)=\left\{\sigma \subset S: \bigcap_{x \in \sigma} V_x(r) \neq \emptyset\right\}
\end{align*}\]

<p>A subset \(\sigma\) of size \(k+1\) is called a k-dimensional simplex of \(\operatorname{Alpha}(S,r)\).</p>

<hr />

<h1 id="do-invariances-hold-">Do invariances hold ?</h1>

<p>Equation \ref{eq:msg_eq} comes to replace \ref{compute_message} with our invariant function. Addtionally, to make the network equivariant we introduce feature vector  \(x\) which contains the positional coordinates in euclidean space. Equation \ref{eq:pos_update} refers to the update in the position embedding of the node. The proofs that with this condition equivariance holds can be found in <d-cite key="satorras2021n"></d-cite>.</p>

\[\begin{equation}\label{eq:msg_eq}
 \mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \operatorname{lnv}\left(\mathbf{x}_i^l, \mathbf{x}_j^l\right), \mathbf{a}_{i j}\right)
 \end{equation}\]

\[\begin{equation}\label{eq:pos_update}
    \mathbf{x}_i^{l+1}=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right)
\end{equation}\]

<h2 id="in-simplices-too-">In simplices too ?</h2>

<p>Using the previous definitions of neighbourhoods <d-cite key="eijkelboom2023n"></d-cite> defines a message for each neighbourhood as Equation \ref{eq:msg_boundary} and Equation \ref{eq:msg_ua} and replaces the hidden representation update to take these messages into account in Equation \ref{eq:update_sc}.</p>

\[\begin{equation}\label{eq:msg_boundary}
m_{\mathcal{B}}(\sigma) = \underset{\tau \in \mathcal{B}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{B}}(h^l_{\sigma}, h^l_{\tau})
\end{equation}\]

\[\begin{equation}\label{eq:msg_ua}
m_{\mathcal{N}_{\uparrow}}(\sigma) = \underset{\tau \in \mathcal{N}_{\uparrow}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{N}_{\uparrow}}(h^l_{\sigma}, h^l_{\tau}))
\end{equation}\]

\[\begin{equation}\label{eq:update_sc}
h_{\sigma}^{l+1} = \phi_{h} (h_{\sigma}^l, m_{\mathcal{B}}(\sigma), m_{\mathcal{N}_{\uparrow}}(\sigma))
\end{equation}\]

<p>Finally, they define a graph embedding as Equation \ref{eq:agg_simp} where the simplices \(\mathcal{K}\) of each dimension \(r\) will be aggregated and the final embedding of the complex will be the concatenation of the embedding of each dimension.</p>

\[\begin{equation}\label{eq:agg_simp}

h_{\mathcal{K}} = \bigoplus_{i=0}^{r} \underset{\sigma \in \mathcal{K}, |\sigma|=i+1}{\operatorname{Agg}} h_\sigma
\end{equation}\]

<hr />
<h1 id="the-new-standard-topox">The new standard: TopoX</h1>

<p>TopoX is a suite of Python packages that aim to provide a standard for developments in TDL. It encompases TopoModelX, TopoNetX, TopoEmbeddX and now TopoBenchmarX. Each of them having functionalities according to their name in the topological domain. Next, we ilustrate the development process and reproduction of <d-cite key="eijkelboom2023n"></d-cite> in the TopoX suite. Additionally, as a base project we use the <a href="https://github.com/pyt-team/challenge-icml-2024">ICML TDL Challenge 2024</a> for development, which structures all these packages together to be used in the development cycle. Additionally, we use the Pytorch Geometric QM9 dataset as it contains graph data.</p>

<h2 id="building-structure">Building structure</h2>

<p>The first thing we are concerned with is the <strong>lifting</strong> of our initial graph or set of points. To perform that task we will make use of GHUDI <d-cite key="gudhi:urm"></d-cite>, a Python library with many methods mainly used for Topological Data Analysis. Additionally, we add the option to fully connect all the \(0\)-simplex.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">rips_lift</span><span class="p">(</span><span class="n">graph</span><span class="p">:</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dis</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fc_nodes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SimplicialComplex</span><span class="p">:</span>
    <span class="n">x_0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">graph</span><span class="p">.</span><span class="n">pos</span>

    <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">pos</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>

    <span class="n">rips_complex</span> <span class="o">=</span> <span class="n">gudhi</span><span class="p">.</span><span class="nc">RipsComplex</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span> <span class="n">max_edge_length</span><span class="o">=</span><span class="n">dis</span><span class="p">)</span>
    <span class="n">simplex_tree</span><span class="p">:</span> <span class="n">SimplexTree</span>  <span class="o">=</span> <span class="n">rips_complex</span><span class="p">.</span><span class="nf">create_simplex_tree</span><span class="p">(</span><span class="n">max_dimension</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fc_nodes</span><span class="p">:</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
        <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">simplex_tree</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">SimplicialComplex</span><span class="p">.</span><span class="nf">from_gudhi</span><span class="p">(</span><span class="n">simplex_tree</span><span class="p">)</span></code></pre></figure>

<p>Now we will use the <code class="language-plaintext highlighter-rouge">Graph2SimplicialLifting</code> base class and override the lifting methods such that we transform an input <code class="language-plaintext highlighter-rouge">pytorch_geometric.data.Data</code>.</p>

<p>We override the init to include the <code class="language-plaintext highlighter-rouge">delta</code> parameter that defines the <em>range</em> of the Vietoris-Ripps lift.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span></code></pre></figure>

<p>Now, we override the main method called <code class="language-plaintext highlighter-rouge">lifted_topology</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="k">def</span> <span class="nf">lift_topology</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">simplicial_complex</span> <span class="o">=</span> <span class="nf">rips_lift</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">complex_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">delta</span><span class="p">)</span>

        <span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">):</span>
            <span class="n">feature_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>

        <span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">set_simplex_attributes</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_lifted_topology</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code></pre></figure>

<p>Then, we move on to <code class="language-plaintext highlighter-rouge">_lifted_topology</code> where the object is built. The <code class="language-plaintext highlighter-rouge">get_complex_connectivity</code> is provided by the library and constructs the connectivity matrices.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="k">def</span> <span class="nf">_get_lifted_topology</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">:</span> <span class="n">SimplicialComplex</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">nx</span><span class="p">.</span><span class="n">Graph</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">lifted_topology</span> <span class="o">=</span> <span class="nf">get_complex_connectivity</span><span class="p">(</span>
            <span class="n">simplicial_complex</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">complex_dim</span><span class="p">,</span> <span class="n">signed</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">signed</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">.</span><span class="n">dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">adjacency_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">adjacency_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_dense</span><span class="p">().</span><span class="nf">nonzero</span><span class="p">().</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">incidence_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">incidence_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_dense</span><span class="p">().</span><span class="nf">nonzero</span><span class="p">().</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>


        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">.</span><span class="n">dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">x_idx_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">skeleton</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">lifted_topology</span><span class="p">[</span><span class="sh">"</span><span class="s">x_0</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="nf">list</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">get_simplex_attributes</span><span class="p">(</span><span class="sh">"</span><span class="s">features</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="nf">values</span><span class="p">())</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">lifted_topology</span></code></pre></figure>

<p>Now we have to do a quick detour to talk about <strong>mini-batching</strong></p>

<h2 id="mini-batching-detour">Mini-batching detour</h2>

<p>To be brief in PyG a dataset compromising graphs is concatenated into one big graph that is represented by a block-diagonal matrix. There is a detailed explanation <a href="https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html">here</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SimplexData</span><span class="p">(</span><span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__inc__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="sh">'</span><span class="s">adjacency</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
            <span class="c1">#return torch.tensor([[getattr(self, f'x_{rank}').size(0)], [getattr(self, f'x_{rank}').size(0)]])
</span>        <span class="k">elif</span> <span class="sh">'</span><span class="s">incidence</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_</span><span class="si">{</span><span class="n">rank</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="p">[</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]])</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_0</span><span class="sh">'</span> <span class="ow">or</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_idx_0</span><span class="sh">'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_0</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_1</span><span class="sh">'</span> <span class="ow">or</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_idx_1</span><span class="sh">'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_0</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
        <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_2</span><span class="sh">'</span> <span class="ow">or</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">'</span><span class="s">x_idx_2</span><span class="sh">'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">x_0</span><span class="sh">'</span><span class="p">).</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
        <span class="k">elif</span> <span class="sh">'</span><span class="s">index</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">num_nodes</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nf">super</span><span class="p">().</span><span class="nf">__inc__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__cat_dim__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="sh">'</span><span class="s">adjacency</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="sh">'</span><span class="s">incidence</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="sh">'</span><span class="s">index</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span></code></pre></figure>

<h2 id="piecing-it-all-together">Piecing it all together</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SimplicialVietorisRipsLifting</span><span class="p">(</span><span class="n">Graph2SimplicialLifting</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span>

    <span class="k">def</span> <span class="nf">lift_topology</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">simplicial_complex</span> <span class="o">=</span> <span class="nf">rips_lift</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">complex_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">delta</span><span class="p">)</span>

        <span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">):</span>
            <span class="n">feature_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>

        <span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">set_simplex_attributes</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_lifted_topology</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_lifted_topology</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">:</span> <span class="n">SimplicialComplex</span><span class="p">,</span> <span class="n">graph</span><span class="p">:</span> <span class="n">nx</span><span class="p">.</span><span class="n">Graph</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">lifted_topology</span> <span class="o">=</span> <span class="nf">get_complex_connectivity</span><span class="p">(</span>
            <span class="n">simplicial_complex</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">complex_dim</span><span class="p">,</span> <span class="n">signed</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">signed</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">.</span><span class="n">dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">adjacency_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">adjacency_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_dense</span><span class="p">().</span><span class="nf">nonzero</span><span class="p">().</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">incidence_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">incidence_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_dense</span><span class="p">().</span><span class="nf">nonzero</span><span class="p">().</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">simplicial_complex</span><span class="p">.</span><span class="n">dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">lifted_topology</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">x_idx_</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">skeleton</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">lifted_topology</span><span class="p">[</span><span class="sh">"</span><span class="s">x_0</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span>
            <span class="nf">list</span><span class="p">(</span><span class="n">simplicial_complex</span><span class="p">.</span><span class="nf">get_simplex_attributes</span><span class="p">(</span><span class="sh">"</span><span class="s">features</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="nf">values</span><span class="p">())</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">lifted_topology</span>
   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span><span class="p">:</span>
        <span class="n">initial_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">()</span>
        <span class="n">lifted_topology</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lift_topology</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">lifted_topology</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_lifting</span><span class="p">(</span><span class="n">lifted_topology</span><span class="p">)</span>
        <span class="k">return</span> <span class="nc">SimplexData</span><span class="p">(</span><span class="o">**</span><span class="n">initial_data</span><span class="p">,</span> <span class="o">**</span><span class="n">lifted_topology</span><span class="p">)</span></code></pre></figure>

<p>The only thing missing is to define our prefered <code class="language-plaintext highlighter-rouge">feature_lifting</code>.</p>

<h2 id="giving-meaning-to-structure">Giving meaning to structure</h2>

<p>Now that we have a higher-order topological space, there is only one thing we are missing. <em>What should the embeddings of the \(r\)-simplex higher than \(0\) be ?</em></p>

<p>Authors in <d-cite key="eijkelboom2023n"></d-cite> perform an element-wise mean of the components of lower \(r\)-simplex, however other alternative are also experimentally tried. The following definition comes in place of the <code class="language-plaintext highlighter-rouge">lift_features</code> as can be found in the example <code class="language-plaintext highlighter-rouge">ProjectionSum</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">lift_features</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span> <span class="o">|</span> <span class="nb">dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch_geometric</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Data</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">max_dim</span> <span class="o">=</span> <span class="nf">max</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">if</span> <span class="sh">"</span><span class="s">incidence</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">key</span><span class="p">])</span>


        <span class="n">simplex_test_dict</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># For the k-component point in the i-simplices where k &lt;= i
</span>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># Get the k-node indices of the i-simplices (i.e. the k-components of the i-simplices)
</span>                <span class="n">z_i_idx</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">x_idx_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">'</span><span class="p">][:,</span> <span class="n">k</span><span class="p">]</span>
                <span class="c1"># Get the node embeddings for the k-components of the i-simplices
</span>                <span class="n">z_i</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">x_0</span><span class="sh">'</span><span class="p">][</span><span class="n">z_i_idx</span><span class="p">]</span>
                <span class="n">z</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">z_i</span><span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Mean along the simplex dimension
</span>            <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Assign to each i-simplex the corresponding feature
</span>            <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
                <span class="n">simplex_test_dict</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">simplex_test_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">data</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">x_</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span></code></pre></figure>

<p>Below is a visualization of the embedding process.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/simplex_features-480.webp 480w,/assets/img/2024-06-30-smpn/simplex_features-800.webp 800w,/assets/img/2024-06-30-smpn/simplex_features-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/simplex_features.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h2 id="the-network">The network</h2>
<p>An overall image is presented here</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/arch-480.webp 480w,/assets/img/2024-06-30-smpn/arch-800.webp 800w,/assets/img/2024-06-30-smpn/arch-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h3 id="defining-a-layer">Defining a layer</h3>

<p>We are using the definitions we stated previously and setting a convolutional kernel for the two types of communication: 1) \(r\)-simplex to \(r\)-simplex and \((r-1)\)-simplex to \(r\)-simplex. They will come to be named <code class="language-plaintext highlighter-rouge">convs_same_rank</code> and <code class="language-plaintext highlighter-rouge">convs_low_to_high</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">channels</span><span class="p">,</span>
        <span class="n">max_rank</span><span class="p">,</span>
        <span class="n">n_inv</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span> <span class="c1"># Number of invariances from r-simplex to r-simplex
</span>        <span class="n">aggr_func</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">update_func</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tanh</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">silu</span><span class="sh">"</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">aggr_update_func</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tanh</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">silu</span><span class="sh">"</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_rank</span> <span class="o">=</span> <span class="n">max_rank</span>
        
        <span class="c1"># convolutions within the same rank
</span>        <span class="n">self</span><span class="p">.</span><span class="n">convs_same_rank</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> 
                    <span class="nc">EConv</span><span class="p">(</span>
                        <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
                        <span class="n">weight_channels</span><span class="o">=</span><span class="n">n_inv</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">][</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">],</span> <span class="c1">#from r-simplex to r-simplex
</span>                        <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">with_linear_transform_1</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">with_linear_transform_2</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">with_weighted_message</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">update_func</span><span class="o">=</span><span class="n">update_func</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_rank</span><span class="p">)</span> <span class="c1"># Same rank conv up to r-1
</span>            <span class="p">}</span>
        <span class="p">)</span>

        <span class="c1"># convolutions from lower to higher rank
</span>        <span class="n">self</span><span class="p">.</span><span class="n">convs_low_to_high</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> 
                    <span class="nc">EConv</span><span class="p">(</span>
                        <span class="n">in_channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
                        <span class="n">weight_channels</span><span class="o">=</span><span class="n">n_inv</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">][</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">],</span> <span class="c1">#from r-1-simplex to r-simplex
</span>                        <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">with_linear_transform_1</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">with_linear_transform_2</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">with_weighted_message</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">update_func</span><span class="o">=</span><span class="n">update_func</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_rank</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="c1"># aggregation functions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scatter_aggregations</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ScatterAggregation</span><span class="p">(</span>
                    <span class="n">aggr_func</span><span class="o">=</span><span class="n">aggr_func</span><span class="p">,</span> 
                    <span class="n">update_func</span><span class="o">=</span><span class="n">aggr_update_func</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span></code></pre></figure>

<p>Finally, we define \(\phi_m\) which will be the MLP over the message update.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Perform an update over the received
</span>        <span class="c1"># messages by each other layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">update</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_rank</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">factor</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">target_dict</span> <span class="ow">in</span> <span class="n">n_inv</span><span class="p">.</span><span class="nf">values</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">target_rank</span> <span class="ow">in</span> <span class="n">target_dict</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
                    <span class="n">factor</span> <span class="o">+=</span> <span class="nf">int</span><span class="p">(</span><span class="n">target_rank</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="nf">str</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">update</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">factor</span><span class="o">*</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
                <span class="p">)</span></code></pre></figure>

<p>Now, on the forward pass. We will receive information of features, incidences, adjacencies and the invariances for both of these relationships as well. The steps are the following</p>

<ul>
  <li>Same rank convolutions, where we build a message</li>
  <li>Low to high rank convolutions</li>
  <li>Concat the embeddings for the \(r\)-simplex that have received messages from different hierarchies</li>
  <li>Pass the embeddings over \(\phi_m\)</li>
  <li>Add residual connections</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">adjacencies</span><span class="p">,</span> <span class="n">incidences</span><span class="p">,</span> <span class="n">invariances_r_r</span><span class="p">,</span> <span class="n">invariances_r_r_minus_1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="nf">assert</span><span class="p">(</span><span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">feature</span><span class="p">).</span><span class="nf">any</span><span class="p">())</span>
            <span class="nf">assert</span><span class="p">(</span><span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="nf">isinf</span><span class="p">(</span><span class="n">feature</span><span class="p">).</span><span class="nf">any</span><span class="p">())</span>
        <span class="n">aggregation_dict</span> <span class="o">=</span> <span class="p">{}</span> 

        <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Same rank convolutions
</span>        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_rank</span><span class="p">):</span>
            <span class="c1"># Get the convolution operation for the same rank
</span>            <span class="n">conv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">convs_same_rank</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span>

            <span class="n">rank_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span>
            <span class="n">x_source</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>
            <span class="n">x_target</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>

            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">adjacencies</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>
            <span class="n">x_weights</span> <span class="o">=</span> <span class="n">invariances_r_r</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>

            <span class="c1"># print(rank, x_source.size(), x_target.size(), x_weights.size(), edge_index.size())
</span>
            <span class="n">send_idx</span><span class="p">,</span> <span class="n">recv_idx</span> <span class="o">=</span> <span class="n">edge_index</span>

            <span class="c1"># Run the convolution
</span>            <span class="n">message</span> <span class="o">=</span> <span class="nf">conv</span><span class="p">(</span><span class="n">x_source</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">x_weights</span><span class="p">,</span> <span class="n">x_target</span><span class="p">)</span>

            <span class="nf">assert</span><span class="p">(</span><span class="n">message</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">recv_idx</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="nf">assert</span><span class="p">(</span><span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">message</span><span class="p">).</span><span class="nf">any</span><span class="p">())</span>
            <span class="nf">assert</span><span class="p">(</span><span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="nf">isinf</span><span class="p">(</span><span class="n">message</span><span class="p">).</span><span class="nf">any</span><span class="p">())</span>

            <span class="n">message_aggr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scatter_aggregations</span><span class="p">[</span><span class="n">rank_str</span><span class="p">](</span><span class="n">message</span><span class="p">,</span> <span class="n">recv_idx</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_dim</span><span class="o">=</span><span class="n">x_target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="n">message_aggr</span><span class="p">],</span>
                <span class="sh">"</span><span class="s">recv_idx</span><span class="sh">"</span><span class="p">:</span> <span class="n">recv_idx</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="c1"># Low to high convolutions starting from rank 1 and looking
</span>        <span class="c1"># backward to convolute
</span>        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">max_rank</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">conv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">convs_low_to_high</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span>

            <span class="n">rank_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span>
            <span class="n">rank_minus_1_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span>

            <span class="n">x_source</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">rank_minus_1_str</span><span class="p">]</span>
            <span class="n">x_target</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>
            <span class="n">x_weights</span> <span class="o">=</span> <span class="n">invariances_r_r_minus_1</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>
            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">incidences</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>

            <span class="n">send_idx</span><span class="p">,</span> <span class="n">recv_idx</span> <span class="o">=</span> <span class="n">edge_index</span>

            <span class="c1"># Run the convolution
</span>            <span class="n">message</span> <span class="o">=</span> <span class="nf">conv</span><span class="p">(</span><span class="n">x_source</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">x_weights</span><span class="p">,</span> <span class="n">x_target</span><span class="p">)</span>
            <span class="n">message_aggr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scatter_aggregations</span><span class="p">[</span><span class="n">rank_str</span><span class="p">](</span><span class="n">message</span><span class="p">,</span> <span class="n">recv_idx</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_dim</span><span class="o">=</span><span class="n">x_target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="nf">assert</span><span class="p">(</span><span class="n">message</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">recv_idx</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

            <span class="c1"># Aggregate the message
</span>            <span class="k">if</span> <span class="n">rank_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">aggregation_dict</span><span class="p">:</span>
                <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="n">message_aggr</span><span class="p">],</span>
                    <span class="sh">"</span><span class="s">recv_idx</span><span class="sh">"</span><span class="p">:</span> <span class="n">recv_idx</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank_str</span><span class="p">][</span><span class="sh">'</span><span class="s">message</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">message_aggr</span><span class="p">)</span> 
                <span class="sh">'''</span><span class="s">
                aggregation_dict[rank_str] = {
                    </span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="s">: [prev_msg, message],
                    #</span><span class="sh">"</span><span class="s">recv_idx</span><span class="sh">"</span><span class="s">: torch.cat((aggregation_dict[rank][</span><span class="sh">"</span><span class="s">recv_idx</span><span class="sh">"</span><span class="s">], recv_idx)),
                    for prev_msg in aggregation_dict[rank_str][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="s">]
                }
                </span><span class="sh">'''</span>

        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_rank</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Check for ranks not receiving any messages
</span>            <span class="n">rank_str</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span>
            <span class="k">if</span> <span class="n">rank_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">aggregation_dict</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank_str</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">recv_idx</span> <span class="o">=</span> <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank_str</span><span class="p">][</span><span class="sh">"</span><span class="s">recv_idx</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">x_target</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">rank_str</span><span class="p">]</span>

            <span class="c1"># Aggregate the message
</span>            <span class="c1">#h[rank_str] = self.scatter_aggregations[rank_str](message, recv_idx, dim=0, target_dim=x_target.shape[0])
</span>
        <span class="c1"># Update over the final embeddings with another MLP
</span>
        <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">feat_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">msg_i</span> <span class="ow">in</span> <span class="n">aggregation_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">]:</span>
                <span class="n">feat_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">msg_i</span><span class="p">)</span>
            <span class="n">h</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">feat_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">rank</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">update</span><span class="p">[</span><span class="n">rank</span><span class="p">](</span><span class="n">feature</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">h</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="c1"># Residual connection
</span>        <span class="n">x</span> <span class="o">=</span> <span class="p">{</span><span class="n">rank</span><span class="p">:</span> <span class="n">feature</span> <span class="o">+</span> <span class="n">h</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

        <span class="k">return</span> <span class="n">x</span></code></pre></figure>

<h3 id="all-together">All together</h3>

<p>Piecing it all together, our modules look like this.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">inv_dims</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span> 
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">max_dim</span> <span class="o">=</span> <span class="n">max_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feature_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="nc">EMPSNLayer</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">max_dim</span><span class="p">,</span> <span class="n">aggr_func</span><span class="o">=</span><span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">,</span> <span class="n">update_func</span><span class="o">=</span><span class="sh">"</span><span class="s">silu</span><span class="sh">"</span><span class="p">,</span> <span class="n">aggr_update_func</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_inv</span><span class="o">=</span><span class="n">inv_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="c1"># Pre-pooling operation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pre_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">pre_pool</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span><span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Post-pooling operation over all dimensions
</span>        <span class="c1"># and final classification
</span>        <span class="n">self</span><span class="p">.</span><span class="n">post_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">((</span><span class="n">max_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="p">)</span></code></pre></figure>

<p>In the first line we call <code class="language-plaintext highlighter-rouge">decompose_batch</code> which grabs the batch object and returns the features, the adjacencies and also calculates the invariances for a given batch. Then we proceed running through the network.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">features</span><span class="p">,</span> <span class="n">edge_index_adj</span><span class="p">,</span> <span class="n">edge_index_inc</span><span class="p">,</span> <span class="n">inv_r_r</span><span class="p">,</span> <span class="n">inv_r_minus_1_r</span><span class="p">,</span> <span class="n">x_batch</span> <span class="o">=</span> <span class="nf">decompose_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">max_dim</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">rank</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_embedding</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span> 
            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index_adj</span><span class="p">,</span> <span class="n">edge_index_inc</span><span class="p">,</span> <span class="n">inv_r_r</span><span class="p">,</span> <span class="n">inv_r_minus_1_r</span><span class="p">)</span>

        <span class="c1"># read out
</span>        <span class="n">x</span> <span class="o">=</span> <span class="p">{</span><span class="n">rank</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">pre_pool</span><span class="p">[</span><span class="n">rank</span><span class="p">](</span><span class="n">feature</span><span class="p">)</span> <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">{</span><span class="n">rank</span><span class="p">:</span> <span class="nf">global_add_pool</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">batch</span><span class="o">=</span><span class="n">x_batch</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span> <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="nf">tuple</span><span class="p">([</span><span class="n">feature</span> <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="nf">items</span><span class="p">()]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">post_pool</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># Classifier across 19 variables 
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></code></pre></figure>

<h1 id="experiments">Experiments</h1>

<p>We performed the experiments using both the Alpha Complex Lifting and the Vietoris-Rips Complex Lifting. The figure below show the MAE for validation and training on a random subsample of 1000 molecules. The figure below shows that the overall performance of the Alpha Complex is lower that the Vietoris-Rips complex for this task. We assume this is likely due to the fact that Alpha complex creates fewer (higher order) simplices, which might lead to a worse outcome, especially on this dataset.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/train_mae_topox-480.webp 480w,/assets/img/2024-06-30-smpn/train_mae_topox-800.webp 800w,/assets/img/2024-06-30-smpn/train_mae_topox-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/train_mae_topox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-smpn/validation_mae_topox-480.webp 480w,/assets/img/2024-06-30-smpn/validation_mae_topox-800.webp 800w,/assets/img/2024-06-30-smpn/validation_mae_topox-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-smpn/validation_mae_topox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<h1 id="conclusions">Conclusions</h1>

<p>In this post, we have investigated the novel development suite for Topological Deep Learning and how it can be used to tackle a particular problem. We go over concepts in <strong>geometric deep learning</strong> and show why they work and how can we leverage topological representations to better learn in message passing networks. The usage of the unified TopoX framework allows for ease of development and standarization in regards of the reproducibility. As this framework grows, ease of development in TDL should follow. Additionally, we the work of <d-cite key="eijkelboom2023n"></d-cite> and explore the performance on the Alpha Complex. Given the computing limitations for executing these models, our experiments fall short of complete. However, it remains useful to further explore ways in which to optimize this models, given the pontential expressivity they achieve.</p>]]></content><author><name>Anonymous</name></author><category term="TDL" /><summary type="html"><![CDATA[Studying the properties of message passing accross TNNs using the TopoX Suite]]></summary></entry><entry><title type="html">Learning Embedding Spaces with Metrics via Contrastive Learning</title><link href="http://localhost:4000/blog/2024/contrast-learning/" rel="alternate" type="text/html" title="Learning Embedding Spaces with Metrics via Contrastive Learning" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/contrast-learning</id><content type="html" xml:base="http://localhost:4000/blog/2024/contrast-learning/"><![CDATA[<p>Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired property, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo <d-cite key="he_momentum_2020"></d-cite>, <d-cite key="chen_empirical_2021"></d-cite>, SimCLR <d-cite key="chen_simple_2020"></d-cite>, <d-cite key="chen_big_2020"></d-cite>, and BYOL <d-cite key="grill_bootstrap_2020"></d-cite>, as well as supervised methods such as SupCon <d-cite key="khosla_supervised_2020"></d-cite> and SINCERE <d-cite key="feeney_sincere_2024"></d-cite>.</p>

<p>In contrastive learning, there are two components that determine the constraints on the learned embedding space: the similarity function and the contrastive loss. The similarity function takes a pair of embedding vectors and quantifies how similar they are as a scalar. The contrastive loss determines which pairs of embeddings have similarity evaluated and how the resulting set of similarity values are used to measure error with respect to a task, such as classification. Backpropagating to minimize this error causes a model to learn embeddings that best satisfy the constraints induced by the similarity function and contrastive loss.</p>

<p>This blog post examines how similarity functions and contrastive losses affect the learned embedding spaces. We first examine the different choices for similarity functions and contrastive losses. Then we conclude with a brief case study investigating the effects of different similarity functions on supervised contrastive learning.</p>

<h2 id="similarity-functions">Similarity Functions</h2>

<p>A similarity function \(s(z_1, z_2): \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\) maps a pair of \(d\)-dimensional embedding vectors \(z_1\) and \(z_2\) to a real similarity value, with greater values indicating greater similarity. A temperature hyperparameter \(0 &lt; \tau \leq 1\) is often included, via \(\frac{s(z_1, z_2)}{\tau}\), to scale a similarity function. If the similarity function has a range that is a subset of \(\mathbb{R}\), then \(\tau\) can increase that range. \(\tau\) is omitted for simplicity here.</p>

<h3 id="cosine-similarity">Cosine Similarity</h3>

<p>A common similarity function is cosine similarity:</p>

\[s(z_1, z_2) = \frac{z_1 \cdot z_2}{||z_1|| \cdot ||z_2||}\]

<p>This function measures the cosine of the angle between \(z_1\) and \(z_2\) as a scalar in \([-1, 1]\). Cosine similarity violates the triangle inequality, making it the only similarity function discussed here that is not derived from a distance metric.</p>

<h3 id="negative-arc-length">Negative Arc Length</h3>

<p>The recently proposed negative arc length similarity function <d-cite key="koishekenov_geometric_2023"></d-cite> provides an analogue for cosine similarity that is a distance metric:</p>

\[s(z_1, z_2) = 1 - \frac{\text{arccos}(z_1 \cdot z_2)}{\pi}\]

<p>This function assumes that 
\(||z_1|| = ||z_2|| = 1\)
which is a common normalization <d-cite key="le-khac_contrastive_2020"></d-cite> that restricts the embeddings to a hypersphere. The arc length \(\text{arccos}(z_1 \cdot z_2)\) is a natural choice for comparing such vectors as it is the geodesic distance, or the length of the shortest path between \(z_1\) and \(z_2\) on the hypersphere. Subtracting the arc length converts the distance metric into a similarity function with range \([0, 1]\).</p>

<h3 id="negative-euclidean-distance">Negative Euclidean Distance</h3>

<p>The negative Euclidean distance similarity function is simply:</p>

\[s(z_1, z_2) = -||z_1 - z_2||_2\]

<p>Euclidean distance measures the shortest path in Euclidean space, making it the geodesic distance when \(z_1\) and \(z_2\) can take any value in \(\mathbb{R}^d\). In this case the similarity function has range \([-\infty, 0]\).</p>

<p>The negative Euclidean distance can also be used with embeddings restricted to a hypersphere, resulting in range \([-2, 0]\). However, this is not the geodesic distance for the hypersphere as the path being measured is inside the sphere. The Euclidean distance will be less than the arc length unless \(z_1 = z_2\), in which case they both equal 0.</p>

<h2 id="contrastive-losses">Contrastive Losses</h2>

<p>A contrastive loss function maps a set of embeddings and a similarity function to a scalar value. Losses are written such that derivatives for backpropagation are taken with respect to the embedding \(z\).</p>

<h3 id="margin-losses">Margin Losses</h3>

<p>The original contrastive loss <d-cite key="chopra_learning_2005"></d-cite> maximizes similarity for examples \(z^+\) and minimizes similarity for examples \(z^-\) until the similarity is below margin hyperparameter \(m\):</p>

\[L(z, z^+) = s(z, z^+); L(z, z^-) = \max( 0, m - s(z, z^-) )\]

<p>The structure of this loss implies that \(z_1\) and \(z_2\) share a class if \(s(z_1, z_2) &lt; m\) and otherwise they do not share a class. This margin hyperparameter can be challenging to tune for efficiency throughout the training process because it needs to be satisfiable but also provide \(z^-\) samples within the margin in order to backpropagate the error.</p>

<p>The triplet loss <d-cite key="schroff_facenet_2015"></d-cite> avoids this by using a margin between similarity values:</p>

\[L(z, z^+, z^-) = \max( 0, s(z, z^+) - s(z, z^-) + m)\]

<p>The triplet loss only updates a network when its loss is positive, so finding triplets satisfying that condition are important for learning efficiency.</p>

<p>Lifted Structured Loss <d-cite key="oh_song_deep_2016"></d-cite> handles this by precomputing similarities for all pairs in a batch then selecting the \(z^-\) with maximal similarity:</p>

\[L(z, z^+) = \max( 0, s(z, z^+) + m - \max [ \max_{z^-} s(z, z^-), \max_{z^-} s(z^+, z^-) ] )\]

<p>The Batch Hard loss <d-cite key="hermans_defense_2017"></d-cite> takes this even further by selecting \(z^+\) with minimal similarity:</p>

\[L(z, z^+) = \max( 0, \min_{z^+} [ s(z, z^+) ] + m - \max_{z^-} [ s(z, z^-) ] )\]

<p>The decision to compute the loss based on comparisons between \(z\), a single \(z^+\), and a single \(z^-\) comes with advantages and disadvantages. These methods can be easier to adapt for learning with varying levels of supervision because complete knowledge of whether similarity should be maximized or minimized for each pair in the dataset is not required. However, these methods also make training efficiently difficult and provide relatively loose constraints on the embedding space.</p>

<h3 id="cross-entropy-losses">Cross Entropy Losses</h3>

<p>A common contrastive loss is the Information Noise Contrastive Estimation (InfoNCE) <d-cite key="oord_representation_2019"></d-cite> loss:</p>

\[L(z, z^+, z^-_1, z^-_2, \ldots, z^-_n) = -\log \frac{ e^{s(z, z^+)} }{ e^{s(z, z^+)} + \sum_{i=1}^n e^{s(z, z^-_i)} }\]

<p>InfoNCE is a cross entropy loss whose logits are similarities for \(z\). \(z^+\) is a single embedding whose similarity with \(z\) should be maximized while \(z^-_1, z^-_2, \ldots, z^-_n\) are a set of \(n\) embeddings whose similarity with \(z\) should be minimized. The structure of this loss implies that \(z_1\) shares a class with \(z_2\) if no other embedding has greater similarity with \(z_1\).</p>

<p>The choice of \(z^+\) and \(z^-\) sets varies across methods. The self-supervised InfoNCE loss chooses \(z^+\) to be an embedding of an augmentation of the input that produced \(z\) and \(z^-\) to be the other inputs and augmentations in the batch. This is called instance discrimination because only augmentations of the same input instance have their similarity maximized.</p>

<p>Supervised methods expand the definition of \(z^+\) to also include embeddings which share a class with \(z\). The expectation of InfoNCE loss over choices of \(z^+\) is used to jointly maximize their similarity to \(z\). The Supervised Contrastive (SupCon) loss <d-cite key="khosla_supervised_2020"></d-cite> uses all embeddings not currently set as \(z\) or \(z^+\) as \(z^-\), including embeddings that share a class with \(z\) and therefore will also be used as \(z^+\). This creates loss terms that would minimize similarity between embeddings that share a class. Supervised Information Noise-Contrastive Estimation REvisited (SINCERE) loss <d-cite key="feeney_sincere_2024"></d-cite> removes embeddings that share a class with \(z\) from \(z^-\), leaving only embeddings with different classes. An additional margin hyperparameter can also be added to these losses <d-cite key="barbano_unbiased_2022"></d-cite>, which allows for interpolation between the original losses and losses with the \(e^{s(z, z^+)}\) term removed from the denominator.</p>

<p>Considering a set of similarities during loss calculation allows the loss to implicitly perform hard negative mining <d-cite key="khosla_supervised_2020"></d-cite>, avoiding the challenge of selecting triplets required by a margin loss. The lack of a margin places strict constraints on the embedding space, as similarities are always being pushed towards the maximum or minimum. This enables analysis of embedding spaces that minimize the loss. For example, InfoNCE and SINCERE losses with cosine similarity are minimized by embedding spaces with clusters of inputs mapped to single points (maximizing similarity) that are uniformly distributed on the unit sphere (minimizing similarity) <d-cite key="wang_understanding_nodate"></d-cite>.</p>

<h2 id="case-study-contrastive-learning-on-a-hypersphere">Case Study: Contrastive Learning on a Hypersphere</h2>

<p>Many modern contrastive learning techniques build off of the combination of cosine similarity and cross entropy losses. However, few papers have explored changing similarity functions and losses outside of the context of a more complex model.</p>

<p>Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses. This change is motivated by the desire to use the geodesic distance on the embedding space, which in this case is a unit hypersphere. We investigate whether replacing cosine similarity with negative arc length similarity can improve performance with the SINCERE loss, which is supervised, and how each similarity affects the learned embedding space.</p>

<h3 id="supervised-learning-accuracy">Supervised Learning Accuracy</h3>

<p>We utilize the methodology of Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite> to evaluate if the results of Koishekenov et al. <d-cite key="koishekenov_geometric_2023"></d-cite> generalize to supervised cross entropy losses. Specifically, we train models with SINCERE loss and each similarity function then evaluate the models with nearest neighbor classifiers on the test set.</p>

<table>
  <tr>
   <td>
   </td>
   <td colspan="2"><strong>CIFAR-10</strong>
   </td>
   <td colspan="2"><strong>CIFAR-100</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Similarity</strong>
   </td>
   <td><strong>1NN</strong>
   </td>
   <td><strong>5NN</strong>
   </td>
   <td><strong>1NN</strong>
   </td>
   <td><strong>5NN</strong>
   </td>
  </tr>
  <tr>
   <td>Cosine
   </td>
   <td>95.88
   </td>
   <td>95.91
   </td>
   <td>76.23
   </td>
   <td>76.13
   </td>
  </tr>
  <tr>
   <td>Negative Arc Length
   </td>
   <td>95.66
   </td>
   <td>95.65
   </td>
   <td>75.81
   </td>
   <td>76.41
   </td>
  </tr>
</table>

<p>We find no statistically significant difference based on the 95% confidence interval of the accuracy difference <d-cite key="foody_classification_2009"></d-cite> from 1,000 iterations of test set bootstrapping. This aligns with the results in Feeney and Hughes <d-cite key="feeney_sincere_2024"></d-cite>, which found a similar lack of statistically significant results across choices of supervised contrastive cross entropy losses. This suggests that supervised learning accuracy is similar across choices of reasonable similarity functions and contrastive losses.</p>

<h3 id="supervised-learning-embedding-space">Supervised Learning Embedding Space</h3>

<p>We also visualize the learned embedding space for each CIFAR-10 model. For each test set image, the similarity value is plotted for the closest training set image that shares a class (“Target”) and that does not share a class (“Noise”). This visualizes the 1-nearest neighbor decision process. Both similarity functions are plotted for each model, with the title denoting the similarity function used during training.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/cos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with cosine similarity maximizes the similarity to target images well. There are a small number of noise images with near maximal similarity, but the majority are below 0.3 cosine similarity. Interestingly, the peaks seen in the noise similarity reflects the fact that individual classes will have different modes of their noise histograms.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_cos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-480.webp 480w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-800.webp 800w,/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-13-contrast-learning/arccos_cifar10_all_arccos.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The model trained with negative arc length similarity does a better job of forcing target similarity values very close to 1 negative arc length similarity, but also has a notable number of target similarities near 0.5 negative arc length similarity. The noise distribution also reflects the fact that individual classes have different modes for their noise histograms, but in this case the modes are spread across more similarity values. Notably the peak for the horse class is very close to the max similarity due to a high similarity to the dog class, although they are still separated enough from the target similarities to not have an impact on accuracy.</p>

<h3 id="discussion">Discussion</h3>

<p>The choice of similarity function clearly has an effect on the learned embedding space despite a lack of statistically significant changes in accuracy. The cosine similarity histogram most cleanly aligns with the intuition that contrastive losses should be maximizing and minimizing similarities. In contrast, the negative arc length similarity histogram suggests similarity minimization is sacrificed for very consistent maximization. These differences in the learned embedding spaces could affect performance on downstream tasks such as transfer learning.</p>]]></content><author><name>Anonymous</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired property, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo , , SimCLR , , and BYOL , as well as supervised methods such as SupCon and SINCERE .]]></summary></entry><entry><title type="html">Cancer Immunotherapy Design with Geometric Deep Learning</title><link href="http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI/" rel="alternate" type="text/html" title="Cancer Immunotherapy Design with Geometric Deep Learning" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI</id><content type="html" xml:base="http://localhost:4000/blog/2024/Cancer-Immunotherapy-with-AI/"><![CDATA[<h1 id="cancer-immunotherapy-design-with-geometric-deep-learning">Cancer Immunotherapy Design with Geometric Deep Learning</h1>

<p>Cancer remains one of the most formidable challenges in medicine, but recent advancements in immunotherapy and artificial intelligence (AI) are opening new frontiers in treatment. This blog post explores the intersection of cancer immunotherapy and AI.</p>

<h2 id="what-is-cancer">What is Cancer?</h2>

<p>Cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. It begins when genetic mutations in a cell’s DNA disrupt the normal process of cell growth, division, and death. This leads to the formation of tumors, which can be benign (non-cancerous) or malignant (cancerous). Malignant tumors can invade nearby tissues and metastasize, spreading to other parts of the body. There are various types of cancer, classified by the type of cell initially affected <d-cite key="hanahan2011hallmarks"></d-cite>. Treatment options vary and may include surgery, radiation therapy, chemotherapy, targeted therapy, and immunotherapy.</p>

<h2 id="why-is-cancer-so-difficult-to-cure">Why is Cancer So Difficult to Cure?</h2>

<p>Cancer is challenging to cure due to its genetic diversity and constant mutation, leading to treatment resistance <d-cite key="siegel2020colorectal"></d-cite>. Tumors consist of heterogeneous cells, making it difficult for a single therapy to target all cancer cells effectively. Additionally, cancer cells can invade nearby tissues and metastasize, forming secondary tumors that are hard to detect and treat. The ability of cancer cells to evade the immune system and the protective nature of the tumor microenvironment further complicate treatment efforts. Moreover, many cancer treatments, such as chemotherapy and radiation, can harm healthy cells, causing severe side effects and limiting safe dosage levels. Late detection often means the disease is more advanced and harder to treat successfully. Personalized treatment plans are necessary but complex to develop, as each patient’s cancer is unique. These factors underscore the need for ongoing research to develop more effective and targeted therapies.</p>

<h2 id="what-is-cancer-immunotherapy-and-what-makes-it-different">What is Cancer Immunotherapy and What Makes It Different?</h2>

<p>Cancer immunotherapy is a treatment that enhances the body’s immune response to fight cancer. It involves mechanisms such as the Major Histocompatibility Complex (MHC) and T-cell response. MHC is a protein complex found on almost all nucleated cells in the body that presents fragments of proteins (antigens) on their surface. These antigens can be normal self-antigens or, in the case of cancer cells, abnormal or mutated antigens. T-cells, a type of white blood cell, patrol the body searching for these antigens. When a T-cell recognizes an abnormal antigen presented by the MHC on a cancer cell, it becomes activated and can kill cancer cells directly or recruit other immune cells to help eliminate the cancer.</p>

<p>Cancer immunotherapy is relatively new compared to traditional treatments like surgery, chemotherapy, and radiation. The concept dates back over a century, but significant advancements have been made in the past few decades. The first immune checkpoint inhibitor, ipilimumab (Yervoy), was approved by the FDA in 2011 <d-cite key="pardoll2012blockade"></d-cite>.</p>

<p>The main types of cancer immunotherapy include immune checkpoint inhibitors, CAR-T cell therapy, and cancer vaccines. Immune checkpoint inhibitors block proteins that prevent the immune system from attacking cancer cells more effectively. CAR-T cell therapy involves modifying a patient’s T cells to target and kill cancer cells <d-cite key="june2018car"></d-cite>. Cancer vaccines stimulate the immune system to attack cancer cells by presenting them with specific antigens found on cancer cells <d-cite key="melief2015therapeutic"></d-cite>.</p>

<p>Immunotherapy differs from traditional treatments in several ways. It enhances the body’s immune response to identify and attack cancer cells, potentially leading to long-lasting protection against cancer recurrence. Immunotherapy can be highly specific, targeting only cancer cells and sparing normal cells, reducing side effects compared to traditional therapies. Some patients experience long-term remission with immunotherapy, as the immune system can continue to recognize and attack cancer cells even after treatment ends. Additionally, immunotherapy can be combined with other cancer treatments to enhance overall effectiveness.</p>

<p>These distinctive features make cancer immunotherapy a promising and rapidly evolving area in cancer treatment. Understanding the interaction between the immune system and cancer is key to overcoming the challenges and improving the effectiveness of these therapies. Therefore, it is essential to explore how AI can enhance the development and effectiveness of cancer immunotherapy.</p>

<h2 id="ai-for-cancer-immunotherapy">AI for Cancer Immunotherapy</h2>

<p>The two most important interactions are peptide-MHC interaction, to predict which peptides will be shown at the cell surface, and MHC-T-Cell interaction. In this blog post, I will focus on peptide-MHC structure modeling. Accurate modeling of the 3D structure and properties of p-MHC can lead to better-targeted therapies and improved immune responses against cancer cells. Additionally, speed is extremely important due to the massive number of peptide-MHC pairs for each patient and the diversity of MHCs. While there is a very good physical simulation model called Pandora <d-cite key="marzella2022pandora"></d-cite>, it is much slower than potential neural network-based models. One major challenge for using neural networks here is that there is very limited data available, making it challenging to generalize from.</p>

<h2 id="peptide-mhc-structure-modeling-with-diffusion-models">Peptide-MHC Structure Modeling with Diffusion Models</h2>

<p>We need to choose a framework for modeling p-MHC. AI for structural biology has developed quickly in the last few years. The first major breakthroughs in using AI for biology came with non-generative models like AlphaFold 2 <d-cite key="jumper2021highly"></d-cite> and RoseTTAFold <d-cite key="baek2021accurate"></d-cite>. AlphaFold 2 and RoseTTAFold revolutionized protein structure prediction by accurately modeling the three-dimensional shapes of proteins from their amino acid sequences. Both models have significantly advanced our understanding of protein folding, paving the way for new therapeutic discoveries. However, generative models represent the next frontier in AI for biology. Unlike non-generative models that predict static outcomes, generative models can create new data instances, offering more dynamic and flexible solutions. Diffusion models <d-cite key="yang2023diffusion"></d-cite>, flow matching <d-cite key="lipman2022flow"></d-cite>, and GFlowNets <d-cite key="bengio2023gflownet"></d-cite> are prominent examples of generative models. Diffusion models, in particular, have gained popularity for their ability to generate high-quality molecular structures by simulating the gradual process of denoising from a random state.</p>

<p>Work using Diffusion Models such as DiffSBDD <d-cite key="schneuing2022structure"></d-cite> has shown that it is possible to create small molecules for protein pockets using diffusion models. RF-Diffusion has demonstrated the ability to design the backbone structure of proteins with these models. Very recently, AlphaFold 3 <d-cite key="abramson2024accurate"></d-cite> has put diffusion models in the spotlight again by replacing AlphaFold 2’s structure prediction model, extending the structure prediction capabilities from proteins to any multi-component complex in biology. Given their flexibility and power, we choose a diffusion model for our task.</p>

<h3 id="choosing-an-input-representation">Choosing an Input Representation</h3>

<p>For p-MHC we are dealing with a large number of degrees of freedom. For this reason, our first aim is to reduce the degrees of freedom with prior knowledge as much as possible. It has been shown that modeling peptides and proteins at their amino acid level is sufficient to model the full structure. This means that instead of modelling every single atom of the peptide and the MHC pocket we only model a single node for each amino acid. We can later add back the remaining atoms using a simple non-generative regression model <d-cite key="dauparas2022robust"></d-cite>. For the amino acid, we can use several representations:</p>

<ul>
  <li><strong>Frame-based representation:</strong> Captures the structure of the entire amino acid.</li>
  <li><strong>C-alpha only representation:</strong> Uses only the alpha carbon of the amino acid, simplifying the structure.</li>
  <li><strong>C-alpha + side chain orientation representation:</strong> Includes both the alpha carbon and the side chain orientation, providing more detailed structural information.</li>
</ul>

<p>In this blog post, we will discuss C-alpha only representation and C-alpha + side chain orientation representation. Since only the protein pocket is relevant for the binding process and its structure is largely fixed, we can use AlphaFold2 to predict the structure of the MHC and its pocket. There are a lot more peptides than there are MHCs for each person, which makes this approach computationally feasible.</p>

<p>First, we use the 3D position of the alpha carbon (C-alpha only representation) and a one hot encoding of the amino acid type as node feature.</p>

<h3 id="concept-of-equivariance-and-equivariant-neural-networks">Concept of Equivariance and Equivariant Neural Networks</h3>

<p>For the neural network, we can use our knowledge of geometric representations. The joint translation and rotation of the peptide-MHC complex does not matter for its function. But if we use a normal graph neural network, then it would have to learn to treat a translated or rotated complex the same as the original complex. This usually requires more data and is generally harder. For our task, we want our neural network to be equivariant to rotation and translation but not to reflection, hence SE(3) normally. An equivariant function with respect to a group transforms the input in a way that applying a group action to the input is equivalent to applying the same group action to the output. There are different ways to encode equivariance into a network’s architecture. The most straightforward way is to only let the network use inherently equivariant information.</p>

<p>For a graph in (\mathbb{R}^3), the weighted sum of pairwise differences between nodes is inherently equivariant with respect to rotation and translation. This is the key idea behind the construction of the EGNN network <d-cite key="satorras2021n"></d-cite>, which is the default choice for equivariant diffusion models.</p>

<p>Using amino acids as nodes is very efficient but it does lose a lot of potential information, for instance, the amino acid sidechain orientation (C-alpha + side chain orientation representation). While the neural network might be able to learn this implicitly from enough data, we can also give the orientation as further input without increasing the number of nodes. This, however, changes the space that the SE(3) group acts on to (\mathbb{R}^3 \times S^2) or (\mathbb{R}^3 \times S^3). The advantage of using (\mathbb{R}^3 \times S^2) is that it is a lot more computationally efficient than indexing on (\mathbb{R}^3 \times S^3). Further, (\mathbb{R}^3 \times S^2) is sufficient to fully represent information on (\mathbb{R}^3 \times S^3).</p>

<p>Ponita <d-cite key="bekkers2023fast"></d-cite> is a new, very lean and fast architecture. It achieves equivariance on (\mathbb{R}^3 \times S^2) by defining a bijective mapping from any point pair to an invariant attribute such that any point pair in the equivalence class of the group G is mapped to the same attribute and any attribute maps only to one such equivalence class.</p>

\[\mathbb{R}^3 \times S^2 : \quad [(\mathbf{p}_i, \mathbf{o}_i), (\mathbf{p}_j, \mathbf{o}_j)] \quad \mapsto \quad a_{ij} = \left( \begin{array}{c} 
\| (\mathbf{p}_j - \mathbf{p}_i) - \mathbf{o}_i^\top (\mathbf{p}_j - \mathbf{p}_i) \mathbf{o}_i \| \\
\mathbf{o}_i^\top (\mathbf{p}_j - \mathbf{o}_i) \\
\arccos (\mathbf{o}_i^\top \mathbf{o}_j)
\end{array} \right)\]

<p>Ponita doesn’t have the high computational overhead of using specialized Clebsch-Gordan tensor products and is very efficient due to the ability to separate group convolutions over different group parts. It also achieves state-of-the-art performance on QM9.</p>

<p>Both using the C-alpha atom only representation with an EGNN and using the C-alpha atom + sidechain orientation representation with Ponita have shown great promise for modeling molecules and for conditional generation in my experiments. Peptide-MHC is difficult as the data we are working with is often not very diverse, which makes it hard to learn for generative models. There are numerous more implementation details involved in building a model like this, which exceed the scope of this blog post. The two most significant, which I want to mention, are encoding the location of the amino acids within the peptide chain as well as using energy guidance to guide the denoising process during sampling. When constructed correctly, a diffusion model can effectively predict the joint structure of peptide-MHCs.</p>

<p><img src="pMHC-Diffusion.png" alt="Diffusion Model generates peptide-MHC structure from random noise: Round points represent C-alpha atoms, while the rest forms the MHC protein pocket. Peptide nodes start as random noise and are progressively denoised by the trained diffusion model into the correct structure." /></p>

<h2 id="open-challenges">Open Challenges</h2>

<p>Beyond peptide-MHC interaction, many open problems in cancer immunotherapy can potentially be solved with AI. Modeling the structure of T-cells interacting with p-MHCs is one such problem. The regions of the T-cell receptors are highly variable, making predicting their structure even more challenging than that of p-MHC. Additionally, designing new TCRs (T-cell receptors) with feature diffusion is highly relevant for enhanced T-cell therapy, where TCRs are modified to target a patient’s specific cancer. Adding time dynamics to the structure modeling is also an intriguing direction, as biological complexes are always dynamic and change their structure to fulfill different functions at different times. Other promising methods for molecule generation include Diffusion Models, Flow Matching, and GFlow Networks. Finally, there is a significant need for better evaluation metrics. Current metrics often result in molecules that look good theoretically but fail in biological experiments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>AI has immense potential to revolutionize cancer immunotherapy by providing advanced tools to model and predict complex biological interactions. While significant progress has been made, many challenges remain. Continued advancements in AI and machine learning will be critical in overcoming these challenges and developing more effective, personalized cancer treatments.</p>

<h2 id="references">References</h2>

<p>[1] Josh Abramson et al. “Accurate structure prediction of biomolecular interactions with AlphaFold 3”. In: Nature (2024), pp. 1–3.<br />
[2] Minkyung Baek et al. “Accurate prediction of protein structures and interactions using a three-track neural network”. In: Science 373.6557 (2021), pp. 871–876.<br />
[3] Erik J Bekkers et al. “Fast, Expressive SE (n) Equivariant Networks through Weight-Sharing in Position-Orientation Space”. In: arXiv preprint arXiv:2310.02970 (2023).<br />
[4] Yoshua Bengio et al. “Gflownet foundations”. In: The Journal of Machine Learning Research 24.1 (2023), pp. 10006–10060.<br />
[5] Justas Dauparas et al. “Robust deep learning–based protein sequence design using ProteinMPNN”. In: Science 378.6615 (2022), pp. 49–56.<br />
[6] Douglas Hanahan and Robert A Weinberg. “Hallmarks of cancer: the next generation”. In: cell 144.5 (2011), pp. 646–674.<br />
[7] John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: nature 596.7873 (2021), pp. 583–589.<br />
[8] Carl H June et al. “CAR T cell immunotherapy for human cancer”. In: Science 359.6382 (2018), pp. 1361–1365.<br />
[9] Yaron Lipman et al. “Flow matching for generative modeling”. In: arXiv preprint arXiv:2210.02747 (2022).<br />
[10] Dario F Marzella et al. “PANDORA: a fast, anchor-restrained modelling protocol for peptide: MHC complexes”. In: Frontiers in Immunology 13 (2022), p. 878762.<br />
[11] Cornelis JM Melief et al. “Therapeutic cancer vaccines”. In: The Journal of clinical investigation 125.9 (2015), pp. 3401–3412.<br />
[12] Drew M Pardoll. “The blockade of immune checkpoints in cancer immunotherapy”. In: Nature reviews cancer 12.4 (2012), pp. 252–264.<br />
[13] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. “E (n) equivariant graph neural networks”. In: International conference on machine learning. PMLR. 2021, pp. 9323–9332.<br />
[14] Arne Schneuing et al. “Structure-based drug design with equivariant diffusion models”. In: arXiv preprint arXiv:2210.13695 (2022).<br />
[15] Rebecca L Siegel et al. “Colorectal cancer statistics, 2020”. In: CA: a cancer journal for clinicians 70.3 (2020), pp. 145–164.<br />
[16] Ling Yang et al. “Diffusion models: A comprehensive survey of methods and applications”. In: ACM Computing Surveys 56.4 (2023), pp. 1–39.</p>]]></content><author><name>Anonymous</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[Cancer Immunotherapy Design with Geometric Deep Learning]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="http://localhost:4000/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+03:00</published><updated>2021-05-22T00:00:00+03:00</updated><id>http://localhost:4000/blog/2021/distill</id><content type="html" xml:base="http://localhost:4000/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/distill-template/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/distill-template/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="images">Images</h2>

<p>This is an example post with image galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/9-480.webp 480w,/assets/img/distill-template/9-800.webp 800w,/assets/img/distill-template/9-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<p>Images can be made zoomable.
Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/8-480.webp 480w,/assets/img/distill-template/8-800.webp 800w,/assets/img/distill-template/8-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/10-480.webp 480w,/assets/img/distill-template/10-800.webp 800w,/assets/img/distill-template/10-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/11-480.webp 480w,/assets/img/distill-template/11-800.webp 800w,/assets/img/distill-template/11-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/12-480.webp 480w,/assets/img/distill-template/12-800.webp 800w,/assets/img/distill-template/12-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>
    <p>Unordered list can use asterisks</p>
  </li>
  <li>
    <p>Or minuses</p>
  </li>
  <li>
    <p>Or pluses</p>
  </li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>