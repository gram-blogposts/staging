
@inproceedings{he_momentum_2020,
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
	urldate = {2023-02-07},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year = {2020},
	pages = {9729--9738},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\74Q5NEFE\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}

@inproceedings{chen_empirical_2021,
	title = {An Empirical Study of Training Self-Supervised Vision Transformers},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-09},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	year = {2021},
	pages = {9640--9649},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\PEZ3YU49\\Chen et al. - 2021 - An Empirical Study of Training Self-Supervised Vis.pdf:application/pdf},
}

@inproceedings{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-02-07},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\YH9678I3\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\patri\\Zotero\\storage\\43FQGARF\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{chen_big_2020,
	title = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html},
	urldate = {2023-07-28},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
	year = {2020},
	pages = {22243--22255},
	annote = {SimCLRv2
},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\HSFFQCC3\\Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf:application/pdf},
}

@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6\% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
	urldate = {2023-03-08},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	year = {2020},
	pages = {21271--21284},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\EUELZWY5\\Grill et al. - 2020 - Bootstrap Your Own Latent - A New Approach to Self.pdf:application/pdf},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised Contrastive Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. In reduced data settings, it outperforms cross-entropy significantly. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2024-03-20},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	year = {2020},
	pages = {18661--18673},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\RHVNGPT3\\Khosla et al. - 2020 - Supervised Contrastive Learning.pdf:application/pdf},
}

@misc{feeney_sincere_2024,
	title = {SINCERE: Supervised Information Noise-Contrastive Estimation REvisited},
	copyright = {All rights reserved},
	shorttitle = {{SINCERE}},
	url = {http://arxiv.org/abs/2309.14277},
	doi = {10.48550/arXiv.2309.14277},
	abstract = {The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic bound that relates SINCERE loss to the symmeterized KL divergence between data-generating distributions for a target class and all other classes.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Feeney, Patrick and Hughes, Michael C.},
	month = feb,
	year = {2024},
	note = {arXiv:2309.14277 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\patri\\Zotero\\storage\\K7IJK9JI\\Feeney and Hughes - 2024 - SINCERE Supervised Information Noise-Contrastive .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\patri\\Zotero\\storage\\GNBELMHL\\2309.html:text/html},
}

@inproceedings{koishekenov_geometric_2023,
	address = {Paris, France},
	title = {Geometric Contrastive Learning},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350307443},
	url = {https://ieeexplore.ieee.org/document/10350974/},
	doi = {10.1109/ICCVW60793.2023.00028},
	abstract = {Contrastive learning has been a long-standing research area due to its versatility and importance in learning representations. Recent works have shown improved results if the learned representations are constrained to be on a hypersphere. However, this prior geometric constraint is not fully utilized during training. In this work, we propose making use of geodesic distances on the hypersphere to learn contrasts between representations. Through empirical results, we show that this contrastive learning approach improves downstream tasks across different contrastive learning frameworks. We show that having geometric inductive priors perform even better in contrastive learning if used along with other correct geometric information.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
	publisher = {IEEE},
	author = {Koishekenov, Yeskendir and Vadgama, Sharvaree and Valperga, Riccardo and Bekkers, Erik J.},
	month = oct,
	year = {2023},
	pages = {206--215},
	file = {Koishekenov et al. - 2023 - Geometric Contrastive Learning.pdf:C\:\\Users\\patri\\Zotero\\storage\\VQG7YTCA\\Koishekenov et al. - 2023 - Geometric Contrastive Learning.pdf:application/pdf},
}

@article{le-khac_contrastive_2020,
	title = {Contrastive Representation Learning: A Framework and Review},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Contrastive {Representation} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9226466/},
	doi = {10.1109/ACCESS.2020.3031549},
	abstract = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many ﬁelds and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simpliﬁes and uniﬁes many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-ﬁelds of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
	language = {en},
	urldate = {2023-11-09},
	journal = {IEEE Access},
	author = {Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.},
	year = {2020},
	pages = {193907--193934},
	file = {Le-Khac et al. - 2020 - Contrastive Representation Learning A Framework a.pdf:C\:\\Users\\patri\\Zotero\\storage\\VSSIF2YB\\Le-Khac et al. - 2020 - Contrastive Representation Learning A Framework a.pdf:application/pdf},
}

@inproceedings{chopra_learning_2005,
	title = {Learning a similarity metric discriminatively, with application to face verification},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/1467314?casa_token=fbldvqZUi0gAAAAA:sIzRPsJfUiIGmux-j7j6aoov1PNxo944AwjVUdSzTh47MueCdAM3fLFyFf20oLu2lr33z_ph},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
	urldate = {2024-05-28},
	booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Support vector machine classification, Support vector machines, Artificial neural networks, Character generation, Drives, Face recognition, Glass, Robustness, Spatial databases, System testing},
	pages = {539--546 vol. 1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\patri\\Zotero\\storage\\LASRM6BZ\\1467314.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\N5HSX4RP\\Chopra et al. - 2005 - Learning a similarity metric discriminatively, wit.pdf:application/pdf},
}

@inproceedings{schroff_facenet_2015,
	title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
	shorttitle = {{FaceNet}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
	urldate = {2024-05-28},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	pages = {815--823},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\LKVW6L9T\\Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf:application/pdf},
}

@inproceedings{oh_song_deep_2016,
	title = {Deep Metric Learning via Lifted Structured Feature Embedding},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Metric_Learning_CVPR_2016_paper.html},
	urldate = {2024-05-28},
	author = {Oh Song, Hyun and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},
	year = {2016},
	pages = {4004--4012},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\EUNE7MSG\\Oh Song et al. - 2016 - Deep Metric Learning via Lifted Structured Feature.pdf:application/pdf},
}

@misc{hermans_defense_2017,
	title = {In Defense of the Triplet Loss for Person Re-Identification},
	url = {http://arxiv.org/abs/1703.07737},
	doi = {10.48550/arXiv.1703.07737},
	abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = nov,
	year = {2017},
	note = {arXiv:1703.07737 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Lucas Beyer and Alexander Hermans contributed equally. Updates: Minor fixes, new SOTA comparisons, add CUHK03 results},
	file = {arXiv Fulltext PDF:C\:\\Users\\patri\\Zotero\\storage\\MFIWVDIS\\Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\patri\\Zotero\\storage\\VHUY234L\\1703.html:text/html},
}

@misc{oord_representation_2019,
	title = {Representation Learning with Contrastive Predictive Coding},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\patri\\Zotero\\storage\\SL4LLPRV\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\patri\\Zotero\\storage\\QCEAPBD7\\1807.html:text/html},
}

@inproceedings{barbano_unbiased_2022,
	title = {Unbiased Supervised Contrastive Learning},
	url = {https://openreview.net/forum?id=Ph5cJSfD2XN},
	abstract = {Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (\${\textbackslash}epsilon\$-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets including CIFAR10, CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with \${\textbackslash}epsilon\$-SupInfoNCE, reaching state-of-the-art performance on a number of biased datasets, including real instances of biases "in the wild".},
	language = {en},
	urldate = {2024-05-28},
	author = {Barbano, Carlo Alberto and Dufumier, Benoit and Tartaglione, Enzo and Grangetto, Marco and Gori, Pietro},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\patri\\Zotero\\storage\\TAKGZ7JE\\Barbano et al. - 2022 - Unbiased Supervised Contrastive Learning.pdf:application/pdf},
}

@article{wang_understanding_nodate,
	title = {Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
	abstract = {Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets conﬁrm the strong agreement between both metrics and downstream task performance. Directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: ssnl.github.io/hypersphere. Code: github.com/SsnL/align uniform. Alignment: Similar samples have similar features.},
	language = {en},
	author = {Wang, Tongzhou and Isola, Phillip},
	file = {Wang and Isola - Understanding Contrastive Representation Learning .pdf:C\:\\Users\\patri\\Zotero\\storage\\EW5A5T8B\\Wang and Isola - Understanding Contrastive Representation Learning .pdf:application/pdf},
}

@article{foody_classification_2009,
	title = {Classification accuracy comparison: Hypothesis tests and the use of confidence intervals in evaluations of difference, equivalence and non-inferiority},
	volume = {113},
	issn = {0034-4257},
	shorttitle = {Classification accuracy comparison},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425709000923},
	doi = {10.1016/j.rse.2009.03.014},
	abstract = {The comparison of classification accuracy statements has generally been based upon tests of difference or inequality when other scenarios and approaches may be more appropriate. Procedures for evaluating two scenarios with interest focused on the similarity in accuracy values, non-inferiority and equivalence, are outlined following a discussion of tests of difference (inequality). It is also suggested that the confidence interval of the difference in classification accuracy may be used as well as or instead of conventional hypothesis testing to reveal more information about the disparity in the classification accuracy values compared.},
	number = {8},
	urldate = {2024-05-28},
	journal = {Remote Sensing of Environment},
	author = {Foody, Giles M.},
	month = aug,
	year = {2009},
	keywords = {Classification accuracy, Confidence interval, Difference, Hypothesis test},
	pages = {1658--1663},
	file = {ScienceDirect Snapshot:C\:\\Users\\patri\\Zotero\\storage\\3BDBKFT2\\S0034425709000923.html:text/html},
}
