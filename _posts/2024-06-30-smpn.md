---
layout: distill
title: Applications of TopoX to Topological Deep Learning
description: Studying the properties of message passing accross TNNs using the TopoX Suite
tags: TDL
giscus_comments: true
date: 2024-06-30
featured: true
feature: true


authors:
  - name: Anonymous

bibliography: 2024-06-30-smpn.bib
toc:
  - name: Introduction 
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Message Passing in GNNs
  - name: Equivariance and Invariance
  - name: Higher-order networks and why topology is useful
  - subsections:
    - name: Higher-order neighbourhoods
  - name: Lifting techniques
  - subsections: 
    - name: Vietoris-Rips Complex
    - name: Alpha Complex
  - name: Do invariances hold ?
  - subsections:
    - name: "In simplices too ?"
  - name: "The new standard: TopoX"
  - subsections:
    - name: Building structure
    - name: Mini-batching detour
    - name: Piecing it all together
  - name: Experiments
  - name: Conclusions

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
---
# Introduction

Representation learning using Graph Neural Networks (GNNs) is rapidly growing approach to complex tasks in chemistry <d-cite key="ballester2024attending,bekkers2023fast,eijkelboom2023n,battiloro2024n"></d-cite>. Particularly, in a subset of these tasks a crucial aspect is maintaining equivariance to different transformations such as *translation*, *rotation* and *reflection*. Learning representations such that **equivariance** or **invariance** can be applied has proved very helpful <d-cite key="bekkers2023fast,eijkelboom2023n"></d-cite>. Additionally, incorporating higher-order relations in GNNs such that they encode more complex topological spaces is a recent effort to increase the expressivity of GNNs <d-cite key="hajij2022topological,eijkelboom2023n,giusti2024topological"></d-cite>.


The aim of this blogpost is to draw attention to Topological Deep Learning (TDL) by using the suite of Python packages TopoX <d-cite key="hajij2024topox"></d-cite> to replicate the work of <d-cite key='eijkelboom2023n'></d-cite> and show how much simpler development is in this framework. Additionally, we experiment with a different topological spaces with more geometric information and compare the results with the original work.


<!--
 We will introduce the concepts needed to address **equivariance** and **invariance** as well as definitions of what exactly constitute this *higher-order* structures. Then, we will introduce TopoX and the benefits of development in this framework, show some examples and present the results. Our results show that development in this platform is beneficial for the investigator conducting research as well as the scientific community. New architectures and experiments developed using TopoX allow a standardaized way to share this among people interested in TDL.
 -->
---

# Message passing in GNNs 
Let $$G = (V,E)$$ be a graph consisting of nodes $$V$$ and edges $$E$$. Then let each node $$ v_i \in V$$ and edge $$e_{ij} \in E$$ have an associated node feature $$\mathbf{f}_i \in \mathbb{R}^{c_n} $$ and edge feature $$ a_{ij} \in \mathbb{R}^{c_e} $$, with dimensionality $$ c_n, c_e \in \mathbb{N}_{>0} $$. Then, we define a *message passing layer* as:

$$
\begin{equation}\label{compute_message}
\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \mathbf{a}_{i j}\right)
\end{equation}
$$

$$
\begin{equation}\label{aggregate_messages}
    \mathbf{m}_i=\underset{j \in \mathcal{N}(i)}{\operatorname{Agg}} \mathbf{m}_{i j}
\end{equation}
$$
$$
\begin{equation}\label{update_hidden}
    \mathbf{h}_i^{l+1}=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{equation}
$$

---

# Equivariance and Invariance

**Invariance** is when an object or set of objects remain the same after a transformation. In contrast, **equivariance** is a symmetry with respect to a function and a transformation. At first glance this definitions might be hard to picture, however with some group theory they will become more clear.

Let $$ G $$ be a group and let $$ X $$, $$ Y $$ be sets on which $$ G $$ acts. A function $$ f: X \rightarrow Y $$ is called equivariant with respect to $$ G $$ if it commutes with the group action. Equation \ref{eq:equi} expresses this notion formally.

$$
\begin{equation}\label{eq:equi}
f(g \cdot x)=g \cdot f(x)
\end{equation}
$$

Conversly, Equation \ref{eq:inv} shows that **invariance** is when the application of the transformation $$g \in G$$ does not affect the output of the map $$ f $$, 

$$
\begin{equation}\label{eq:inv}
f(g \cdot x)=f(x) 
\end{equation}
$$

---

# Higher-order networks and why topology is useful
Regular graph relations fall short of modelling multi-interacions, as such we turn to higher order networks. An *abstract simplicial complex* (ASC) is the combinatorial expression of a non-empty set of simplices. 

Concretly, let $$ \mathcal{P}(S) $$ be the powerset of $$ S $$ and another set $$ \mathcal{K} \subset \mathcal{P}(S) $$, then $$ \mathcal{K} $$ is an ASC if for every $$ X \in \mathcal{K} $$ and every non-empty $$ Y \subseteq X $$ it holds that $$ Y \in \mathcal{K} $$. Also, we define $$ \mid\mathcal{K}\mid $$ to be the highest cardinality of a simplex in an ASC minus 1. If the rank is $$r$$ then it holds $$ \forall X \in \mathcal{K}: r \geq \mid X \mid $$. 

<!-- 
## Geometric realization

Although an ASC is a purely combinatorial object, it always entails a **geometric realization**. For the case of $$r=1$$ then it can be represented as a graph. A **simplicial complex** is the geometric realization of an ASC, constructed out of the underlying geometric of the points in $$ \mathcal{K}$$. As such, they can be constructed using the set of verticies $$ V $$ of a graph as disjoint points in space or even taking a graph $$ G = (V, E) $$ where $$ V $$ is the set of 0-cells and $$ E $$ is the set of 1-cells. Transforming a set of points to a simplicial complex is called **lifting**. 
 
As an example, we may consider the clique lifting procedure. A *clique* $$ C \subseteq V $$, such that $$ C $$ is complete. In other words, there is an edge between every pair of vertices. In this lifting procedure each clique will become an r-cell and have it's own set of neighbours. Note that the time complexity of this lift is $$ \mathcal{O}(3^{n/3}) $$
-->


## Higher-order neighbourhoods
To define proximite relations such as graph adjacencies in $$r$$-simplex we establish some definitions. We will work with only two types of adjacencies as they have proven to be as expressive as using all of them. First, let $$ \sigma$$ and $$ \tau $$ be two simplices, we say that $$\sigma \text{ is on the bound of } \tau $$ as $$ \sigma \prec \tau $$ and: 1) $$ \sigma \subset \tau$$, 2) $$ \nexists \delta: \sigma  \subset \delta \subset \tau $$  

Equation \ref{eq:bound_adj} referes to the relation between a $$r$$-simplex and the $$(r-1)$$-simplex that compose it. Equation \ref{eq:bound_up} referes to the relationship between  $$(r-1)$$-simplex and other $$(r-1)$$-simplex that are a part of a higher $$r$$-simplex. They are also refered to as **cofaces** in the literature <d-cite key="hajij2022topological"></d-cite>.

$$ 
\begin{equation}\label{eq:bound_adj}
\mathcal{B}(\sigma) = \{\tau \mid \tau \prec \sigma\}
\end{equation}
$$

$$

\begin{equation}\label{eq:bound_up}
\mathcal{N}_{\uparrow}(\sigma) = \{\tau \mid \exists \delta, \tau \prec \delta \land \sigma \prec \delta\}
\end{equation}
$$

--- 
# Lifting techniques
How a higher-order representation of points in the space or a particular graph is to be constructed  depends on the properties that want to be attained and, as always, how efficient is to compute. Let's go over some alternatives.

## Vietoris-Ripps Complex
The Vietoris-Ripps complex is a common way to form a topological space. The time complexity for generating of the procedure depends on the maximum dimension of the simplices in the complex $$ r $$ and number of points $$ n $$  given by $$ \mathcal{O}(n^{r+1}) $$. If the points are embedded in Euclidean space then it is an approximation of a larger and richer complex called the *Cech Complex*. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/rips-lift.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>


## Alpha Complex 
The alpha complex is a fundamental data structure from computational geometry given by a subset $$ \sigma = \{x_{i0},...,x_{ik} \}\subset S $$ belongs to $$ \operatorname{Alpha}(S,r) $$ if there exists a point $$ y \in \mathbb{R}^m $$ that is equidistant from every member of $$ \sigma $$, so that 

$$
\begin{align*}
    \rho := || y- x_{i0}||=...=||y-x_{ik}|| \leq r
\end{align*}
$$
and thus $$||y-x|| \leq r\ \ \  \forall x \in S$$. 
Formally, the alpha complex is defined as the collection:

$$
\begin{align*}
    \operatorname{Alpha}(S, r)=\left\{\sigma \subset S: \bigcap_{x \in \sigma} V_x(r) \neq \emptyset\right\}
\end{align*}
$$

A subset $$ \sigma $$ of size $$ k+1 $$ is called a k-dimensional simplex of $$\operatorname{Alpha}(S,r)$$.

--- 

# Do invariances hold ?

 Equation \ref{eq:msg_eq} comes to replace \ref{compute_message} with our invariant function. Addtionally, to make the network equivariant we introduce feature vector  $$ x $$ which contains the positional coordinates in euclidean space. Equation \ref{eq:pos_update} refers to the update in the position embedding of the node. The proofs that with this condition equivariance holds can be found in <d-cite key="satorras2021n"></d-cite>.

$$
\begin{equation}\label{eq:msg_eq}
 \mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \operatorname{lnv}\left(\mathbf{x}_i^l, \mathbf{x}_j^l\right), \mathbf{a}_{i j}\right)
 \end{equation}
 $$


$$
\begin{equation}\label{eq:pos_update}
    \mathbf{x}_i^{l+1}=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right)
\end{equation} 
$$

## In simplices too ?

Using the previous definitions of neighbourhoods <d-cite key="eijkelboom2023n"></d-cite> defines a message for each neighbourhood as Equation \ref{eq:msg_boundary} and Equation \ref{eq:msg_ua} and replaces the hidden representation update to take these messages into account in Equation \ref{eq:update_sc}. 

$$
\begin{equation}\label{eq:msg_boundary}
m_{\mathcal{B}}(\sigma) = \underset{\tau \in \mathcal{B}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{B}}(h^l_{\sigma}, h^l_{\tau})
\end{equation}
$$

$$
\begin{equation}\label{eq:msg_ua}
m_{\mathcal{N}_{\uparrow}}(\sigma) = \underset{\tau \in \mathcal{N}_{\uparrow}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{N}_{\uparrow}}(h^l_{\sigma}, h^l_{\tau}))
\end{equation}
$$

$$
\begin{equation}\label{eq:update_sc}
h_{\sigma}^{l+1} = \phi_{h} (h_{\sigma}^l, m_{\mathcal{B}}(\sigma), m_{\mathcal{N}_{\uparrow}}(\sigma))
\end{equation}
$$

Finally, they define a graph embedding as Equation \ref{eq:agg_simp} where the simplices $$ \mathcal{K} $$ of each dimension $$ r $$ will be aggregated and the final embedding of the complex will be the concatenation of the embedding of each dimension.


$$
\begin{equation}\label{eq:agg_simp}

h_{\mathcal{K}} = \bigoplus_{i=0}^{r} \underset{\sigma \in \mathcal{K}, |\sigma|=i+1}{\operatorname{Agg}} h_\sigma
\end{equation}
$$

---
# The new standard: TopoX

TopoX is a suite of Python packages that aim to provide a standard for developments in TDL. It encompases TopoModelX, TopoNetX, TopoEmbeddX and now TopoBenchmarX. Each of them having functionalities according to their name in the topological domain. Next, we ilustrate the development process and reproduction of <d-cite key="eijkelboom2023n"></d-cite> in the TopoX suite. Additionally, as a base project we use the [ICML TDL Challenge 2024](https://github.com/pyt-team/challenge-icml-2024) for development, which structures all these packages together to be used in the development cycle. Additionally, we use the Pytorch Geometric QM9 dataset as it contains graph data.

## Building structure

The first thing we are concerned with is the **lifting** of our initial graph or set of points. To perform that task we will make use of GHUDI <d-cite key="gudhi:urm"></d-cite>, a Python library with many methods mainly used for Topological Data Analysis. Additionally, we add the option to fully connect all the $$0$$-simplex.

{% highlight python %}
def rips_lift(graph: torch_geometric.data.Data, dim: int, dis: float, fc_nodes: bool = True) -> SimplicialComplex:
    x_0, pos = graph.x, graph.pos

    points = [pos[i].tolist() for i in range(pos.shape[0])]

    rips_complex = gudhi.RipsComplex(points=points, max_edge_length=dis)
    simplex_tree: SimplexTree  = rips_complex.create_simplex_tree(max_dimension=dim)

    if fc_nodes:
        nodes = [i for i in range(x_0.shape[0])]
        for edge in combinations(nodes, 2):
            simplex_tree.insert(edge)

    return SimplicialComplex.from_gudhi(simplex_tree)

{% endhighlight %}

Now we will use the ```Graph2SimplicialLifting``` base class and override the lifting methods such that we transform an input ```pytorch_geometric.data.Data```. 

We override the init to include the ```delta``` parameter that defines the *range* of the Vietoris-Ripps lift.
{% highlight python %}

def __init__(self, delta: float = 0, **kwargs):
        super().__init__(**kwargs)
        self.delta = delta

{% endhighlight %}

Now, we override the main method called ```lifted_topology```. 

{% highlight python %}
 def lift_topology(self, data: torch_geometric.data.Data) -> dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)
{% endhighlight %}

Then, we move on to ```_lifted_topology``` where the object is built. The ```get_complex_connectivity``` is provided by the library and constructs the connectivity matrices. 

{% highlight python %}
    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -> dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'adjacency_{r}'] = lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()
            lifted_topology[f'incidence_{r}'] = lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()


        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'x_idx_{r}'] = torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology

{% endhighlight %}

Now we have to do a quick detour to talk about **mini-batching**

## Mini-batching detour

To be brief in PyG a dataset compromising graphs is concatenated into one big graph that is represented by a block-diagonal matrix. There is a detailed explanation [here](https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html). 

{% highlight python %}
class SimplexData(torch_geometric.data.Data):
    def __inc__(self, key: str, value, *args, **kwargs):
        if 'adjacency' in key:
            rank = int(key.split('_')[-1])
            return torch.tensor([getattr(self, f'x_{rank}').size(0)])
            #return torch.tensor([[getattr(self, f'x_{rank}').size(0)], [getattr(self, f'x_{rank}').size(0)]])
        elif 'incidence' in key:
            rank = int(key.split('_')[-1])
            if rank == 0:
                return torch.tensor([getattr(self, f'x_{rank}').size(0)])
            return torch.tensor([[getattr(self, f'x_{rank-1}').size(0)], [getattr(self, f'x_{rank}').size(0)]])
        elif key == 'x_0' or key == 'x_idx_0':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif key == 'x_1' or key == 'x_idx_1':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif key == 'x_2' or key == 'x_idx_2':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif 'index' in key:
            return self.num_nodes
        else:
            return super().__inc__(key, value, *args, **kwargs)

    def __cat_dim__(self, key: str, value, *args, **kwargs):
        if 'adjacency' in key or 'incidence' in key or 'index' in key:
            return 1
        else:
            return 0

{% endhighlight %}


## Piecing it all together

{% highlight python %}
class SimplicialVietorisRipsLifting(Graph2SimplicialLifting):
    def __init__(self, delta: float = 0, **kwargs):
        super().__init__(**kwargs)
        self.delta = delta

    def lift_topology(self, data: torch_geometric.data.Data) -> dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)

    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -> dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'adjacency_{r}'] = lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()
            lifted_topology[f'incidence_{r}'] = lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'x_idx_{r}'] = torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology
   def forward(self, data: torch_geometric.data.Data) -> torch_geometric.data.Data:
        initial_data = data.to_dict()
        lifted_topology = self.lift_topology(data)
        lifted_topology = self.feature_lifting(lifted_topology)
        return SimplexData(**initial_data, **lifted_topology)

{% endhighlight %}

The only thing missing is to define our prefered ```feature_lifting```.

## Giving meaning to structure

Now that we have a higher-order topological space, there is only one thing we are missing. *What should the embeddings of the $$r$$-simplex higher than $$0$$ be ?*

Authors in <d-cite key="eijkelboom2023n"></d-cite> perform an element-wise mean of the components of lower $$r$$-simplex, however other alternative are also experimentally tried. The following definition comes in place of the ```lift_features``` as can be found in the example ```ProjectionSum```.

{% highlight python %}

def lift_features(
        self, data: torch_geometric.data.Data | dict
    ) -> torch_geometric.data.Data | dict:
        max_dim = max([int(key.split("_")[1]) for key in data.keys() if "incidence" in key])


        simplex_test_dict = defaultdict(list)
        for i in range(max_dim+1):
            z = []
            # For the k-component point in the i-simplices where k <= i
            for k in range(i+1):
                # Get the k-node indices of the i-simplices (i.e. the k-components of the i-simplices)
                z_i_idx = data[f'x_idx_{i}'][:, k]
                # Get the node embeddings for the k-components of the i-simplices
                z_i = data['x_0'][z_i_idx]
                z.append(z_i)
            z = torch.stack(z, dim=2)
            # Mean along the simplex dimension
            z = z.mean(axis=2)
            # Assign to each i-simplex the corresponding feature
            for l, embedding in enumerate(z):
                simplex_test_dict[i].append(z[l])

        for k, v in simplex_test_dict.items():
            data[f'x_{k}'] = torch.stack(v)
        return data

{% endhighlight %}
Below is a visualization of the embedding process. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/simplex_features.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>



## The network
An overall image is presented here

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/arch.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>


### Defining a layer

We are using the definitions we stated previously and setting a convolutional kernel for the two types of communication: 1) $$r$$-simplex to $$r$$-simplex and $$(r-1)$$-simplex to $$r$$-simplex. They will come to be named ```convs_same_rank``` and ```convs_low_to_high```.

{% highlight python %}
def __init__(
        self,
        channels,
        max_rank,
        n_inv: Dict[int, Dict[int, int]], # Number of invariances from r-simplex to r-simplex
        aggr_func: Literal["mean", "sum"] = "sum",
        update_func: Literal["relu", "sigmoid", "tanh", "silu"] | None = "sigmoid",
        aggr_update_func: Literal["relu", "sigmoid", "tanh", "silu"] | None = "sigmoid"
    ) -> None:
        super().__init__()
        self.channels = channels
        self.max_rank = max_rank
        
        # convolutions within the same rank
        self.convs_same_rank = torch.nn.ModuleDict(
            {
                f"rank_{rank}": 
                    EConv(
                        in_channels=channels,
                        weight_channels=n_inv[f"rank_{rank}"][f"rank_{rank}"], #from r-simplex to r-simplex
                        out_channels=1,
                        with_linear_transform_1=True,
                        with_linear_transform_2=True,
                        with_weighted_message=True,
                        update_func=update_func,
                    )

                for rank in range(max_rank) # Same rank conv up to r-1
            }
        )

        # convolutions from lower to higher rank
        self.convs_low_to_high = torch.nn.ModuleDict(
            {
                f"rank_{rank}": 
                    EConv(
                        in_channels=channels,
                        weight_channels=n_inv[f"rank_{rank-1}"][f"rank_{rank}"], #from r-1-simplex to r-simplex
                        out_channels=1,
                        with_linear_transform_1=True,
                        with_linear_transform_2=True,
                        with_weighted_message=True,
                        update_func=update_func,
                    )

                for rank in range(1, max_rank+1)
            }
        )
        # aggregation functions
        self.scatter_aggregations = torch.nn.ModuleDict(
            {
                f"rank_{rank}": ScatterAggregation(
                    aggr_func=aggr_func, 
                    update_func=aggr_update_func
                )
                for rank in range(max_rank + 1)
            }
        )
{% endhighlight %}

Finally, we define $$\phi_m$$ which will be the MLP over the message update.

{% highlight python %}
        # Perform an update over the received
        # messages by each other layer
        self.update = torch.nn.ModuleDict()
        for rank in range(max_rank+1):
            factor = 1
            for target_dict in n_inv.values():
                for target_rank in target_dict.keys():
                    factor += int(target_rank[-1] == str(rank))
            self.update[f"rank_{rank}"] = nn.Sequential(
                nn.Linear(factor*channels, channels),
                nn.SiLU(),
                nn.Linear(channels, channels)
                )

{% endhighlight %}


Now, on the forward pass. We will receive information of features, incidences, adjacencies and the invariances for both of these relationships as well. The steps are the following

+ Same rank convolutions, where we build a message
+ Low to high rank convolutions
+ Concat the embeddings for the $$r$$-simplex that have received messages from different hierarchies
+ Pass the embeddings over $$\phi_m$$
+ Add residual connections

{% highlight python %}
def forward(self, features, adjacencies, incidences, invariances_r_r, invariances_r_r_minus_1) -> Dict[int, Tensor]:
        for rank, feature in features.items():
            assert(not torch.isnan(feature).any())
            assert(not torch.isinf(feature).any())
        aggregation_dict = {} 

        h = {}

        # Same rank convolutions
        for rank in range(self.max_rank):
            # Get the convolution operation for the same rank
            conv = self.convs_same_rank[f"rank_{rank}"]

            rank_str = f"rank_{rank}"
            x_source = features[rank_str]
            x_target = features[rank_str]

            edge_index = adjacencies[rank_str]
            x_weights = invariances_r_r[rank_str]

            # print(rank, x_source.size(), x_target.size(), x_weights.size(), edge_index.size())

            send_idx, recv_idx = edge_index

            # Run the convolution
            message = conv(x_source, edge_index, x_weights, x_target)

            assert(message.size(0) == recv_idx.size(0))
            assert(not torch.isnan(message).any())
            assert(not torch.isinf(message).any())

            message_aggr = self.scatter_aggregations[rank_str](message, recv_idx, dim=0, target_dim=x_target.shape[0])

            aggregation_dict[rank_str] = {
                "message": [message_aggr],
                "recv_idx": recv_idx,
            }

        # Low to high convolutions starting from rank 1 and looking
        # backward to convolute
        for rank in range(1, self.max_rank+1):
            conv = self.convs_low_to_high[f"rank_{rank}"]

            rank_str = f"rank_{rank}"
            rank_minus_1_str = f"rank_{rank-1}"

            x_source = features[rank_minus_1_str]
            x_target = features[rank_str]
            x_weights = invariances_r_r_minus_1[rank_str]
            edge_index = incidences[rank_str]

            send_idx, recv_idx = edge_index

            # Run the convolution
            message = conv(x_source, edge_index, x_weights, x_target)
            message_aggr = self.scatter_aggregations[rank_str](message, recv_idx, dim=0, target_dim=x_target.shape[0])

            assert(message.size(0) == recv_idx.size(0))

            # Aggregate the message
            if rank_str not in aggregation_dict:
                aggregation_dict[rank_str] = {
                    "message": [message_aggr],
                    "recv_idx": recv_idx,
                }
            else:
                aggregation_dict[rank_str]['message'].append(message_aggr) 
                '''
                aggregation_dict[rank_str] = {
                    "message": [prev_msg, message],
                    #"recv_idx": torch.cat((aggregation_dict[rank]["recv_idx"], recv_idx)),
                    for prev_msg in aggregation_dict[rank_str]["message"]
                }
                '''

        for rank in range(self.max_rank+1):
            # Check for ranks not receiving any messages
            rank_str = f"rank_{rank}"
            if rank_str not in aggregation_dict:
                continue
            message = aggregation_dict[rank_str]["message"]
            recv_idx = aggregation_dict[rank_str]["recv_idx"]
            x_target = features[rank_str]

            # Aggregate the message
            #h[rank_str] = self.scatter_aggregations[rank_str](message, recv_idx, dim=0, target_dim=x_target.shape[0])

        # Update over the final embeddings with another MLP

        h = {}
        for rank, feature in features.items():
            feat_list = [feature]
            for msg_i in aggregation_dict[rank]["message"]:
                feat_list.append(msg_i)
            h[rank] = torch.cat(feat_list, dim=1)

        h = {
            rank: self.update[rank](feature)
            for rank, feature in h.items()
        }

        # Residual connection
        x = {rank: feature + h[rank] for rank, feature in features.items()}

        return x
{% endhighlight %}


### All together

Piecing it all together, our modules look like this.

{% highlight python %}
def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, n_layers: int, max_dim: int, inv_dims: Dict) -> None: 
        super().__init__()

        self.max_dim = max_dim
        self.feature_embedding = nn.Linear(in_channels, hidden_channels)

        self.layers = nn.ModuleList(
            [EMPSNLayer(hidden_channels, self.max_dim, aggr_func="sum", update_func="silu", aggr_update_func=None, n_inv=inv_dims) for _ in range(n_layers)]
        )

        # Pre-pooling operation
        self.pre_pool = nn.ModuleDict()
        for rank in range(self.max_dim+1):
            self.pre_pool[f"rank_{rank}"]= nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels),
                nn.SiLU(),
                nn.Linear(hidden_channels, hidden_channels)
            )

        # Post-pooling operation over all dimensions
        # and final classification
        self.post_pool = nn.Sequential(
            nn.Linear((max_dim + 1) * hidden_channels, hidden_channels),
            nn.SiLU(),
            nn.Linear(hidden_channels, out_channels)
        )
{% endhighlight %}


In the first line we call ```decompose_batch``` which grabs the batch object and returns the features, the adjacencies and also calculates the invariances for a given batch. Then we proceed running through the network.

{% highlight python %}
def forward(self, batch)-> Tensor:
        features, edge_index_adj, edge_index_inc, inv_r_r, inv_r_minus_1_r, x_batch = decompose_batch(batch, self.max_dim)

        x = {
            rank: self.feature_embedding(feature) 
            for rank, feature in features.items()
        }
        for layer in self.layers:
            x = layer(x, edge_index_adj, edge_index_inc, inv_r_r, inv_r_minus_1_r)

        # read out
        x = {rank: self.pre_pool[rank](feature) for rank, feature in x.items()}
        x = {rank: global_add_pool(x[rank], batch=x_batch[rank]) for rank, feature in x.items()}
        state = torch.cat(tuple([feature for rank, feature in x.items()]), dim=1)
        out = self.post_pool(state) # Classifier across 19 variables 
        out = torch.squeeze(out)

        return out

{% endhighlight %}

# Experiments

We performed the experiments using both the Alpha Complex Lifting and the Vietoris-Rips Complex Lifting. The figure below show the MAE for validation and training on a random subsample of 1000 molecules. The figure below shows that the overall performance of the Alpha Complex is lower that the Vietoris-Rips complex for this task. We assume this is likely due to the fact that Alpha complex creates fewer (higher order) simplices, which might lead to a worse outcome, especially on this dataset.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/train_mae_topox.jpg" class="img-fluid rounded z-depth-1"%}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/validation_mae_topox.jpg" class="img-fluid rounded z-depth-1"%}
    </div>
</div>



# Conclusions


In this post, we have investigated the novel development suite for Topological Deep Learning and how it can be used to tackle a particular problem. We go over concepts in **geometric deep learning** and show why they work and how can we leverage topological representations to better learn in message passing networks. The usage of the unified TopoX framework allows for ease of development and standarization in regards of the reproducibility. As this framework grows, ease of development in TDL should follow. Additionally, we the work of <d-cite key="eijkelboom2023n"></d-cite> and explore the performance on the Alpha Complex. Given the computing limitations for executing these models, our experiments fall short of complete. However, it remains useful to further explore ways in which to optimize this models, given the pontential expressivity they achieve.

