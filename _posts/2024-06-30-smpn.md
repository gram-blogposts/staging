---
layout: distill
title: Applications of TopoX to Topological Deep Learning
description: Studying the properties of message passing accross TNNs using the TopoX Suite
tags: TDL
giscus_comments: true
date: 2024-06-30
featured: true
feature: true


authors:
  - name: Anonymous

bibliography: 2024-06-30-smpn.bib
toc:
  - name: Introduction 
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Message Passing in GNNs
  - name: Equivariance and Invariance
  - name: Higher-order networks and why topology is useful
  - subsections:
    - name: Geometric Realization
    - name: Higher-order neighbourhoods
  - name: Lifting techniques
  - subsections: 
    - name: Vietoris-Rips Complex
    - name: Alpha Complex
  - name: Do invariances hold ?
  - subsections:
    - name: "In simplices too ?"
  - name: "The new standard: TopoX"
  - subsections:
    - name: Building structure
    - name: Piecing it all together
  - name: Experiments
  - subsections:
    - name: Lifting times
    - name: File size
    - name: Forward pass
  - name: Conclusions

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
---
# Introduction

Representation learning using Graph Neural Networks (GNNs) is rapidly growing approach to complex tasks in chemistry <d-cite key="ballester2024attending,bekkers2023fast,eijkelboom2023n,battiloro2024n"></d-cite>. Particularly, in a subset of these tasks a crucial aspect is maintaining equivariance to different transformations such as *translation*, *rotation* and *reflection*. Learning representations such that **equivariance** or **invariance** can be applied has proved very helpful <d-cite key="bekkers2023fast,eijkelboom2023n"></d-cite>. Additionally, incorporating higher-order relations in GNNs such that they encode more complex topological spaces is a recent effort to increase the expressivity of GNNs <d-cite key="hajij2022topological,eijkelboom2023n,giusti2024topological"></d-cite>.


The aim of this blogpost is to draw attention to Topological Deep Learning (TDL) by using the suite of Python packages TopoX <d-cite key="hajij2024topox"></d-cite> to replicate the work of <d-cite key='eijkelboom2023n'></d-cite> and show how much simpler development is in this framework. Additionally, we experiment with a different topological spaces with more geometric information and compare the results with the original work.


<!--
 We will introduce the concepts needed to address **equivariance** and **invariance** as well as definitions of what exactly constitute this *higher-order* structures. Then, we will introduce TopoX and the benefits of development in this framework, show some examples and present the results. Our results show that development in this platform is beneficial for the investigator conducting research as well as the scientific community. New architectures and experiments developed using TopoX allow a standardaized way to share this among people interested in TDL.
 -->
---

# Message passing in GNNs 
Let $$G = (V,E)$$ be a graph consisting of nodes $$V$$ and edges $$E$$. Then let each node $$ v_i \in V$$ and edge $$e_{ij} \in E$$ have an associated node feature $$\mathbf{f}_i \in \mathbb{R}^{c_n} $$ and edge feature $$ a_{ij} \in \mathbb{R}^{c_e} $$, with dimensionality $$ c_n, c_e \in \mathbb{N}_{>0} $$. Then, we define a *message passing layer* as:

$$
\begin{equation}\label{compute_message}
\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \mathbf{a}_{i j}\right)
\end{equation}
$$

$$
\begin{equation}\label{aggregate_messages}
    \mathbf{m}_i=\underset{j \in \mathcal{N}(i)}{\operatorname{Agg}} \mathbf{m}_{i j}
\end{equation}
$$
$$
\begin{equation}\label{update_hidden}
    \mathbf{h}_i^{l+1}=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{equation}
$$

---

# Equivariance and Invariance

**Invariance** is when an object or set of objects remain the same after a transformation. In contrast, **equivariance** is a symmetry with respect to a function and a transformation. At first glance this definitions might be hard to picture, however with some group theory they will become more clear.

Let $$ G $$ be a group and let $$ X $$, $$ Y $$ be sets on which $$ G $$ acts. A function $$ f: X \rightarrow Y $$ is called equivariant with respect to $$ G $$ if it commutes with the group action. Equation \ref{eq:equi} expresses this notion formally.

$$
\begin{equation}\label{eq:equi}
f(g \cdot x)=g \cdot f(x)
\end{equation}
$$

Conversly, Equation \ref{eq:inv} shows that **invariance** is when the application of the transformation $$g \in G$$ does not affect the output of the map $$ f $$, 

$$
\begin{equation}\label{eq:inv}
f(g \cdot x)=f(x) 
\end{equation}
$$

---

# Higher-order networks and why topology is useful
Regular graph relations fall short of modelling multi-interacions, as such we turn to higher order networks. An *abstract simplicial complex* (ASC) is the combinatorial expression of a non-empty set of simplices. 

Concretly, let $$ \mathcal{P}(S) $$ be the powerset of $$ S $$ and another set $$ \mathcal{K} \subset \mathcal{P}(S) $$, then $$ \mathcal{K} $$ is an ASC if for every $$ X \in \mathcal{K} $$ and every non-empty $$ Y \subseteq X $$ it holds that $$ Y \in \mathcal{K} $$. Also, we define $$ \mid\mathcal{K}\mid $$ to be the highest cardinality of a simplex in an ASC minus 1. If the rank is $$r$$ then it holds $$ \forall X \in \mathcal{K}: r \geq \mid X \mid $$. 

## Geometric realization

Although an ASC is a purely combinatorial object, it always entails a **geometric realization**. For the case of $$r=1$$ then it can be represented as a graph. A **simplicial complex** is the geometric realization of an ASC, constructed out of the underlying geometric of the points in $$ \mathcal{K}$$. As such, they can be constructed using the set of verticies $$ V $$ of a graph as disjoint points in space or even taking a graph $$ G = (V, E) $$ where $$ V $$ is the set of 0-cells and $$ E $$ is the set of 1-cells. Transforming a set of points to a simplicial complex is called **lifting**. 
 
As an example, we may consider the clique lifting procedure. A *clique* $$ C \subseteq V $$, such that $$ C $$ is complete. In other words, there is an edge between every pair of vertices. In this lifting procedure each clique will become an r-cell and have it's own set of neighbours. Note that the time complexity of this lift is $$ \mathcal{O}(3^{n/3}) $$

## Higher-order neighbourhoods
To define proximite relations such as graph adjacencies in $$r$$-simplex we establish some definitions. We will work with only two types of adjacencies as they have proven to be as expressive as using all of them. First, let $$ \sigma$$ and $$ \tau $$ be two simplices, we say that $$\sigma \text{ is on the bound of } \tau $$ as $$ \sigma \prec \tau $$ and: 1) $$ \sigma \subset \tau$$, 2) $$ \nexists \delta: \sigma  \subset \delta \subset \tau $$  

Equation \ref{eq:bound_adj} referes to the relation between a $$r$$-simplex and the $$(r-1)$$-simplex that compose it. Equation \ref{eq:bound_up} referes to the relationship between  $$(r-1)$$-simplex and other $$(r-1)$$-simplex that are a part of a higher $$r$$-simplex. They are also refered to as **cofaces** in the literature <d-cite key="hajij2022topological"></d-cite>.

$$ 
\begin{equation}\label{eq:bound_adj}
\mathcal{B}(\sigma) = \{\tau \mid \tau \prec \sigma\}
\end{equation}
$$

$$

\begin{equation}\label{eq:bound_up}
\mathcal{N}_{\uparrow}(\sigma) = \{\tau \mid \exists \delta, \tau \prec \delta \land \sigma \prec \delta\}
\end{equation}
$$

--- 
# Lifting techniques
How a higher-order representation of points in the space or a particular graph is to be constructed  depends on the properties that want to be attained and, as always, how efficient is to compute. Let's go over some alternatives.

## Vietoris-Ripps Complex
The Vietoris-Ripps complex is a common way to form a topological space. The time complexity for generating of the procedure depends on the maximum dimension of the simplices in the complex $$ r $$ and number of points $$ n $$  given by $$ \mathcal{O}(n^{r+1}) $$. If the points are embedded in Euclidean space then it is an approximation of a larger and richer complex called the *Cech Complex*. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/rips-lift.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>


## Alpha Complex 
The alpha complex is a fundamental data structure from computational geometry given by a subset $$ \sigma = \{x_{i0},...,x_{ik} \}\subset S $$ belongs to $$ \operatorname{Alpha}(S,r) $$ if there exists a point $$ y \in \mathbb{R}^m $$ that is equidistant from every member of $$ \sigma $$, so that 

$$
\begin{align*}
    \rho := || y- x_{i0}||=...=||y-x_{ik}|| \leq r
\end{align*}
$$
and thus $$||y-x|| \leq r\ \ \  \forall x \in S$$. 
Formally, the alpha complex is defined as the collection:

$$
\begin{align*}
    \operatorname{Alpha}(S, r)=\left\{\sigma \subset S: \bigcap_{x \in \sigma} V_x(r) \neq \emptyset\right\}
\end{align*}
$$

A subset $$ \sigma $$ of size $$ k+1 $$ is called a k-dimensional simplex of $$\operatorname{Alpha}(S,r)$$.

--- 

# Do invariances hold ?

 Equation \ref{eq:msg_eq} comes to replace \ref{compute_message} with our invariant function. Addtionally, to make the network equivariant we introduce feature vector  $$ x $$ which contains the positional coordinates in euclidean space. Equation \ref{eq:pos_update} refers to the update in the position embedding of the node. The proofs that with this condition equivariance holds can be found in <d-cite key="satorras2021n"></d-cite>.

$$
\begin{equation}\label{eq:msg_eq}
 \mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \operatorname{lnv}\left(\mathbf{x}_i^l, \mathbf{x}_j^l\right), \mathbf{a}_{i j}\right)
 \end{equation}
 $$


$$
\begin{equation}\label{eq:pos_update}
    \mathbf{x}_i^{l+1}=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right)
\end{equation} 
$$

## In simplices too ?

Using the previous definitions of neighbourhoods <d-cite key="eijkelboom2023n"></d-cite> defines a message for each neighbourhood as Equation \ref{eq:msg_boundary} and Equation \ref{eq:msg_ua} and replaces the hidden representation update to take these messages into account in Equation \ref{eq:update_sc}. 

$$
\begin{equation}\label{eq:msg_boundary}
m_{\mathcal{B}}(\sigma) = \underset{\tau \in \mathcal{B}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{B}}(h^l_{\sigma}, h^l_{\tau})
\end{equation}
$$

$$
\begin{equation}\label{eq:msg_ua}
m_{\mathcal{N}_{\uparrow}}(\sigma) = \underset{\tau \in \mathcal{N}_{\uparrow}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{N}_{\uparrow}}(h^l_{\sigma}, h^l_{\tau}))
\end{equation}
$$

$$
\begin{equation}\label{eq:update_sc}
h_{\sigma}^{l+1} = \phi_{h} (h_{\sigma}^l, m_{\mathcal{B}}(\sigma), m_{\mathcal{N}_{\uparrow}}(\sigma))
\end{equation}
$$

Finally, they define a graph embedding as Equation \ref{eq:agg_simp} where the simplices $$ \mathcal{K} $$ of each dimension $$ r $$ will be aggregated and the final embedding of the complex will be the concatenation of the embedding of each dimension.


$$
\begin{equation}\label{eq:agg_simp}

h_{\mathcal{K}} = \bigoplus_{i=0}^{r} \underset{\sigma \in \mathcal{K}, |\sigma|=i+1}{\operatorname{Agg}} h_\sigma
\end{equation}
$$

---

# The new standard: TopoX

TopoX is a suite of Python packages that aim to provide a standard for developments in TDL. It encompases TopoModelX, TopoNetX, TopoEmbeddX and now TopoBenchmarX. Each of them having functionalities according to their name in the topological domain. Next, we ilustrate the development process and reproduction of <d-cite key="eijkelboom2023n"></d-cite> in the TopoX suite. Additionally, as a base project we use the [ICML TDL Challenge 2024](https://github.com/pyt-team/challenge-icml-2024) for development, which structures all these packages together to be used in the development cycle. Additionally, we use the Pytorch Geometric QM9 dataset as it contains graph data.

## Building structure

The first thing we are concerned with is the **lifting** of our initial graph or set of points. To perform that task we will make use of GHUDI <d-cite key="gudhi:urm"></d-cite>, a Python library with many methods mainly used for Topological Data Analysis. Additionally, we add the option to fully connect all the $$0$$-simplex per the implementation of <d-cite key="eijkelboom2023n"></d-cite>

{% highlight python %}
def rips_lift(graph: torch_geometric.data.Data, dim: int, dis: float, fc_nodes: bool = True) -> SimplicialComplex:
    x_0, pos = graph.x, graph.pos

    points = [pos[i].tolist() for i in range(pos.shape[0])]

    rips_complex = gudhi.RipsComplex(points=points, max_edge_length=dis)
    simplex_tree: SimplexTree  = rips_complex.create_simplex_tree(max_dimension=dim)

    if fc_nodes:
        nodes = [i for i in range(x_0.shape[0])]
        for edge in combinations(nodes, 2):
            simplex_tree.insert(edge)

    return SimplicialComplex.from_gudhi(simplex_tree)

{% endhighlight %}

Now we will use the ```Graph2SimplicialLifting``` base class and override the lifting methods such that we transform an input ```pytorch_geometric.data.Data```. 

We override the init to include the ```delta``` parameter that defines the *range* of the Vietoris-Ripps lift.
{% highlight python %}

def __init__(self, delta: float = 0, **kwargs):
        super().__init__(**kwargs)
        self.delta = delta

{% endhighlight %}

Now, we override the main method called ```lifted_topology```. 

{% highlight python %}
 def lift_topology(self, data: torch_geometric.data.Data) -> dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)
{% endhighlight %}

Then, we move on to ```_lifted_topology``` where the object is built. The ```get_complex_connectivity``` is provided by the library and constructs the connectivity matrices. 

{% highlight python %}
    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -> dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'adjacency_{r}'] = lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()
            lifted_topology[f'incidence_{r}'] = lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()


        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'x_idx_{r}'] = torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology

{% endhighlight %}

Note that we transform the **adjacency** and **incidence** matricies to their *edge_index* form by using `nonzero().t().contigous()`. This is to be able to perform mini-batching during training.


## Giving meaning to structure

Now that we have a higher-order topological space, there is only one thing we are missing. *What should the embeddings of the $$r$$-simplex higher than $$0$$ be ?*

There is still the question of what a good feature lifting technique constitues and what information should it take from the underlying representation. Authors in <d-cite key="eijkelboom2023n"></d-cite> perform an element-wise mean of the components of lower $$r$$-simplex, however other alternative are also experimentally tried. As an example, take a simplex $$\sigma = \{a, b, c\}$$ where $$a,b,c \in R^d$$ and denote the feature embedding of $$\sigma$$ with $$\sigma_f$$. Then, given that simplices on the boundry are given by the powerset such that $$\tau_1 = \{a, b\}, \tau_2=\{b, c\}, \tau_3=\{a, c\}$$ and $$\forall i \in \{1, 2, 3\} : \tau_i \prec \sigma $$ and $$\delta_1=\{a\}, \delta_2=\{b\}$$ we have that $${\tau_1}_f = mean({\delta_1}_f, {\delta_2}_f)$$ and $$\sigma_f = mean({\tau_1}_f, {\tau_2}_f, {\tau_3}_f)$$. More generally, 

$$ \sigma_f = \{  mean({\tau_1}_f, {\tau_2}_f, \cdot \cdot \cdot, {\tau_i}_f ) \in \tau \; | \; \tau \prec \sigma \wedge \nexists \delta:   \tau \prec \delta \prec \sigma \} $$


where given the definition of a simplex it can be show that a given $$\sigma$$ will have $$card(\sigma)$$ of size $$card(\sigma)-1$$ subsets, whose embeddings can be averaged down recursively to the lowest level of embeddings provided by the dataset. As part of our contribution, we leverage TopoX to make this calculations faster by vectorizing this part of this process.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/simplex_features.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>


---

# Experiments

We performed experiments with the implementation in the TopoX framework. Additionally, we vectorized and improved the following sections: 1) the lifting procedure now results in a smaller file size and the vectorization of the feature embeddings and adjancency/incidence matrix calculation with the help of TopoX is faster, 2) the computation of the invariants was organized and some opeartions were further vectorized. 

Next, we present the comparison of lifting times of the whole QM9 dataset, the size of the pre-processed file (after lifting) and the time it takes to run a forward pass. 


## Lifting times

The lifting procedure has two challenging areas which were optimized: 1) calculation of the adjacency and incidence relationships for which we rely on TopoX and 2) feature lifting which we manually optimzed. In total, we were able to bring down the lifting time from about 1.5 hours to about 28 minutes on the same hardware. Next we show the lifting time of each graph in the dataset and the total lifting time comparison. We see that our optimization are not singificantly faster on the heavier graphs, however there is a set that are reduce to the baseline.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/lift_time_exp.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/total_lift_time.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>

## File size
The size of the preprocessed dataset after the lifting also reduced in size. This is due to the way incidences and adjacencies are stored. Instead of having the invariance information directly, we can just store the relationships such as boundries and upper adjacencies and the embeddings which makes it enough. We could have also stored these as sparse tensors, however handling the mini-batching proved cumbersome on that representation. Ultimately, we reduced the size from 8.7G to 6.3G.


## Forward pass

One of the bottlenecks of these models is the amount of time they take to train for an epoch. The original version took close to 70 hours in the cluster we had access to. Some of this time is related to the amount of messages that are passed and some to the calculation of the invariances, something which has to be done each forward pass. We managed to optimize this calculation and thus speed up the execution of the model almost ten-fold. This results are calculated over the forward on a batch size of $$96$$, as per the original implementation and take into account 96 batches.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/total_forward_pass_time.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>

---

# Conclusions


In this post, we have investigated the novel development suite for Topological Deep Learning and how it can be used to tackle a particular problem. We go over concepts in **geometric deep learning** and show why they work and how can we leverage topological representations to better learn in message passing networks. The usage of the unified TopoX framework allows for ease of development and standarization in regards of the reproducibility. Additionally, optimization for computationally heavy procedures such as the ones inherent to TDL is more straightforward. As this framework grows, ease of development in TDL should follow. Additionally, we replicate the work of <d-cite key="eijkelboom2023n"></d-cite>. Based on our tests we were not able to reach the reported results, however we took the original repository and replicated the implementation in TopoX, thus we cannot know the particular configurations which achieve the presented results.

