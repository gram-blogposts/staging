---
layout: distill
title: E$(n)$ Equivariant Simplicial Message Passing Networks - A replication study
description: Studying the properties of message passing accross TNNs
tags: TDL
giscus_comments: true
date: 2024-06-30
featured: true
feature: true


authors:
  - name: Anonymous
# TODO Uncomment if accepted
# authors:
#   - name: Martin Carrasco
#   - name: Andreas
#   - name: Valeria
#   - name: Nordin
#   - name: Jesse

bibliography: 2024-06-30-smpn.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction 
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Message Passing in GNNs
  - name: Equivariance and Invariance
  - subsections:
    - name: Examples
  - name: Higher-order networks and why is topology useful
  - subsections:
    - name: Geometric realization
    - name: Higher-order neighbourhoods
  - name: Lifting techniques
  - subsections: 
    - name: Vietoris-Rips Complex
    - name: Alpha Complex
  - name: Do invariances hold ?
  - name: "The new standard: TopoX"
  - name: Expriments
  - name: Conclusions

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
# Introduction



Representation learning using Graph Neural Networks (GNNs) is rapidly growing approach to complex tasks in chemistry <d-cite key="ballester2024attending,bekkers2023fast,eijkelboom2023n,battiloro2024n"></d-cite>. Particularly, in a subset of these tasks a crucial aspect is maintaining equivariance to different transformations such as *translation*, *rotation* and *reflection*. Learning representations such that **equivariance** or **invariance** can be applied has proved very helpful <d-cite key="bekkers2023fast,eijkelboom2023n"></d-cite>. Additionally, incorporating higher-order relations in GNNs such that they encode more complex topological spaces is a recent effort to increase the expressivity of GNNs <d-cite key="hajij2022topological,eijkelboom2023n,giusti2024topological"></d-cite>.


The aim of this blogpost is to draw attention to Topological Deep Learning (TDL) by using the suite of Python packages TopoX <d-cite key="hajij2024topox"></d-cite> to replicate the work of <d-cite key='eijkelboom2023n'></d-cite> and show how much simpler development is in this framework. Additionally, we experiment with a different topological space with more geometric information and compare the results with the original work. We will introduce the concepts needed to address **equivariance** and **invariance** as well as definitions of what exactly constitute this *higher-order* structures. Then, we will introduce TopoX and the benefits of development in this framework, show some examples and present the results. Our results show that development in this platform is beneficial for the investigator conducting research as well as the scientific community. New architectures and experiments developed using TopoX allow a standardaized way to share this among people interested in TDL.

---

# Message passing in GNNs 
Let $$G = (V,E)$$ be a graph consisting of nodes $$V$$ and edges $$E$$. Then let each node $$ v_i \in V$$ and edge $$e_{ij} \in E$$ have an associated node feature $$\mathbf{f}_i \in \mathbb{R}^{c_n} $$ and edge feature $$ a_{ij} \in \mathbb{R}^{c_e} $$, with dimensionality $$ c_n, c_e \in \mathbb{N}_{>0} $$. Then, we define a *message passing layer* as:

$$
\begin{equation}\label{compute_message}
\mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \mathbf{a}_{i j}\right)
\end{equation}
$$

$$
\begin{equation}\label{aggregate_messages}
    \mathbf{m}_i=\underset{j \in \mathcal{N}(i)}{\operatorname{Agg}} \mathbf{m}_{i j}
\end{equation}
$$
$$
\begin{equation}\label{update_hidden}
    \mathbf{h}_i^{l+1}=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{equation}
$$

Where the first layer uses the initial feature as the previous hidden representation or $$ h_i^0 = l$$. First, a message is **passed** from $$ v_j $$ to $$ v_i $$ along it's respective edge attribute in Equation \ref{compute_message}. Then, Equation \ref{aggregate_messages} represents a permutation-invariant function $$\operatorname{Agg}$$ that aggregates the messages of all neighbours of $$ v_i $$ denoted $$ \mathcal{N}(i) $$. Finally, $$ h_i^{l+1} $$ takes the value of a function over the aggregated messages $$ m_i $$ and the current hidden representation $$ h_i^l $$. The functions $$ \phi_m $$ and $$\phi_f$$ are usually learned using MLPs.


---

# Equivariance and Invariance

Te consice, **invariance** is when an object or set of objects remain the same after a transformation. In contrast, **equivariance** is a symmetric with respect to a function and a transformation, namely that if the function is applied before the transformation has the same result as if the order is inverted. At first glance this definitions might be hard to picture, however with some group theory they will become more clear.

Let $$ G $$ be a group and let $$ X $$, $$ Y $$ be sets on which $$ G $$ acts. A function $$ f: X \rightarrow Y $$ is called equivariant with respect to $$ G $$ if it commutes with the group action. In other words, applying a transformation $$g \in G $$ followed by the function $$ f $$ yields the same result as first applying $$ f $$ and then the transformation. Equation \ref{eq:equi} expresses this notion formally.

$$
\begin{equation}\label{eq:equi}
f(g \cdot x)=g \cdot f(x)
\end{equation}
$$

Conversly, Equation \ref{eq:inv} shows that **invariance** is when the application of the transformation $$g \in G$$ does not affect the output of the map $$ f $$, 

$$
\begin{equation}\label{eq:inv}
f(g \cdot x)=f(x) 
\end{equation}
$$

---

# Higher-order networks and why is topology useful
Higher-order representations of graph relations will be usefull to model the way natural phenomena relates. Regular graphs can only encode pair-wise relation information such an edge between two vertices. However, we might want to model relationships between triplets of nodes, for example. A way tu represent tuples of nodes relating is through the use of simplices. A simplex is a tuple of nodes that encodes a relation between them and a geometric realization of it. An *abstract simplicial complex* (ASC) is the combinatorial expression of a non-empty set of simplices. 

Concretly, let $$ \mathcal{P}(S) $$ be the powerset of $$ S $$ and another set $$ \mathcal{K} \subset \mathcal{P}(S) $$, then $$ \mathcal{K} $$ is an ASC if for every $$ X \in \mathcal{K} $$ and every non-empty $$ Y \subseteq X $$ it holds that $$ Y \in \mathcal{K} $$. Also, we define $$ \mid\mathcal{K}\mid $$ to be the highest cardinality of a simplex in an ASC minus 1. If the rank is $$r$$ then it holds $$ \forall X \in \mathcal{K}: r \geq \mid X \mid $$. A simplex can then be thought of an extension of a triangle to multiple dimensions. An $$ n $$-simplex consists of $$ n+1 $$  fully connected points, i.e. a $$0$$-simplex is a point, a $$1$$-simplex is a line, a $$ 2 $$-simplex is a triangle, a $$ 3 $$-simplex is a tetrahedron. Following on <d-cite key="hajij2022topological"></d-cite> and in an effort to standardize terminology, simplices will be refered to as r-cells. Where $$ r $$ denotes the *rank* of the particular simplex. To assign features to higher-dimensional simplices in graphs, we turn to the definition of a generalized notion of graphs called abstract simplicial complexes.

## Geometric realization

Although an ASC is a purely combinatorial object, it always entails a **geometric realization**. For the case of $$r=1$$ then it can be represented as a graph. A **simplicial complex** is the geometric realization of an ASC, constructed out of the underlying geometric of the points in $$ \mathcal{K}$$. As such, they can be constructed using the set of verticies $$ V $$ of a graph as disjoint points in space or even taking a graph $$ G = (V, E) $$ where $$ V $$ is the set of 0-cells and $$ E $$ is the set of 1-cells. Transforming a set of points to a simplicial complex is called **lifting**. 
 
As an example, we may consider the clique lifting procedure. A *clique* $$ C \subseteq V $$, such that $$ C $$ is complete. In other words, there is an edge between every pair of vertices. In this lifting procedure each clique will become an r-cell and have it's own set of neighbours. Note that the time complexity of this lift is $$ \mathcal{O}(3^{n/3}) $$


## Higher-order neighbourhoods

We can think of each $$r$$-cell as being squished into a point and represented again in another graph. It will have connections to other $$r$$-cells, lower cells such as $$(r-1)$$-cells and higher cells such as $$(r+1)$$-cell. Thus, we will ditch the name *neighbour* and start using adjacencies to define a relation between $$r$$-cells. We will work with only two types of adjacencies as they have proven to be as expressive as using all of them. First, let $$ \sigma$$ and $$ \tau $$ be two simplices, we say that $$\sigma \text{ is on the bound of } \tau $$ as $$ \sigma \prec \tau $$ and: 1) $$ \sigma \subset \tau$$, 2) $$ \nexists \delta: \sigma  \subset \delta \subset \tau $$  

Equation \ref{eq:bound_adj} referes to the relation between a $$r$$-cell and the $$(r-1)$$-cells that compose it. This means that we can think of an *edge* going from each $$r$$-cell to all $$(r-1)$$-cells that are parts of it. Equation \ref{eq:bound_up} referes to the relationship between  $$(r-1)$$-cells and other $$(r-1)$$-cells that are a part of a higher $$r$$-cell. They can be though of the adjacency of composing pieces of a higher-order structure, they are also refered to as **cofaces** in the literature <d-cite key="hajij2022topological"></d-cite>.

$$ 
\begin{equation}\label{eq:bound_adj}
\mathcal{B}(\sigma) = \{\tau \mid \tau \prec \sigma\}
\end{equation}
$$

$$

\begin{equation}\label{eq:bound_up}
\mathcal{N}_{\uparrow}(\sigma) = \{\tau \mid \exists \delta, \tau \prec \delta \land \sigma \prec \delta\}
\end{equation}
$$

--- 
# Lifting techniques
How a higher-order representation of points in the space or a particular graph is to be constructed  depends on the properties that want to be attained and, as always, how efficient is to compute. The *clique lifting* procedure is an NP-Hard problem and at most there will be $$3^{n/3}$$ maximal cliques for a $$n$$-vertex graph. As such the worst-case complexity of the common Bron-Kerbosch algorithm is $$ \mathcal{O}(3^{n/3}) $$. Better algorithms have been discovered however it remains and exponentially costly procedure. Let's go over the alternatives.

## Vietoris-Ripps Complex
The Vietoris-Ripps complex is a common way to form a topological space and a subcomplex of the *Clique Complex*. In practice it is a very good approximation. The time complexity for generating of the procedure depends on the maximum dimension of the simplices in the complex $$ r $$ and number of points $$ n $$ and denoted is given by $$ \mathcal{O}(n^{r+1}) $$. If the points are embedded in Euclidean space then it is an approximation of a larger and richer complex called the Cech Complex. It works by growing balls arround the points and making a connection when these balls intersect. [FIGURE] 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-30-smpn/rips-lift.png" class="img-fluid rounded z-depth-1"%}
    </div>
</div>


## Alpha Complex 
The alpha complex is a fundamental data structure from computational geometry given by a subset $$ \sigma = \{x_{i0},...,x_{ik} \}\subset S $$ belongs to $$ \operatorname{Alpha}(S,r) $$ if there exists a point $$ y \in \mathbb{R}^m $$ that is equidistant from every member of $$ \sigma $$, so that 

$$
\begin{align*}
    \rho := || y- x_{i0}||=...=||y-x_{ik}|| \leq r
\end{align*}
$$
and thus $$||y-x|| \leq r\ \ \  \forall x \in S$$. 
Formally, the alpha complex is defined as the collection:
$$
\begin{align*}
    \operatorname{Alpha}(S, r)=\left\{\sigma \subset S: \bigcap_{x \in \sigma} V_x(r) \neq \emptyset\right\}
\end{align*}
$$
A subset $$ \sigma $$ of size $$ k+1 $$ is called a k-dimensional simplex of $$\operatorname{Alpha}(S,r)$$. \cite{carlsson_computing_2023}

--- 

# Do invariances hold ?


Now let's look at the message passing framework. Equation \ref{eq:msg_eq} comes to replace \ref{compute_message} with our invariant function. Addtionally, to make the network equivariant another step will be performed on a feature vector of a node called $$ x $$ which will contain the positional coordinates of the point in euclidean space. Equation \ref{eq:pos_update} refers to the update in the position embedding of the node so that equivariance is preserved. The proofs that with this condition equivariance holds can be found in <d-cite key="satorras2021n"></d-cite>.

$$
\begin{equation}\label{eq:msg_eq}
 \mathbf{m}_{i j}=\phi_m\left(\mathbf{h}_i^l, \mathbf{h}_j^l, \operatorname{lnv}\left(\mathbf{x}_i^l, \mathbf{x}_j^l\right), \mathbf{a}_{i j}\right)
 \end{equation}
 $$


$$
\begin{equation}\label{eq:pos_update}
    \mathbf{x}_i^{l+1}=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right)
\end{equation} 
$$

## In simplices too ?


Using the previous definitions of neighbourhoods <d-cite key="eijkelboom2023n"></d-cite> defines a message for each neighbourhood as Equation \ref{eq:msg_boundary} and Equation \ref{eq:msg_ua} and replaces the hidden representation update to take these messages into account in Equation \ref{eq:update_sc}. 

$$
\begin{equation}\label{eq:msg_boundary}
m_{\mathcal{B}}(\sigma) = \underset{\tau \in \mathcal{B}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{B}}(h^l_{\sigma}, h^l_{\tau})
\end{equation}
$$

$$
\begin{equation}\label{eq:msg_ua}
m_{\mathcal{N}_{\uparrow}}(\sigma) = \underset{\tau \in \mathcal{N}_{\uparrow}(\sigma)}{\operatorname{Agg}} (\phi_{\mathcal{N}_{\uparrow}}(h^l_{\sigma}, h^l_{\tau}))
\end{equation}
$$

$$
\begin{equation}\label{eq:update_sc}
h_{\sigma}^{l+1} = \phi_{h} (h_{\sigma}^l, m_{\mathcal{B}}(\sigma), m_{\mathcal{N}_{\uparrow}}(\sigma))
\end{equation}
$$

Finally, they define a graph embedding as Equation \ref{eq:agg_simp} where the simplices $$ \mathcal{K} $$ of each dimension $$ r $$ will be aggregated and the final embedding of the complex will be the concatenation of the embedding of each dimension.


$$
\begin{equation}\label{eq:agg_simp}

h_{\mathcal{K}} = \bigoplus_{i=0}^{r} \underset{\sigma \in \mathcal{K}, |\sigma|=i+1}{\operatorname{Agg}} h_\sigma
\end{equation}
$$

---
# The new standard: TopoX

TopoX is a suite of Python packages that aim to provide a standard for developments in TDL. It encompases TopoModelX, TopoNetX, TopoEmbeddX and now TopoBenchmarX. Each of them having functionalities according to their name in the topological domain. Next, we ilustrate the development process and reproduction of <d-cite key="eijkelboom2023n"></d-cite> in the TopoX suite. Additionally, as a base project we use the [ICML TDL Challenge 2024](https://github.com/pyt-team/challenge-icml-2024) for development, which structures all these packages together to be used in the development cycle. Additionally, we use the Pytorch Geometric QM9 dataset as it contains graph data.

## Building structure

The first thing we are concerned with is the **lifting** of our initial graph or set of points. To perform that task we will make use of GHUDI <d-cite key="gudhi:urm"></d-cite>, a Python library with many methods mainly used for Topological Data Analysis. We define a function called ```rips_lift``` which will build a ```SimplicialComplex``` (a TopoNetX representation) from the ```SimplexTree``` returned by GHUDI. Additionally, we add the option to fully connect all the $$0$$-cells.

{% highlight python %}
def rips_lift(graph: torch_geometric.data.Data, dim: int, dis: float, fc_nodes: bool = True) -> SimplicialComplex:
    x_0, pos = graph.x, graph.pos

    points = [pos[i].tolist() for i in range(pos.shape[0])]

    rips_complex = gudhi.RipsComplex(points=points, max_edge_length=dis)
    simplex_tree: SimplexTree  = rips_complex.create_simplex_tree(max_dimension=dim)

    if fc_nodes:
        nodes = [i for i in range(x_0.shape[0])]
        for edge in combinations(nodes, 2):
            simplex_tree.insert(edge)

    return SimplicialComplex.from_gudhi(simplex_tree)

{% endhighlight %}

Now we will use the ```Graph2SimplicialLifting``` base class and override the lifting methods such that we transform an input ```pytorch_geometric.data.Data``` class into another ```Data``` but now representing a higher-order network.

We override the init to include the ```delta``` parameter that defines the *range* of the Vietoris-Ripps lift.
{% highlight python %}

def __init__(self, delta: float = 0, **kwargs):
        super().__init__(**kwargs)
        self.delta = delta

{% endhighlight %}

Now, we override the main method called ```lifted_topology``` which calls our previous ```rips_lift``` and passes it on to the internal construction of the new ```Data```.

{% highlight python %}
 def lift_topology(self, data: torch_geometric.data.Data) -> dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)
{% endhighlight %}

Then, we move on to ```_lifted_topology``` where the object is built. The ```get_complex_connectivity``` is provided by the library and will return a dictionary with the keys ```adjacency_i``` and ```incidence_i``` to represent relations from $$ r $$-cells to $$r$$-cells and $$r$$-cells to $$(r-1)$$-cells respectively. Finally, we assign the ```Data``` object the features of $$0$$-cells (nodes).

{% highlight python %}
    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -> dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'adjacency_{r}'] = lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()
            lifted_topology[f'incidence_{r}'] = lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()


        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'x_idx_{r}'] = torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology

{% endhighlight %}

As you may have noticed we are messing with the adjacency and incidence matrices. This is because we want to make them *edge_index* format, as doing batching with that is easier on PyG. Intially they are a sparse matrix but after transforming they will have the shape ```[2, num_relation]```. We also save a tensor for each dimension called ```x_idx_i``` where we have a *row* for the $$0$$-cells compromising each $$r$$-cell for all ranks. Now we have to do a quick detour

### Mini-batching detour

While we are here we will handle batching. To be brief in PyG a dataset compromising graphs is concatenated into one big graph that is represented by a block-diagonal matrix. As such, the *edge_index* tensor will span all relations, not just the ones of one graph so the indices need to be updated so that graphs do not collide. This is simple in the case of square matrices but note that **incidence** matrices might not be square as they connect $$(r-1)$$-cells to $$r$$-cells. There is a detailed explanation [here](https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html). Next, the structure we use to represent a simplex inherits from ```Data```.

{% highlight python %}
class SimplexData(torch_geometric.data.Data):
    def __inc__(self, key: str, value, *args, **kwargs):
        if 'adjacency' in key:
            rank = int(key.split('_')[-1])
            return torch.tensor([getattr(self, f'x_{rank}').size(0)])
            #return torch.tensor([[getattr(self, f'x_{rank}').size(0)], [getattr(self, f'x_{rank}').size(0)]])
        elif 'incidence' in key:
            rank = int(key.split('_')[-1])
            if rank == 0:
                return torch.tensor([getattr(self, f'x_{rank}').size(0)])
            return torch.tensor([[getattr(self, f'x_{rank-1}').size(0)], [getattr(self, f'x_{rank}').size(0)]])
        elif key == 'x_0' or key == 'x_idx_0':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif key == 'x_1' or key == 'x_idx_1':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif key == 'x_2' or key == 'x_idx_2':
            return torch.tensor([getattr(self, f'x_0').size(0)])
        elif 'index' in key:
            return self.num_nodes
        else:
            return super().__inc__(key, value, *args, **kwargs)

    def __cat_dim__(self, key: str, value, *args, **kwargs):
        if 'adjacency' in key or 'incidence' in key or 'index' in key:
            return 1
        else:
            return 0

{% endhighlight %}


### Piecing it all together
Now we present the final class that contains our lifting procedure with the addition of ```forward``` which we override and only change the final return to use ```SimplexData``` instead of ```Data```

{% highlight python %}
class SimplicialVietorisRipsLifting(Graph2SimplicialLifting):
    def __init__(self, delta: float = 0, **kwargs):
        super().__init__(**kwargs)
        self.delta = delta

    def lift_topology(self, data: torch_geometric.data.Data) -> dict:
        simplicial_complex = rips_lift(data, self.complex_dim, self.delta)

        feature_dict = {}
        for i, node in enumerate(data.x):
            feature_dict[i] = node

        simplicial_complex.set_simplex_attributes(feature_dict, name='features')

        return self._get_lifted_topology(simplicial_complex, data)

    def _get_lifted_topology(
        self, simplicial_complex: SimplicialComplex, graph: nx.Graph
    ) -> dict:
        lifted_topology = get_complex_connectivity(
            simplicial_complex, self.complex_dim, signed=self.signed
        )

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'adjacency_{r}'] = lifted_topology[f'adjacency_{r}'].to_dense().nonzero().t().contiguous()
            lifted_topology[f'incidence_{r}'] = lifted_topology[f'incidence_{r}'].to_dense().nonzero().t().contiguous()

        for r in range(0, simplicial_complex.dim+1):
            lifted_topology[f'x_idx_{r}'] = torch.tensor(simplicial_complex.skeleton(r), dtype=torch.int)

        lifted_topology["x_0"] = torch.stack(
            list(simplicial_complex.get_simplex_attributes("features", 0).values())
        )

        return lifted_topology
   def forward(self, data: torch_geometric.data.Data) -> torch_geometric.data.Data:
        initial_data = data.to_dict()
        lifted_topology = self.lift_topology(data)
        lifted_topology = self.feature_lifting(lifted_topology)
        return SimplexData(**initial_data, **lifted_topology)

{% endhighlight %}

The only thing missing is to define our prefered ```feature_lifting```.

## Giving meaning to structure

Now that we have a higher-order topological space, there is only one thing we are missing. *What should the embeddings of the $$r$$-cells higher than $$0$$ be ?*

 There is not much literature on this matter aside from naive approaches such as the sum, mean or concat of lower dimensions. For this task we use the same method as the authors <d-cite key="eijkelboom2023n"></d-cite> . Performing an element-wise mean of the components of lower $$r$$-cells. The following definition comes in place of the ```lift_features``` as can be found in the example ```ProjectionSum```.

{% highlight python %}

def lift_features(
        self, data: torch_geometric.data.Data | dict
    ) -> torch_geometric.data.Data | dict:
        max_dim = max([int(key.split("_")[1]) for key in data.keys() if "incidence" in key])


        simplex_test_dict = defaultdict(list)
        for i in range(max_dim+1):
            z = []
            # For the k-component point in the i-simplices where k <= i
            for k in range(i+1):
                # Get the k-node indices of the i-simplices (i.e. the k-components of the i-simplices)
                z_i_idx = data[f'x_idx_{i}'][:, k]
                # Get the node embeddings for the k-components of the i-simplices
                z_i = data['x_0'][z_i_idx]
                z.append(z_i)
            z = torch.stack(z, dim=2)
            # Mean along the simplex dimension
            z = z.mean(axis=2)
            # Assign to each i-simplex the corresponding feature
            for l, embedding in enumerate(z):
                simplex_test_dict[i].append(z[l])

        for k, v in simplex_test_dict.items():
            data[f'x_{k}'] = torch.stack(v)
        return data

{% endhighlight %}

## The network


### Defining a convolutional kernel

### Defining a layer

### All together


## 

# Experiments
# Conclusions
